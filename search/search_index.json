{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"GIANT","text":"<p>Gigapixel Image Agent for Navigating Tissue</p> <p>GIANT is an agentic system that uses large language models to autonomously navigate whole-slide images (WSIs) for pathology analysis. The agent iteratively examines regions of a gigapixel image, zooming in on areas of interest until it can answer a question about the slide.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Autonomous Navigation - LLM decides where to look next based on visual content</li> <li>Multi-Scale Analysis - Examines both tissue architecture and cellular detail</li> <li>Provider Agnostic - Supports OpenAI and Anthropic providers (Gemini planned)</li> <li>Benchmark Evaluation - Reproduce results on MultiPathQA (GTEx, TCGA, PANDA)</li> <li>Trajectory Visualization - Interactive viewer for agent reasoning</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code># Install dependencies\nuv sync\n\n# Activate virtual environment\nsource .venv/bin/activate\n\n# Configure API key\nexport OPENAI_API_KEY=sk-...\n\n# Run on a single WSI\ngiant run /path/to/slide.svs -q \"What tissue is this?\"\n\n# Run benchmark\ngiant benchmark gtex --provider openai\n</code></pre>"},{"location":"#how-it-works","title":"How It Works","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  1. Load WSI + Generate Thumbnail           \u2502\n\u2502     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                     \u2502\n\u2502     \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2502 + Axis Guides       \u2502\n\u2502     \u2502 \u2588   Tissue    \u2588 \u2502                     \u2502\n\u2502     \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2502                     \u2502\n\u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  2. LLM Analyzes + Selects Region           \u2502\n\u2502     \"I see suspicious area at (45K, 32K)    \u2502\n\u2502      Let me zoom in for cellular detail...\" \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  3. Extract High-Res Crop                   \u2502\n\u2502     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                               \u2502\n\u2502     \u2502\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2502 1000x1000 @ high resolution   \u2502\n\u2502     \u2502\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2502                               \u2502\n\u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                               \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  4. Repeat Until Answer                     \u2502\n\u2502     \"Based on cellular morphology,          \u2502\n\u2502      this is adenocarcinoma.\"               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"#benchmark-results","title":"Benchmark Results","text":"Benchmark Task Our Result Paper (GIANT x1) GTEx Organ Classification (20-way) 67.6% 53.7% TCGA Cancer Diagnosis (30-way) 25.2% 32.3% PANDA Prostate Grading (6-way) Pending 23.2% <ul> <li>GTEx (67.6%): Exceeds the paper's GPT-5 + GIANT x1 (53.7%) and x5 (60.7%) results.</li> <li>TCGA (25.2%): Below the paper's GIANT x1 (32.3%), but above paper thumbnail (9.2%) and patch (12.8%) baselines.</li> </ul>"},{"location":"#supported-models","title":"Supported Models","text":"Provider Model Status OpenAI <code>gpt-5.2</code> Default Anthropic <code>claude-sonnet-4-5-20250929</code> Supported Google <code>gemini-3-pro-preview</code> Planned (not yet implemented)"},{"location":"#documentation","title":"Documentation","text":""},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Installation - Set up your development environment</li> <li>Quickstart - Run your first inference</li> <li>First Benchmark - Reproduce paper results</li> </ul>"},{"location":"#understanding-giant","title":"Understanding GIANT","text":"<ul> <li>What is GIANT? - High-level overview</li> <li>Architecture - System design</li> <li>Navigation Algorithm - Core algorithm explanation</li> <li>LLM Integration - Provider implementations</li> </ul>"},{"location":"#how-to-guides","title":"How-To Guides","text":"<ul> <li>Running Inference - Single WSI analysis</li> <li>Running Benchmarks - Full benchmark evaluation</li> <li>Configuring Providers - API key setup</li> <li>Visualizing Trajectories - Inspect agent behavior</li> </ul>"},{"location":"#reference","title":"Reference","text":"<ul> <li>CLI Reference - Command-line options</li> <li>Configuration - All configuration options</li> <li>Project Structure - Codebase organization</li> <li>Model Registry - Approved models</li> </ul>"},{"location":"#development","title":"Development","text":"<ul> <li>Contributing - How to contribute</li> <li>Testing - Testing practices</li> <li>Specifications - Implementation specs (Spec-01 to Spec-12)</li> </ul>"},{"location":"#data-requirements","title":"Data Requirements","text":"<p>The MultiPathQA benchmark requires 862 unique WSI files (~500+ GiB):</p> <ul> <li>TCGA: 474 <code>.svs</code> files (~472 GiB)</li> <li>GTEx: 191 <code>.tiff</code> files</li> <li>PANDA: 197 <code>.tiff</code> files</li> </ul> <p>See Data Acquisition for download instructions.</p>"},{"location":"#links","title":"Links","text":"<ul> <li>GitHub Repository</li> <li>GIANT Paper</li> <li>MultiPathQA Dataset</li> </ul>"},{"location":"data-acquisition/","title":"Data Acquisition Guide","text":""},{"location":"data-acquisition/#overview","title":"Overview","text":"<p>GIANT evaluates against the MultiPathQA benchmark, which comprises 934 WSI-level questions across 862 unique whole-slide images (WSIs). The benchmark metadata (questions, prompts, answers) is available on HuggingFace, but the WSI files themselves must be acquired separately from their original sources due to licensing and size constraints.</p> <p>This is a critical operational requirement. Without the actual WSI files, you cannot: - Run the GIANT agent on real pathology images - Reproduce the paper's benchmark results - Validate the end-to-end system</p>"},{"location":"data-acquisition/#multipathqa-benchmark-structure","title":"MultiPathQA Benchmark Structure","text":"<p>The benchmark contains 5 distinct tasks spanning 3 data sources:</p> Benchmark Questions Unique WSIs Task Data Source Metric <code>tcga</code> 221 221 Cancer Diagnosis (30-way) TCGA Balanced Accuracy <code>tcga_expert_vqa</code> 128 76 Pathologist-Authored VQA TCGA Accuracy <code>tcga_slidebench</code> 197 183 SlideBench VQA TCGA Accuracy <code>gtex</code> 191 191 Organ Classification (20-way) GTEx Balanced Accuracy <code>panda</code> 197 197 Prostate Grading (6-way) PANDA Balanced Accuracy Total 934 862 - - - <p>Note: The 3 TCGA benchmarks share some slides. Total unique TCGA files needed: 474 (not 221).</p>"},{"location":"data-acquisition/#data-components","title":"Data Components","text":""},{"location":"data-acquisition/#1-multipathqa-metadata-already-available","title":"1. MultiPathQA Metadata (Already Available)","text":"File Location Size Status <code>MultiPathQA.csv</code> <code>data/multipathqa/</code> ~150KB Included <p>This CSV contains: - <code>benchmark_name</code>: Task identifier (tcga, gtex, panda, tcga_expert_vqa, tcga_slidebench) - <code>image_path</code>: Filename of the WSI (e.g., <code>GTEX-OIZH-0626.tiff</code>) - <code>prompt</code>: Question to ask about the slide - <code>answer</code>: Ground truth label - <code>options</code>: Multiple choice options (when applicable)</p>"},{"location":"data-acquisition/#2-whole-slide-images-must-be-acquired","title":"2. Whole-Slide Images (Must Be Acquired)","text":"Source Files Needed Format Size (Approx) License TCGA 474 <code>.svs</code> ~472 GiB total (GDC API sum of <code>file_size</code>) Open Access GTEx 191 <code>.tiff</code> Varies (see notes below) GTEx Portal (terms vary) PANDA 197 <code>.tiff</code> Varies; Kaggle packaging is large Kaggle Competition Total 862 - \u2265 ~472 GiB (TCGA alone) Mixed <p>Reality check: the earlier \u201c~95\u2013135 GB total\u201d estimate was wrong by an order of magnitude. TCGA alone is ~472 GiB for the 474 slides referenced by MultiPathQA.</p>"},{"location":"data-acquisition/#3-file-lists-provided","title":"3. File Lists (Provided)","text":"<p>We have generated exact file lists from the MultiPathQA CSV:</p> File Contents <code>data/wsi/tcga_files.txt</code> 474 TCGA slide filenames <code>data/wsi/gtex_files.txt</code> 191 GTEx slide filenames <code>data/wsi/panda_files.txt</code> 197 PANDA slide filenames <p>Use these to verify downloads or create download manifests.</p>"},{"location":"data-acquisition/#directory-structure","title":"Directory Structure","text":"<p>WSIs should be placed under a <code>wsi_root</code> directory with the following structure:</p> <pre><code>data/\n\u251c\u2500\u2500 multipathqa/\n\u2502   \u2514\u2500\u2500 MultiPathQA.csv          # Benchmark metadata (already here)\n\u2514\u2500\u2500 wsi/                          # WSI files (you must populate this)\n    \u251c\u2500\u2500 tcga_files.txt            # File list for TCGA\n    \u251c\u2500\u2500 gtex_files.txt            # File list for GTEx\n    \u251c\u2500\u2500 panda_files.txt           # File list for PANDA\n    \u251c\u2500\u2500 tcga/                     # TCGA slides (all 3 benchmarks)\n    \u2502   \u251c\u2500\u2500 TCGA-02-0266-01Z-00-DX1.svs\n    \u2502   \u251c\u2500\u2500 TCGA-HT-A616-01Z-00-DX1.svs  # Used in expert_vqa\n    \u2502   \u251c\u2500\u2500 TCGA-HC-7080-01Z-00-DX1.svs  # Used in slidebench\n    \u2502   \u2514\u2500\u2500 ... (474 total)\n    \u251c\u2500\u2500 gtex/                     # GTEx organ classification slides\n    \u2502   \u251c\u2500\u2500 GTEX-OIZH-0626.tiff\n    \u2502   \u2514\u2500\u2500 ... (191 total)\n    \u2514\u2500\u2500 panda/                    # PANDA prostate grading slides\n        \u251c\u2500\u2500 dbf7cc49ae2e9831448c3ca54ad92708.tiff\n        \u2514\u2500\u2500 ... (197 total)\n</code></pre> <p>The evaluation runner accepts <code>--wsi-root data/wsi</code> and resolves each <code>image_path</code> from the CSV to the appropriate subdirectory based on <code>benchmark_name</code>.</p>"},{"location":"data-acquisition/#acquisition-instructions","title":"Acquisition Instructions","text":""},{"location":"data-acquisition/#tcga-the-cancer-genome-atlas","title":"TCGA (The Cancer Genome Atlas)","text":"<p>Source: NIH Genomic Data Commons (GDC) Access: Open access (no approval required) Files Needed: 474 <code>.svs</code> files (see <code>data/wsi/tcga_files.txt</code>) Tool: GDC Data Transfer Tool (recommended) or direct download via GDC API</p> <ol> <li> <p>Install GDC Client: <pre><code># macOS\nbrew install gdc-client\n\n# Or download from: https://gdc.cancer.gov/access-data/gdc-data-transfer-tool\n</code></pre></p> </li> <li> <p>Create a manifest file from the GDC Portal:</p> </li> <li>Go to https://portal.gdc.cancer.gov/</li> <li>Filter: Data Type = \"Slide Image\", Data Format = \"SVS\"</li> <li>Use the file list in <code>data/wsi/tcga_files.txt</code> to find specific slides</li> <li> <p>Add to cart and export manifest</p> </li> <li> <p>Download: <pre><code>gdc-client download -m manifest.txt -d data/wsi/tcga/\n</code></pre></p> </li> </ol> <p>Important: MultiPathQA includes a <code>file_id</code> column for TCGA rows (the GDC UUID). Also note that GDC\u2019s downloaded <code>file_name</code> is UUID-suffixed (e.g. <code>TCGA-...-DX1.&lt;uuid&gt;.svs</code>), and <code>gdc-client</code> typically stores files under <code>data/wsi/tcga/&lt;file_id&gt;/&lt;file_name&gt;</code>.</p> <p>The evaluation runner supports this default <code>gdc-client</code> layout (no manual renaming required), as long as you keep the <code>file_id</code> column in <code>MultiPathQA.csv</code>.</p> <p>Helper (recommended for planning / smoke data):</p> <pre><code># Estimate TCGA total size (uses GDC API, no download)\nuv run python -m giant.data.tcga estimate\n\n# Download the N smallest TCGA slides into data/wsi/tcga/&lt;file_id&gt;/&lt;file_name&gt;\nuv run python -m giant.data.tcga download --smallest 5\n</code></pre>"},{"location":"data-acquisition/#gtex-genotype-tissue-expression","title":"GTEx (Genotype-Tissue Expression)","text":"<p>Source: GTEx Portal Access: Terms vary; verify your access/terms before bulk download Files Needed: 191 <code>.tiff</code> files (see <code>data/wsi/gtex_files.txt</code>) URL: https://www.gtexportal.org/</p> <ol> <li>Download via GTEx Portal:</li> <li>Navigate to: https://www.gtexportal.org/home/histologyPage</li> <li>Filter by tissue type</li> <li>Download individual slides or bulk</li> </ol> <p>Note: The previously documented <code>s3://gtex-resources/histology/</code> bucket does not exist (NoSuchBucket). If you have a correct bulk-download source (AWS/GCP/etc), add it here once verified.</p>"},{"location":"data-acquisition/#panda-prostate-cancer-grade-assessment","title":"PANDA (Prostate Cancer Grade Assessment)","text":"<p>Source: Kaggle Competition (2020) Access: Requires Kaggle account and competition rules acceptance Files Needed: 197 <code>.tiff</code> files (see <code>data/wsi/panda_files.txt</code>) URL: https://www.kaggle.com/c/prostate-cancer-grade-assessment</p> <ol> <li>Accept Competition Rules:</li> <li>Visit the competition page</li> <li>Click \"Late Submission\" or \"Join Competition\"</li> <li> <p>Accept the rules</p> </li> <li> <p>Install Kaggle CLI: <pre><code>pip install kaggle\n# Configure: ~/.kaggle/kaggle.json with your API key\n</code></pre></p> </li> <li> <p>Download: <pre><code>kaggle competitions download -c prostate-cancer-grade-assessment -p data/wsi/panda/\nunzip data/wsi/panda/prostate-cancer-grade-assessment.zip -d data/wsi/panda/\n</code></pre></p> </li> </ol> <p>Warning: The full PANDA dataset is ~400GB. You only need 197 files for MultiPathQA. After unzipping, you can delete files not in <code>data/wsi/panda_files.txt</code>.</p>"},{"location":"data-acquisition/#verification","title":"Verification","text":"<p>After downloading, verify your setup using the built-in checker:</p> <pre><code># Check each dataset\nuv run giant check-data tcga\nuv run giant check-data gtex\nuv run giant check-data panda\n\n# For verbose output showing missing files\nuv run giant check-data tcga -v\n</code></pre> <p>The <code>check-data</code> command: - Validates against <code>MultiPathQA.csv</code> - Handles both flat and <code>gdc-client</code> directory layouts - Reports found/missing counts</p>"},{"location":"data-acquisition/#quick-start-minimal-testing","title":"Quick Start (Minimal Testing)","text":"<p>For development and CI, you can use a single test WSI:</p> <pre><code># Download OpenSlide test data (~10MB)\nmkdir -p tests/integration/wsi/data\ncurl -L -o tests/integration/wsi/data/CMU-1-Small-Region.svs \\\n    https://openslide.cs.cmu.edu/download/openslide-testdata/Aperio/CMU-1-Small-Region.svs\n\n# Run integration tests\nWSI_TEST_FILE=tests/integration/wsi/data/CMU-1-Small-Region.svs \\\n    uv run pytest tests/integration/wsi/ -v\n</code></pre>"},{"location":"data-acquisition/#subset-testing","title":"Subset Testing","text":"<p>For validation without downloading all hundreds of GiB, you can start with a subset:</p> <pre><code># Download first 5 files from each source for Spec-11.5 validation\n# See the file lists in data/wsi/*.txt\n</code></pre> <p>Recommended subset sizes: - Minimal: 1\u20135 per source (expect multiple GiB depending on slide sizes) - Basic: 10\u201320 per source - Full: All 862 (hundreds of GiB; TCGA alone is ~472 GiB)</p>"},{"location":"data-acquisition/#storage-requirements","title":"Storage Requirements","text":"Component Size TCGA WSIs (474) ~472 GiB GTEx WSIs (191) Varies PANDA WSIs (197) Varies Working space (crops, trajectories) ~10\u201350+ GiB (depends on run size) Total recommended Plan for many hundreds of GiB free"},{"location":"data-acquisition/#estimated-download-times","title":"Estimated Download Times","text":"<p>Download time varies widely by source and mirror. As a rough rule of thumb:</p> <p><code>time \u2248 size / bandwidth</code></p> <p>Example: TCGA is ~472 GiB. At 100 Mbps sustained, that\u2019s on the order of ~10\u201311 hours (ignoring overhead/retries).</p>"},{"location":"data-acquisition/#troubleshooting","title":"Troubleshooting","text":""},{"location":"data-acquisition/#wsi-file-not-found-errors","title":"\"WSI file not found\" errors","text":"<ol> <li>Check the file exists: <code>ls data/wsi/tcga/&lt;filename&gt;.svs</code></li> <li>Check file extension case: <code>.SVS</code> vs <code>.svs</code></li> <li>Verify the directory structure matches expected layout</li> <li>Ensure benchmark-to-directory mapping is correct (all TCGA benchmarks use <code>tcga/</code>)</li> </ol>"},{"location":"data-acquisition/#openslide-errors","title":"OpenSlide errors","text":"<pre><code># Verify OpenSlide can read your files\npython -c \"\nimport openslide\nslide = openslide.OpenSlide('data/wsi/tcga/some_file.svs')\nprint(f'Dimensions: {slide.dimensions}')\nprint(f'Levels: {slide.level_count}')\n\"\n</code></pre>"},{"location":"data-acquisition/#disk-space-issues","title":"Disk space issues","text":"<p>WSIs are large. Check available space: <pre><code>df -h data/\n</code></pre></p>"},{"location":"data-acquisition/#panda-dataset-too-large","title":"PANDA dataset too large","text":"<p>The full PANDA Kaggle download is ~400GB but we only need 197 files: <pre><code># After download, keep only needed files\ncd data/wsi/panda\nwhile read -r file; do\n    if [ ! -f \"$file\" ]; then\n        echo \"Missing: $file\"\n    fi\ndone &lt; ../panda_files.txt\n\n# Delete extra files (be careful!)\n# find . -name \"*.tiff\" | while read f; do\n#     grep -q \"$(basename $f)\" ../panda_files.txt || rm \"$f\"\n# done\n</code></pre></p>"},{"location":"data-acquisition/#references","title":"References","text":"<ul> <li>GIANT Paper</li> <li>MultiPathQA on HuggingFace</li> <li>GDC Data Portal</li> <li>GTEx Portal</li> <li>PANDA Challenge</li> <li>OpenSlide Test Data</li> </ul>"},{"location":"archive/bugs/BUG-001-boundary-crop-no-handling/","title":"BUG-001: Boundary Crop Has No Graceful Handling","text":""},{"location":"archive/bugs/BUG-001-boundary-crop-no-handling/#severity-p3-low-priority-policydocumentation","title":"Severity: P3 (Low Priority) - Policy/Documentation","text":""},{"location":"archive/bugs/BUG-001-boundary-crop-no-handling/#status-fixed-documented-and-tested","title":"Status: Fixed - Documented and Tested","text":""},{"location":"archive/bugs/BUG-001-boundary-crop-no-handling/#description","title":"Description","text":"<p><code>CropEngine.crop()</code> does not clamp crop regions to slide bounds. If a region extends beyond the slide boundary, behavior is delegated to OpenSlide.</p>"},{"location":"archive/bugs/BUG-001-boundary-crop-no-handling/#related-spec-tests","title":"Related Spec Tests","text":"<ul> <li>P1-1: Boundary crop (right edge)</li> <li>P1-2: Boundary crop (bottom edge)</li> </ul>"},{"location":"archive/bugs/BUG-001-boundary-crop-no-handling/#current-behavior","title":"Current Behavior","text":"<pre><code># In crop_engine.py - NO bounds validation before read\nraw_image = self._reader.read_region(\n    location=(region.x, region.y),\n    level=selected.level,\n    size=region_size_at_level,\n)\n</code></pre> <p>OpenSlide\u2019s documented behavior for out-of-bounds reads is to return a full tile and pad any out-of-bounds pixels with transparency (RGBA). Since <code>WSIReader.read_region()</code> converts the returned image to RGB, padded pixels become black in the final crop.</p>"},{"location":"archive/bugs/BUG-001-boundary-crop-no-handling/#expected-behavior-from-spec-055","title":"Expected Behavior (from Spec-05.5)","text":"<p>\"Graceful handling (clamp or pad)\" - The pipeline should either: 1. Clamp the region to slide bounds before reading, OR 2. Pad the result with a consistent background color</p> <p>Current behavior already satisfies the \u201cpad\u201d option (black padding after RGBA\u2192RGB conversion).</p>"},{"location":"archive/bugs/BUG-001-boundary-crop-no-handling/#code-location","title":"Code Location","text":"<p><code>src/giant/core/crop_engine.py:102-165</code> - <code>crop()</code> method</p>"},{"location":"archive/bugs/BUG-001-boundary-crop-no-handling/#existing-infrastructure","title":"Existing Infrastructure","text":"<p>The codebase already has <code>GeometryValidator.clamp_region()</code> in <code>src/giant/geometry/validators.py</code>, but Spec-09 places bounds validation at the agent layer (strict by default; clamp only as an explicit recovery path).</p>"},{"location":"archive/bugs/BUG-001-boundary-crop-no-handling/#proposed-fix","title":"Proposed Fix","text":"<p>Document and decide the policy explicitly:</p> <ul> <li>Option A (keep current): Accept OpenSlide padding (black after conversion) and document it as the canonical behavior for out-of-bounds regions.</li> <li>Option B (agent-level strict validation): In Spec-09, validate crops with <code>GeometryValidator.validate(...)</code> and re-prompt the LLM on invalid regions.</li> <li>Option C (agent-level clamp recovery): In Spec-09, clamp only as an explicit, test-covered recovery path.</li> </ul> <p>If we decide to clamp inside the crop pipeline (Option D), integrate <code>GeometryValidator.clamp_region()</code> into <code>CropEngine.crop()</code>:</p> <pre><code>def crop(\n    self,\n    region: Region,\n    target_size: int = 1000,\n    bias: float = 0.85,\n    jpeg_quality: int = 85,\n) -&gt; CroppedImage:\n    # NEW: Validate/clamp region to slide bounds\n    metadata = self._reader.get_metadata()\n    bounds = Size(width=metadata.width, height=metadata.height)\n    validator = GeometryValidator()\n    clamped_region = validator.clamp_region(region, bounds)\n\n    # Continue with clamped_region instead of region...\n</code></pre>"},{"location":"archive/bugs/BUG-001-boundary-crop-no-handling/#impact","title":"Impact","text":"<ul> <li>Without explicit documentation, engineers may assume out-of-bounds crops are errors or \u201cundefined\u201d.</li> <li>If we want strict bounds semantics, the policy belongs in Spec-09 (agent loop), not necessarily inside <code>CropEngine</code>.</li> </ul>"},{"location":"archive/bugs/BUG-001-boundary-crop-no-handling/#testing-required","title":"Testing Required","text":"<ul> <li>Integration test (real file): Crop region extending past right/bottom edge and assert output size is preserved and padded area is black.</li> </ul>"},{"location":"archive/bugs/BUG-001-boundary-crop-no-handling/#resolution","title":"Resolution","text":"<p>Decision: Option A (keep current behavior with documentation)</p> <p>The boundary behavior has been documented and tested:</p> <ol> <li> <p>Documentation: <code>src/giant/core/crop_engine.py</code> module docstring now explicitly documents the boundary behavior policy: out-of-bounds pixels are filled with black (from OpenSlide's transparent padding after RGB conversion).</p> </li> <li> <p>Integration Tests: Added <code>TestBoundaryCropBehavior</code> class in <code>tests/integration/wsi/test_crop_pipeline.py</code> with:</p> </li> <li><code>test_boundary_crop_right_edge</code> (P1-1)</li> <li><code>test_boundary_crop_bottom_edge</code> (P1-2)</li> <li><code>test_boundary_crop_corner</code> (both edges)</li> <li> <p><code>test_boundary_crop_preserves_size</code> (size/aspect ratio validation)</p> </li> <li> <p>Future Work: Bounds validation/clamping can be added at the agent layer (Spec-09) if needed, using the existing <code>GeometryValidator.clamp_region()</code> utility.</p> </li> </ol>"},{"location":"archive/bugs/BUG-002-spec-contradiction-upsample-small-regions/","title":"BUG-002: Spec Contradiction - Small Region Upsampling","text":""},{"location":"archive/bugs/BUG-002-spec-contradiction-upsample-small-regions/#severity-p2-medium-priority-spec-055-doc-mismatch","title":"Severity: P2 (Medium Priority) - Spec-05.5 Doc Mismatch","text":""},{"location":"archive/bugs/BUG-002-spec-contradiction-upsample-small-regions/#status-fixed","title":"Status: Fixed","text":""},{"location":"archive/bugs/BUG-002-spec-contradiction-upsample-small-regions/#description","title":"Description","text":"<p>There is a contradiction between Spec-05.5's P1-3 test expectation and Spec-05 / the GIANT paper.</p>"},{"location":"archive/bugs/BUG-002-spec-contradiction-upsample-small-regions/#the-contradiction","title":"The Contradiction","text":"<p>Spec-05.5 Test P1-3:</p> <p>\"Tiny region (&lt; target_size) - Request 100x100 L0 region with target_size=1000 - Upsampled correctly, no artifacts\"</p> <p>Spec-05 Acceptance Criteria (line 15):</p> <p>\"Downsamples... (never upsample; if the read image is already \u2264 S, return it unchanged)\"</p> <p>Current Implementation (crop_engine.py:188-190): <pre><code># Never upsample: if image is smaller than target, return as-is\nif original_long_side &lt;= target_size:\n    return image, 1.0\n</code></pre></p>"},{"location":"archive/bugs/BUG-002-spec-contradiction-upsample-small-regions/#analysis","title":"Analysis","text":"<p>The current implementation correctly follows the paper and Spec-05: it never upsamples small regions. Spec-05.5\u2019s P1-3 expected result is the incorrect part.</p>"},{"location":"archive/bugs/BUG-002-spec-contradiction-upsample-small-regions/#why-the-paper-says-never-upsample","title":"Why the Paper Says \"Never Upsample\"","text":"<p>Upsampling pathology images introduces interpolation artifacts that can: 1. Create false texture patterns 2. Blur diagnostic features 3. Mislead the LLM about tissue structure</p> <p>The LMM receives native resolution data, which is more accurate than interpolated data.</p>"},{"location":"archive/bugs/BUG-002-spec-contradiction-upsample-small-regions/#recommendation","title":"Recommendation","text":"<p>Resolved by keeping the current implementation (paper-faithful) and updating Spec-05.5 P1-3 expected behavior:</p> <p>Current: \"Upsampled correctly, no artifacts\" Updated: \"Returns native resolution unchanged (never upsample per Spec-05 / paper)\"</p>"},{"location":"archive/bugs/BUG-002-spec-contradiction-upsample-small-regions/#code-location","title":"Code Location","text":"<ul> <li><code>src/giant/core/crop_engine.py</code> - <code>CropEngine._resize_to_target()</code></li> <li><code>docs/specs/spec-05.5-wsi-integration-checkpoint.md</code> - P1-3 test</li> </ul>"},{"location":"archive/bugs/BUG-002-spec-contradiction-upsample-small-regions/#testing-required","title":"Testing Required","text":"<ul> <li>Verify current test <code>test_region_smaller_than_target_no_upsample</code> passes \u2713</li> <li>Verify current test <code>test_very_small_region</code> passes \u2713</li> <li>Update Spec-05.5 expectation for P1-3 (doc-only)</li> </ul>"},{"location":"archive/bugs/BUG-003-huge-region-no-protection/","title":"BUG-003: Huge Region Has No Memory Protection","text":""},{"location":"archive/bugs/BUG-003-huge-region-no-protection/#severity-p1-high-priority","title":"Severity: P1 (High Priority)","text":""},{"location":"archive/bugs/BUG-003-huge-region-no-protection/#status-fixed","title":"Status: Fixed","text":""},{"location":"archive/bugs/BUG-003-huge-region-no-protection/#description","title":"Description","text":"<p>The <code>CropEngine.crop()</code> method has no protection against extremely large region requests that could exhaust memory. A request for the entire slide dimensions could attempt to load gigabytes of pixel data into RAM.</p>"},{"location":"archive/bugs/BUG-003-huge-region-no-protection/#related-spec-tests","title":"Related Spec Tests","text":"<ul> <li>P1-4: Huge region (entire slide) - \"Request full slide dimensions - Falls back to thumbnail or errors gracefully\"</li> </ul>"},{"location":"archive/bugs/BUG-003-huge-region-no-protection/#current-behavior","title":"Current Behavior","text":"<pre><code># In crop_engine.py - Guardrail prevents extreme reads\nregion_size_at_level = size_at_level(\n    (region.width, region.height), selected.downsample\n)\nif max(region_size_at_level) &gt; effective_max:\n    raise ValueError(\n        \"Region too large ... Use a smaller region or get_thumbnail() ...\"\n    )\n</code></pre> <p><code>openslide.read_region</code> allocates an RGBA buffer (4 bytes/pixel) for <code>w*h</code> pixels before any resizing. <code>WSIReader.read_region()</code> then converts RGBA \u2192 RGB, which can transiently duplicate memory.</p> <p>For a 100,000 \u00d7 80,000 pixel slide at Level-0: - Uncompressed RGBA: <code>100000 * 80000 * 4 \u2248 32 GB</code> (plus overhead)</p> <p>Even at 16\u00d7 downsample (if that\u2019s the coarsest available), reading a full slide: - Size: <code>6250 \u00d7 5000 = 31.25M pixels</code> - Uncompressed RGBA: <code>~125 MB</code> (plus overhead)</p> <p>The level selector will usually pick a coarse level to keep the crop near the biased target size, but worst cases exist: - Slides with few pyramid levels (small max downsample) - Single-level slides (<code>level_count=1</code>) - Very large regions (near full-slide) combined with small max downsample</p>"},{"location":"archive/bugs/BUG-003-huge-region-no-protection/#expected-behavior-from-spec-055","title":"Expected Behavior (from Spec-05.5)","text":"<p>\"Falls back to thumbnail or errors gracefully\"</p>"},{"location":"archive/bugs/BUG-003-huge-region-no-protection/#proposed-fix","title":"Proposed Fix","text":"<p>Implemented: configurable maximum read dimension check.</p> <pre><code>_DEFAULT_MAX_READ_DIMENSION = 10000\n\ndef crop(..., max_read_dimension: int | None = None) -&gt; CroppedImage:\n    # ...\n    effective_max = _DEFAULT_MAX_READ_DIMENSION if max_read_dimension is None else max_read_dimension\n    if effective_max &gt; 0 and max(region_size_at_level) &gt; effective_max:\n        raise ValueError(\n            f\"Region too large: {w}x{h} pixels at level {selected.level} \"\n            f\"exceeds maximum dimension {effective_max}px. \"\n            \"Use a smaller region or get_thumbnail() for full-slide overview.\"\n        )\n</code></pre>"},{"location":"archive/bugs/BUG-003-huge-region-no-protection/#alternative-automatic-thumbnail-fallback","title":"Alternative: Automatic Thumbnail Fallback","text":"<p>For full-slide requests, automatically fall back to <code>reader.get_thumbnail()</code>:</p> <pre><code>if (region.width == metadata.width and\n    region.height == metadata.height):\n    # Full slide requested - use thumbnail\n    thumb = self._reader.get_thumbnail((target_size, target_size))\n    # ... encode and return\n</code></pre>"},{"location":"archive/bugs/BUG-003-huge-region-no-protection/#code-location","title":"Code Location","text":"<p><code>src/giant/core/crop_engine.py:102-165</code> - <code>crop()</code> method</p>"},{"location":"archive/bugs/BUG-003-huge-region-no-protection/#impact","title":"Impact","text":"<ul> <li>OOM (Out of Memory) errors on large region requests</li> <li>Process crashes without meaningful error message</li> <li>LLM cannot recover from crash (no guidance on what went wrong)</li> </ul>"},{"location":"archive/bugs/BUG-003-huge-region-no-protection/#testing-required","title":"Testing Required","text":"<p>Implemented:</p> <ul> <li>Unit tests verify:</li> <li>default rejection (&gt;10000px at selected level)</li> <li>custom limit via <code>max_read_dimension</code></li> <li>disabling via <code>max_read_dimension=0</code> (for property tests)</li> <li>error message includes actual dimensions</li> </ul>"},{"location":"archive/bugs/BUG-004-missing-integration-tests/","title":"BUG-004: No Integration Tests with Real WSI Files","text":""},{"location":"archive/bugs/BUG-004-missing-integration-tests/#severity-p0-critical","title":"Severity: P0 (Critical)","text":""},{"location":"archive/bugs/BUG-004-missing-integration-tests/#status-fixed-and-validated","title":"Status: Fixed and validated","text":""},{"location":"archive/bugs/BUG-004-missing-integration-tests/#description","title":"Description","text":"<p>Without integration tests that use a real WSI file, we cannot validate the real OpenSlide stack end-to-end. This matters because:</p> <ol> <li>OpenSlide integration is untested in CI</li> <li>Vendor-specific behaviors are unknown</li> <li>Edge cases at slide boundaries are untested</li> <li>Memory behavior with real data is untested</li> </ol>"},{"location":"archive/bugs/BUG-004-missing-integration-tests/#related-spec-tests","title":"Related Spec Tests","text":"<p>All P0 tests require real SVS files:</p> <ul> <li>P0-1: Open real SVS file</li> <li>P0-2: Thumbnail generation</li> <li>P0-3: Read region at L0</li> <li>P0-4: Read region at max level</li> <li>P0-5: Coordinate roundtrip</li> <li>P0-6: Level selection for target</li> <li>P0-7: Crop pipeline end-to-end</li> </ul>"},{"location":"archive/bugs/BUG-004-missing-integration-tests/#current-state","title":"Current State","text":"<pre><code>tests/\n\u251c\u2500\u2500 unit/           # Extensive unit tests (mocks used where appropriate)\n\u2514\u2500\u2500 integration/\n    \u2514\u2500\u2500 wsi/        # Opt-in integration tests (skipped unless test file available)\n        \u251c\u2500\u2500 conftest.py\n        \u251c\u2500\u2500 test_wsi_reader.py\n        \u2514\u2500\u2500 test_crop_pipeline.py\n</code></pre>"},{"location":"archive/bugs/BUG-004-missing-integration-tests/#expected-state-from-spec-055","title":"Expected State (from Spec-05.5)","text":"<pre><code>tests/\n\u251c\u2500\u2500 unit/           # Mocked unit tests\n\u2514\u2500\u2500 integration/\n    \u2514\u2500\u2500 wsi/\n        \u251c\u2500\u2500 conftest.py          # Fixtures for test slides\n        \u251c\u2500\u2500 test_wsi_reader.py   # P0-1 through P0-4\n        \u251c\u2500\u2500 test_coordinates.py  # P0-5\n        \u251c\u2500\u2500 test_level_selector.py # P0-6\n        \u2514\u2500\u2500 test_crop_pipeline.py  # P0-7\n</code></pre>"},{"location":"archive/bugs/BUG-004-missing-integration-tests/#test-data-requirements","title":"Test Data Requirements","text":"<p>Minimum: ONE real <code>.svs</code> file for CI.</p> <p>Option 1: OpenSlide Test Data (Recommended for CI)</p> <pre><code># Small (~10MB) synthetic Aperio file\ncurl -LO https://openslide.cs.cmu.edu/download/openslide-testdata/Aperio/CMU-1-Small-Region.svs\n</code></pre> <p>Option 2: Skip Integration in CI</p> <p>Mark integration tests with <code>@pytest.mark.integration</code> and skip in CI unless test data is available.</p>"},{"location":"archive/bugs/BUG-004-missing-integration-tests/#proposed-fix","title":"Proposed Fix","text":"<p>Implemented and validated:</p> <ol> <li>Added tests under <code>tests/integration/wsi/</code></li> <li>Added <code>conftest.py</code> fixtures that:</li> <li>Check for <code>WSI_TEST_FILE</code> env var</li> <li>Skip tests if no test file available</li> <li>Optionally use a local file under <code>tests/integration/wsi/data/</code></li> <li>Implemented P0 tests using real <code>WSIReader</code> and crop pipeline</li> <li>Marked tests with <code>@pytest.mark.integration</code></li> <li>CI wiring remains optional (can be added once a small, public test slide strategy is chosen)</li> </ol>"},{"location":"archive/bugs/BUG-004-missing-integration-tests/#impact","title":"Impact","text":"<ul> <li>Production bugs will only be discovered in real usage</li> <li>Vendor compatibility issues are invisible</li> <li>Memory leaks or resource handling bugs are undetected</li> <li>Spec-05.5 sign-off criteria cannot be met</li> </ul>"},{"location":"archive/bugs/BUG-004-missing-integration-tests/#sign-off-criteria-from-spec-055","title":"Sign-Off Criteria (from Spec-05.5)","text":"<ul> <li>All P0 tests pass</li> <li>At least ONE real <code>.svs</code> file tested end-to-end</li> <li>Integration test file committed to <code>tests/integration/wsi/</code></li> </ul>"},{"location":"archive/bugs/BUG-004-missing-integration-tests/#validation","title":"Validation","text":"<p>Integration tests were run against a real Aperio SVS file from OpenSlide test data:</p> <ul> <li>Slide: <code>tests/integration/wsi/data/CMU-1-Small-Region.svs</code> (gitignored)</li> <li>Command: <code>WSI_TEST_FILE=tests/integration/wsi/data/CMU-1-Small-Region.svs uv run pytest tests/integration/wsi -v</code></li> <li>Result: <code>17 passed</code> (0 skipped)</li> </ul>"},{"location":"archive/bugs/BUG-005-single-level-slide-untested/","title":"BUG-005: Single-Level Slide Behavior Untested with Real Files","text":""},{"location":"archive/bugs/BUG-005-single-level-slide-untested/#severity-p2-medium-priority","title":"Severity: P2 (Medium Priority)","text":""},{"location":"archive/bugs/BUG-005-single-level-slide-untested/#status-fixed-comprehensive-unit-tests-added","title":"Status: Fixed - Comprehensive Unit Tests Added","text":""},{"location":"archive/bugs/BUG-005-single-level-slide-untested/#description","title":"Description","text":"<p>The <code>PyramidLevelSelector</code> handles single-level slides correctly in unit tests (mocked), but this has never been validated with a real single-level TIFF file.</p>"},{"location":"archive/bugs/BUG-005-single-level-slide-untested/#related-spec-tests","title":"Related Spec Tests","text":"<ul> <li>P2-1: Single-level slide - \"Open slide with <code>level_count=1</code> - No index errors, level 0 used\"</li> </ul>"},{"location":"archive/bugs/BUG-005-single-level-slide-untested/#current-unit-test-coverage","title":"Current Unit Test Coverage","text":"<p><code>tests/unit/core/test_level_selector.py::test_single_level_always_returns_level0</code> - PASSES (mocked)</p> <pre><code>def test_single_level_always_returns_level0():\n    metadata = WSIMetadata(\n        level_count=1,\n        level_dimensions=((100000, 80000),),\n        level_downsamples=(1.0,),\n        # ...\n    )\n    # Test passes with mock\n</code></pre>"},{"location":"archive/bugs/BUG-005-single-level-slide-untested/#untested-scenarios","title":"Untested Scenarios","text":"<ol> <li>Real single-level TIFF/SVS file</li> <li>OpenSlide metadata parsing for single-level</li> <li><code>level_dimensions</code> tuple with single element</li> <li>Actual image read at level 0</li> </ol>"},{"location":"archive/bugs/BUG-005-single-level-slide-untested/#code-location","title":"Code Location","text":"<ul> <li><code>src/giant/core/level_selector.py:166-181</code> - <code>_find_closest_level()</code></li> <li><code>src/giant/wsi/reader.py:126-136</code> - Metadata extraction</li> </ul>"},{"location":"archive/bugs/BUG-005-single-level-slide-untested/#testing-required","title":"Testing Required","text":"<ul> <li>Create or obtain single-level test TIFF</li> <li>Integration test: Open, read metadata, verify <code>level_count=1</code></li> <li>Integration test: Crop region, verify level 0 used</li> </ul>"},{"location":"archive/bugs/BUG-005-single-level-slide-untested/#resolution","title":"Resolution","text":"<p>The single-level slide behavior is now comprehensively tested via unit tests. Creating a real vendor-format single-level WSI file programmatically is impractical (OpenSlide requires specific vendor metadata).</p> <p>Added Tests: <code>TestSingleLevelSlideEdgeCases</code> class in <code>tests/unit/core/test_level_selector.py</code>:</p> <ol> <li><code>test_single_level_very_small_region</code> - Region smaller than target</li> <li><code>test_single_level_very_large_region</code> - Region larger than target</li> <li><code>test_single_level_various_target_sizes</code> - Multiple target sizes (100-5000px)</li> <li><code>test_single_level_various_biases</code> - Multiple bias values (0.5-1.0)</li> <li><code>test_single_level_full_slide_region</code> - Full slide region</li> <li><code>test_single_level_minimum_region</code> - 1x1 pixel region</li> <li><code>test_single_level_no_mpp_handling</code> - No MPP metadata (common for generic TIFFs)</li> </ol> <p>Rationale: The level selector algorithm is pure Python math and doesn't depend on OpenSlide internals. Unit tests with mocked metadata provide complete coverage of the <code>level_count=1</code> code path. Integration testing with a real single-level file would only test OpenSlide's metadata parsing, not our code.</p>"},{"location":"archive/bugs/BUG-007-entire-test-suite-mocked/","title":"BUG-007: \u201cEntire Test Suite Is Mocked\u201d Claim Was Inaccurate","text":""},{"location":"archive/bugs/BUG-007-entire-test-suite-mocked/#severity-p3-low-priority-documentation-correction","title":"Severity: P3 (Low Priority) - Documentation Correction","text":""},{"location":"archive/bugs/BUG-007-entire-test-suite-mocked/#status-closed-superseded-by-bug-004","title":"Status: Closed (Superseded by BUG-004)","text":""},{"location":"archive/bugs/BUG-007-entire-test-suite-mocked/#description","title":"Description","text":"<p>This document corrects an overstated claim from the Spec-05.5 checkpoint.</p> <p>What is true: - We have opt-in integration tests, but CI typically does not exercise the real OpenSlide stack unless a real test slide is provided (tracked as BUG-004).</p> <p>What is not true: - The test suite is not \u201c100% mocked\u201d. - The test suite does not \u201conly test mock interactions\u201d. - Several tests exercise real behavior (math, validation, image resizing/encoding).</p>"},{"location":"archive/bugs/BUG-007-entire-test-suite-mocked/#why-mocking-is-appropriate-here-unit-tests","title":"Why mocking is appropriate here (unit tests)","text":"<ul> <li>Whole-slide images are large binaries (10MB\u2013GB); committing them to the repo is undesirable.</li> <li>OpenSlide behavior depends on OS + native libs; unit tests should be deterministic and fast.</li> <li>Mocking OpenSlide allows us to test our responsibilities:</li> <li>Parameter validation</li> <li>Error wrapping (<code>WSIOpenError</code> / <code>WSIReadError</code>)</li> <li>Correct coordinate conventions (Level-0 location, level-space size)</li> <li>RGBA \u2192 RGB conversion</li> <li>Crop resize math + Base64 encoding</li> </ul> <p>Integration tests should cover the real stack (BUG-004), not replace unit tests.</p>"},{"location":"archive/bugs/BUG-007-entire-test-suite-mocked/#examples-of-non-mocked-real-behavior-tests","title":"Examples of non-mocked \u201creal behavior\u201d tests","text":"<ul> <li><code>tests/unit/core/test_level_selector.py</code>: tests level-selection math directly.</li> <li><code>tests/unit/geometry/test_primitives.py</code>: tests Pydantic validation behavior.</li> <li><code>tests/unit/geometry/test_transforms.py</code>: tests coordinate transform math.</li> <li><code>tests/unit/geometry/test_validators.py</code>: tests bounds validation/clamping behavior.</li> <li><code>tests/unit/core/test_crop_engine.py</code>: exercises PIL resizing and verifies Base64 decodes to a JPEG image.</li> </ul>"},{"location":"archive/bugs/BUG-008-api-keys-silent-none/","title":"BUG-008: API Keys Are Silently None - No Validation","text":""},{"location":"archive/bugs/BUG-008-api-keys-silent-none/#severity-p3-low-priority-devex-future-spec-06","title":"Severity: P3 (Low Priority) - DevEx / Future Spec-06","text":""},{"location":"archive/bugs/BUG-008-api-keys-silent-none/#status-fixed-require__key-methods-added","title":"Status: Fixed - require_*_key() methods added","text":""},{"location":"archive/bugs/BUG-008-api-keys-silent-none/#description","title":"Description","text":"<p>API keys in <code>config.py</code> default to <code>None</code>. This is a reasonable default (not every command needs every key), but once LLM provider implementations (Spec-06) or CLI commands (Spec-12) require a key, we should raise a clear, GIANT-specific error at the use site.</p>"},{"location":"archive/bugs/BUG-008-api-keys-silent-none/#current-code","title":"Current Code","text":"<pre><code># src/giant/config.py:19-22\nclass Settings(BaseSettings):\n    # API Keys\n    OPENAI_API_KEY: str | None = None      # \u2190 Silently None\n    ANTHROPIC_API_KEY: str | None = None   # \u2190 Silently None\n    HUGGINGFACE_TOKEN: str | None = None   # \u2190 Silently None\n</code></pre>"},{"location":"archive/bugs/BUG-008-api-keys-silent-none/#what-happens-when-none","title":"What Happens When None","text":"<p>When Spec-06+ implements the LLM agent loop:</p> <pre><code># Future code (Spec-06)\nfrom giant.config import settings\n\nclient = openai.OpenAI(api_key=settings.OPENAI_API_KEY)  # api_key=None\nresponse = client.chat.completions.create(...)  # Cryptic auth error\n</code></pre> <p>Today, this is not a runtime bug because LLM providers are not implemented yet. The future improvement is to raise something like:</p> <pre><code>giant.ConfigError: OPENAI_API_KEY not set. Set it in .env or environment.\n</code></pre>"},{"location":"archive/bugs/BUG-008-api-keys-silent-none/#expected-behavior","title":"Expected Behavior","text":"<p>Option A: Validate at the provider boundary (recommended) <pre><code>@field_validator(\"OPENAI_API_KEY\")\n@classmethod\ndef validate_api_key(cls, v: str | None) -&gt; str | None:\n    # Only validate when running LLM commands\n    return v\n\ndef require_openai_key(self) -&gt; str:\n    \"\"\"Get OpenAI key, raising clear error if not set.\"\"\"\n    if self.OPENAI_API_KEY is None:\n        raise ConfigError(\n            \"OPENAI_API_KEY not configured. \"\n            \"Set it in .env file or OPENAI_API_KEY environment variable.\"\n        )\n    return self.OPENAI_API_KEY\n</code></pre></p> <p>Option B: Warn at startup (optional) <pre><code>def __init__(self, **kwargs):\n    super().__init__(**kwargs)\n    # Warn about missing keys at startup\n    if self.OPENAI_API_KEY is None:\n        logger.warning(\"OPENAI_API_KEY not set - LLM features will fail\")\n</code></pre></p>"},{"location":"archive/bugs/BUG-008-api-keys-silent-none/#similar-issues","title":"Similar Issues","text":"<ul> <li><code>HUGGINGFACE_TOKEN</code> in <code>download.py:38</code>: <code>token=settings.HUGGINGFACE_TOKEN or None</code></li> <li>This silently downloads without auth if token is not set</li> <li>Some HuggingFace repos require auth and will fail cryptically</li> </ul>"},{"location":"archive/bugs/BUG-008-api-keys-silent-none/#code-location","title":"Code Location","text":"<ul> <li><code>src/giant/config.py:19-22</code> - Nullable API keys with no validation</li> <li><code>src/giant/data/download.py:38</code> - Silently uses None token</li> </ul>"},{"location":"archive/bugs/BUG-008-api-keys-silent-none/#impact","title":"Impact","text":"<ul> <li>Minor DevEx friction: users may hit provider-SDK auth errors instead of GIANT\u2019s own actionable message.</li> <li>Not a production correctness issue as long as we validate before making paid API calls.</li> </ul>"},{"location":"archive/bugs/BUG-008-api-keys-silent-none/#testing-required","title":"Testing Required","text":"<ul> <li>Unit test: Verify clear error when API key needed but missing</li> <li>Unit test: Verify warning logged when keys not set at startup</li> </ul>"},{"location":"archive/bugs/BUG-008-api-keys-silent-none/#resolution","title":"Resolution","text":"<p>Implemented: Option A (validate at the provider boundary)</p> <p>Added <code>ConfigError</code> exception and <code>require_*_key()</code> methods to <code>Settings</code> class:</p>"},{"location":"archive/bugs/BUG-008-api-keys-silent-none/#new-code","title":"New Code","text":"<pre><code># src/giant/config.py\n\nclass ConfigError(Exception):\n    \"\"\"Raised when required configuration is missing or invalid.\"\"\"\n    def __init__(self, key_name: str, env_var: str) -&gt; None:\n        self.key_name = key_name\n        self.env_var = env_var\n        message = (\n            f\"{key_name} not configured. \"\n            f\"Set it in .env file or {env_var} environment variable.\"\n        )\n        super().__init__(message)\n\nclass Settings(BaseSettings):\n    # ... existing settings ...\n\n    def require_openai_key(self) -&gt; str:\n        \"\"\"Get OpenAI API key, raising ConfigError if not set.\"\"\"\n        if self.OPENAI_API_KEY is None:\n            raise ConfigError(\"OpenAI API key\", \"OPENAI_API_KEY\")\n        return self.OPENAI_API_KEY\n\n    def require_anthropic_key(self) -&gt; str:\n        \"\"\"Get Anthropic API key, raising ConfigError if not set.\"\"\"\n        if self.ANTHROPIC_API_KEY is None:\n            raise ConfigError(\"Anthropic API key\", \"ANTHROPIC_API_KEY\")\n        return self.ANTHROPIC_API_KEY\n\n    def require_huggingface_token(self) -&gt; str:\n        \"\"\"Get HuggingFace token, raising ConfigError if not set.\"\"\"\n        if self.HUGGINGFACE_TOKEN is None:\n            raise ConfigError(\"HuggingFace token\", \"HUGGINGFACE_TOKEN\")\n        return self.HUGGINGFACE_TOKEN\n</code></pre>"},{"location":"archive/bugs/BUG-008-api-keys-silent-none/#tests-added","title":"Tests Added","text":"<p><code>tests/unit/test_config.py</code>: - <code>TestConfigError</code> - message format, attributes - <code>TestRequireMethods</code> - 6 tests covering when keys are set/missing</p>"},{"location":"archive/bugs/BUG-008-api-keys-silent-none/#usage-spec-06","title":"Usage (Spec-06+)","text":"<pre><code>from giant.config import settings\n\n# Before making API calls:\napi_key = settings.require_openai_key()  # Raises ConfigError if not set\nclient = openai.OpenAI(api_key=api_key)\n</code></pre>"},{"location":"archive/bugs/BUG-009-font-loading-silent-fallback/","title":"BUG-009: Font Loading Has Silent Fallback - No Warning","text":""},{"location":"archive/bugs/BUG-009-font-loading-silent-fallback/#severity-p3-low-priority-devex","title":"Severity: P3 (Low Priority) - DevEx","text":""},{"location":"archive/bugs/BUG-009-font-loading-silent-fallback/#status-fixed","title":"Status: Fixed","text":""},{"location":"archive/bugs/BUG-009-font-loading-silent-fallback/#description","title":"Description","text":"<p>The <code>AxisGuideGenerator._get_font()</code> method silently falls back to PIL's default bitmap font when TrueType fonts aren't available. No warning is logged, so users don't know their overlay text might look terrible.</p>"},{"location":"archive/bugs/BUG-009-font-loading-silent-fallback/#original-code-before-fix","title":"Original Code (Before Fix)","text":"<pre><code># src/giant/geometry/overlay.py (before fix)\ndef _get_font(self) -&gt; ImageFont.FreeTypeFont | ImageFont.ImageFont:\n    \"\"\"Get font for labels, with fallback to default.\"\"\"\n    try:\n        return ImageFont.truetype(\"DejaVuSans.ttf\", self.style.font_size)\n    except OSError:\n        try:\n            return ImageFont.truetype(\"Arial.ttf\", self.style.font_size)\n        except OSError:\n            # SILENT FALLBACK - NO WARNING!\n            return ImageFont.load_default()\n</code></pre>"},{"location":"archive/bugs/BUG-009-font-loading-silent-fallback/#problem","title":"Problem","text":"<ol> <li>No logging: User doesn't know fonts failed to load</li> <li>Quality degradation: PIL's default font is low-resolution bitmap</li> <li>font_size ignored: <code>ImageFont.load_default()</code> ignores the configured <code>font_size</code></li> <li>Silent degradation: Overlays look bad with no indication why</li> </ol>"},{"location":"archive/bugs/BUG-009-font-loading-silent-fallback/#expected-behavior","title":"Expected Behavior","text":"<pre><code>def _get_font(self) -&gt; ImageFont.FreeTypeFont | ImageFont.ImageFont:\n    \"\"\"Get font for labels, with fallback to default.\"\"\"\n    try:\n        return ImageFont.truetype(\"DejaVuSans.ttf\", self.style.font_size)\n    except OSError:\n        try:\n            return ImageFont.truetype(\"Arial.ttf\", self.style.font_size)\n        except OSError:\n            logger.warning(\n                \"No TrueType fonts available (DejaVuSans.ttf, Arial.ttf). \"\n                \"Using low-resolution default font. Install fonts for better quality.\"\n            )\n            return ImageFont.load_default()\n</code></pre>"},{"location":"archive/bugs/BUG-009-font-loading-silent-fallback/#impact","title":"Impact","text":"<ul> <li>Cosmetic/DevEx: overlay labels may look pixelated with no indication why.</li> </ul>"},{"location":"archive/bugs/BUG-009-font-loading-silent-fallback/#code-location","title":"Code Location","text":"<ul> <li><code>src/giant/geometry/overlay.py:176-196</code> - <code>_get_font()</code> method</li> </ul>"},{"location":"archive/bugs/BUG-009-font-loading-silent-fallback/#resolution","title":"Resolution","text":"<p>Implemented the expected behavior. The fallback now logs a warning:</p> <pre><code># src/giant/geometry/overlay.py:176-196 (after fix)\ndef _get_font(self) -&gt; ImageFont.FreeTypeFont | ImageFont.ImageFont:\n    \"\"\"Get font for labels, with fallback to default.\"\"\"\n    try:\n        return ImageFont.truetype(\"DejaVuSans.ttf\", self.style.font_size)\n    except OSError:\n        try:\n            return ImageFont.truetype(\"Arial.ttf\", self.style.font_size)\n        except OSError:\n            # Fall back to PIL's default font\n            logger.warning(\n                \"No TrueType fonts available (DejaVuSans.ttf, Arial.ttf). \"\n                \"Using low-resolution default font. Install fonts for better \"\n                \"quality.\"\n            )\n            return ImageFont.load_default()\n</code></pre>"},{"location":"archive/bugs/BUG-009-font-loading-silent-fallback/#testing","title":"Testing","text":"<ul> <li>\u2705 Existing overlay unit tests pass</li> <li>\u2705 Warning is logged when fallback occurs</li> </ul>"},{"location":"archive/bugs/BUG-010-mpp-nullable-no-guards/","title":"BUG-010: MPP (Microns Per Pixel) Is Nullable With No Guards","text":""},{"location":"archive/bugs/BUG-010-mpp-nullable-no-guards/#severity-p3-low-priority-future-proofing","title":"Severity: P3 (Low Priority) - Future-Proofing","text":""},{"location":"archive/bugs/BUG-010-mpp-nullable-no-guards/#status-archived-2025-12-19-not-an-active-bug","title":"Status: ARCHIVED (2025-12-19) - Not an active bug","text":""},{"location":"archive/bugs/BUG-010-mpp-nullable-no-guards/#archive-reason","title":"Archive Reason","text":"<p>This is not an active bug. Per the original description: \"Today, no production code uses MPP values, so there is no active bug.\"</p> <p>This document serves as a future-proofing reminder for when physical-unit features are implemented. The nullable <code>mpp_x</code>/<code>mpp_y</code> fields are correct by design (many slides lack calibration data). The mitigation patterns below should be applied when physical measurements are added.</p>"},{"location":"archive/bugs/BUG-010-mpp-nullable-no-guards/#description","title":"Description","text":"<p><code>WSIMetadata.mpp_x</code> and <code>mpp_y</code> are <code>float | None</code> to support slides without calibration data. This is correct. Today, no production code uses MPP values, so there is no active bug \u2014 this is a reminder to handle <code>None</code> safely once physical-unit features are implemented.</p>"},{"location":"archive/bugs/BUG-010-mpp-nullable-no-guards/#current-code","title":"Current Code","text":"<pre><code># src/giant/wsi/types.py:45-46\n@dataclass(frozen=True)\nclass WSIMetadata:\n    # ...\n    mpp_x: float | None   # \u2190 Can be None\n    mpp_y: float | None   # \u2190 Can be None\n</code></pre>"},{"location":"archive/bugs/BUG-010-mpp-nullable-no-guards/#the-time-bomb","title":"The Time Bomb","text":"<p>When Spec-XX implements physical distance calculations (e.g., measuring tumor size):</p> <pre><code># Future code\ndef measure_physical_size(region: Region, metadata: WSIMetadata) -&gt; float:\n    \"\"\"Calculate physical size in millimeters.\"\"\"\n    # THIS WILL CRASH when mpp_x is None\n    width_mm = region.width * metadata.mpp_x / 1000  # TypeError: unsupported operand\n</code></pre>"},{"location":"archive/bugs/BUG-010-mpp-nullable-no-guards/#current-usage","title":"Current Usage","text":"<p>The current codebase doesn't use MPP for calculations. It becomes relevant when: 1. Physical measurements are added (Spec-XX) 2. User opens a slide without MPP data (common for TIFF files) 3. Code tries to multiply by <code>None</code></p>"},{"location":"archive/bugs/BUG-010-mpp-nullable-no-guards/#evidence-in-tests","title":"Evidence in Tests","text":"<pre><code># tests/unit/wsi/test_types.py:81-95\ndef test_metadata_with_none_mpp(self) -&gt; None:\n    \"\"\"Test metadata can be created with None MPP values.\"\"\"\n    metadata = WSIMetadata(\n        # ...\n        mpp_x=None,  # \u2190 Test proves None is valid\n        mpp_y=None,\n    )\n    assert metadata.mpp_x is None  # \u2190 But no test for USING None safely\n</code></pre>"},{"location":"archive/bugs/BUG-010-mpp-nullable-no-guards/#expected-pattern","title":"Expected Pattern","text":"<pre><code>def measure_physical_size(region: Region, metadata: WSIMetadata) -&gt; float | None:\n    \"\"\"Calculate physical size in millimeters, or None if uncalibrated.\"\"\"\n    if metadata.mpp_x is None or metadata.mpp_y is None:\n        logger.warning(\n            \"Slide has no MPP calibration data. Physical measurements unavailable.\",\n            path=metadata.path,\n        )\n        return None\n\n    width_mm = region.width * metadata.mpp_x / 1000\n    # ...\n</code></pre>"},{"location":"archive/bugs/BUG-010-mpp-nullable-no-guards/#slides-without-mpp","title":"Slides Without MPP","text":"Vendor MPP Available Notes Aperio (.svs) Usually yes Stored in properties Hamamatsu (.ndpi) Usually yes Stored in properties Generic TIFF Usually NO No standard metadata DICOM Yes In DICOM tags"},{"location":"archive/bugs/BUG-010-mpp-nullable-no-guards/#impact","title":"Impact","text":"<ul> <li>Future correctness risk if physical-unit features assume MPP is always present.</li> </ul>"},{"location":"archive/bugs/BUG-010-mpp-nullable-no-guards/#code-location","title":"Code Location","text":"<ul> <li><code>src/giant/wsi/types.py:45-46</code> - Nullable MPP definition</li> <li>Future specs that will use MPP for measurements</li> </ul>"},{"location":"archive/bugs/BUG-010-mpp-nullable-no-guards/#mitigation","title":"Mitigation","text":"<p>When physical measurements are implemented, add a helper method to WSIMetadata (or a dedicated measurement service) to centralize the guard:</p> <pre><code>def get_mpp(self) -&gt; tuple[float, float]:\n    \"\"\"Get MPP values, raising if uncalibrated.\"\"\"\n    if self.mpp_x is None or self.mpp_y is None:\n        raise ValueError(\n            f\"Slide {self.path} has no MPP calibration data. \"\n            \"Physical measurements are not available.\"\n        )\n    return (self.mpp_x, self.mpp_y)\n\ndef has_calibration(self) -&gt; bool:\n    \"\"\"Check if slide has MPP calibration data.\"\"\"\n    return self.mpp_x is not None and self.mpp_y is not None\n</code></pre>"},{"location":"archive/bugs/BUG-011-unused-geometry-validator/","title":"BUG-011: GeometryValidator Exists But Is Never Used","text":""},{"location":"archive/bugs/BUG-011-unused-geometry-validator/#severity-p3-low-priority-staged-for-spec-09","title":"Severity: P3 (Low Priority) - Staged for Spec-09","text":""},{"location":"archive/bugs/BUG-011-unused-geometry-validator/#status-resolved-archived-2025-12-19","title":"Status: RESOLVED (Archived 2025-12-19)","text":""},{"location":"archive/bugs/BUG-011-unused-geometry-validator/#resolution","title":"Resolution","text":"<p>Fixed in Spec-09 implementation. The <code>GeometryValidator</code> is now fully integrated into <code>GIANTAgent.run()</code>:</p> <ul> <li>Imported at <code>src/giant/agent/runner.py:33</code></li> <li>Used for strict validation at lines 325-329:</li> </ul> <pre><code>try:\n    self._validator.validate(region, self._slide_bounds, strict=True)\nexcept ValidationError as e:\n    logger.warning(\"Invalid crop region: %s\", e)\n    return await self._handle_invalid_region(action, messages, str(e))\n</code></pre> <p>This follows Spec-09's design: \"strict by default; clamp only as an explicit, test-covered recovery path.\"</p>"},{"location":"archive/bugs/BUG-011-unused-geometry-validator/#original-description-historical","title":"Original Description (Historical)","text":"<p><code>GeometryValidator</code> is currently unused in production code, but this is intentional: Spec-09 explicitly places bbox validation in the agent loop (\"strict by default; clamp only as an explicit, test-covered recovery path\"). Until Spec-09 is implemented, <code>GeometryValidator</code> is a staged utility.</p>"},{"location":"archive/bugs/BUG-011-unused-geometry-validator/#the-unused-code","title":"The Unused Code","text":"<pre><code># src/giant/geometry/validators.py - staged utility (used by Spec-09)\nclass GeometryValidator:\n    def validate(self, region: Region, bounds: Size, *, strict: bool = True) -&gt; bool:\n        \"\"\"Validate that a region is within bounds.\"\"\"\n        # FULLY IMPLEMENTED, NEVER CALLED\n\n    def clamp_region(self, region: Region, bounds: Size) -&gt; Region:\n        \"\"\"Clamp a region to valid bounds.\"\"\"\n        # FULLY IMPLEMENTED, NEVER CALLED\n</code></pre>"},{"location":"archive/bugs/BUG-011-unused-geometry-validator/#where-it-should-be-used","title":"Where It Should Be Used","text":"<pre><code># src/giant/core/crop_engine.py:102-165\ndef crop(self, region: Region, ...) -&gt; CroppedImage:\n    # NO VALIDATION HERE!\n    metadata = self._reader.get_metadata()\n    # region could be outside slide bounds - no check!\n\n    # SHOULD BE:\n    # bounds = Size(width=metadata.width, height=metadata.height)\n    # validator = GeometryValidator()\n    # if not validator.is_within_bounds(region, bounds):\n    #     region = validator.clamp_region(region, bounds)\n</code></pre>"},{"location":"archive/bugs/BUG-011-unused-geometry-validator/#evidence","title":"Evidence","text":"<pre><code># Search for GeometryValidator usage in production code\ngrep -r \"GeometryValidator\" src/\n# Only found in: src/giant/geometry/__init__.py (export)\n#                src/giant/geometry/validators.py (definition)\n# NOT found in: src/giant/core/crop_engine.py\n</code></pre>"},{"location":"archive/bugs/BUG-011-unused-geometry-validator/#why-this-is-bad","title":"Why This Is Bad","text":"<p>This is scaffolding for Spec-09. The tests are still valuable because: 1. They lock down bounds semantics before the LLM loop arrives. 2. They provide a known-correct component for error recovery once the agent is built.</p>"},{"location":"archive/bugs/BUG-011-unused-geometry-validator/#the-disconnect-expected-until-spec-09","title":"The Disconnect (Expected Until Spec-09)","text":"Component Status <code>GeometryValidator</code> class \u2713 Implemented <code>GeometryValidator</code> tests \u2713 26 tests pass <code>CropEngine</code> uses validator \u2717 Never integrated Agent error recovery (Spec-09) \u2717 Not implemented (yet)"},{"location":"archive/bugs/BUG-011-unused-geometry-validator/#impact","title":"Impact","text":"<ul> <li>Until Spec-09 is implemented, invalid bbox handling lives in OpenSlide\u2019s padding behavior (out-of-bounds pixels \u2192 transparent \u2192 black after RGB conversion).</li> <li>Once Spec-09 lands, <code>GeometryValidator</code> becomes the single source of truth for bounds validation and recovery policy.</li> </ul>"},{"location":"archive/bugs/BUG-011-unused-geometry-validator/#code-location","title":"Code Location","text":"<ul> <li><code>src/giant/geometry/validators.py</code> - Staged validation/clamping utility</li> <li><code>docs/specs/spec-09-giant-agent.md</code> - Defines how it\u2019s used (strict by default)</li> <li><code>tests/unit/geometry/test_validators.py</code> - Unit tests for bounds semantics</li> </ul>"},{"location":"archive/bugs/BUG-011-unused-geometry-validator/#fix-required","title":"Fix Required","text":"<p>Either: 1. Leave as-is (recommended): integrate in Spec-09 as designed. 2. Document the staging: keep this as a known \u201cwill be used by Spec-09\u201d utility.</p>"},{"location":"archive/bugs/BUG-012-download-silent-auth/","title":"BUG-012: HuggingFace Download Silently Uses No Auth","text":""},{"location":"archive/bugs/BUG-012-download-silent-auth/#severity-p3-low-priority-devex","title":"Severity: P3 (Low Priority) - DevEx","text":""},{"location":"archive/bugs/BUG-012-download-silent-auth/#status-fixed","title":"Status: Fixed","text":""},{"location":"archive/bugs/BUG-012-download-silent-auth/#description","title":"Description","text":"<p><code>download_multipathqa_metadata()</code> proceeds without authentication when <code>HUGGINGFACE_TOKEN</code> is not set. For public repos this is correct; for private/gated repos it will fail with an auth error from <code>huggingface_hub</code>.</p>"},{"location":"archive/bugs/BUG-012-download-silent-auth/#original-code-before-fix","title":"Original Code (Before Fix)","text":"<pre><code># src/giant/data/download.py (before fix)\ndef download_multipathqa_metadata(\n    output_dir: Path = DEFAULT_MULTIPATHQA_DIR,\n    *,\n    force: bool = False,\n) -&gt; Path:\n    csv_path = hf_hub_download(\n        repo_id=MULTIPATHQA_REPO_ID,\n        filename=MULTIPATHQA_CSV_FILENAME,\n        repo_type=\"dataset\",\n        local_dir=output_dir,\n        force_download=force,\n        token=settings.HUGGINGFACE_TOKEN or None,  # \u2190 Silently None\n    )\n</code></pre>"},{"location":"archive/bugs/BUG-012-download-silent-auth/#problems","title":"Problems","text":"<ol> <li>Redundant code: <code>settings.HUGGINGFACE_TOKEN or None</code> - the token is already <code>None</code> if not set</li> <li>Silent behavior: No log message about auth status</li> <li>Confusing failures: If the repo becomes gated, users get \"401 Unauthorized\" with no guidance</li> </ol>"},{"location":"archive/bugs/BUG-012-download-silent-auth/#expected-behavior","title":"Expected Behavior","text":"<pre><code>def download_multipathqa_metadata(...) -&gt; Path:\n    token = settings.HUGGINGFACE_TOKEN\n    if token is None:\n        logger.debug(\n            \"HUGGINGFACE_TOKEN not set, using anonymous access. \"\n            \"Set token in .env for private/gated datasets.\"\n        )\n\n    csv_path = hf_hub_download(\n        ...\n        token=token,\n    )\n</code></pre>"},{"location":"archive/bugs/BUG-012-download-silent-auth/#impact","title":"Impact","text":"<ul> <li>Minor DevEx: when a dataset becomes gated, users may not realize they need <code>HUGGINGFACE_TOKEN</code> until an auth error occurs.</li> </ul>"},{"location":"archive/bugs/BUG-012-download-silent-auth/#code-location","title":"Code Location","text":"<ul> <li><code>src/giant/data/download.py:28-51</code> - <code>download_multipathqa_metadata()</code> function</li> </ul>"},{"location":"archive/bugs/BUG-012-download-silent-auth/#resolution","title":"Resolution","text":"<p>Implemented the expected behavior. The function now logs at DEBUG level when no token is set:</p> <pre><code># src/giant/data/download.py:28-51 (after fix)\ndef download_multipathqa_metadata(\n    output_dir: Path = DEFAULT_MULTIPATHQA_DIR,\n    *,\n    force: bool = False,\n) -&gt; Path:\n    \"\"\"Download MultiPathQA metadata CSV to the local `data/` directory.\"\"\"\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    token = settings.HUGGINGFACE_TOKEN\n    if token is None:\n        logger.debug(\n            \"HUGGINGFACE_TOKEN not set, using anonymous access. \"\n            \"Set token in .env for private/gated datasets.\"\n        )\n\n    csv_path = hf_hub_download(\n        repo_id=MULTIPATHQA_REPO_ID,\n        filename=MULTIPATHQA_CSV_FILENAME,\n        repo_type=\"dataset\",\n        local_dir=output_dir,\n        force_download=force,\n        token=token,\n    )\n    return Path(csv_path)\n</code></pre>"},{"location":"archive/bugs/BUG-012-download-silent-auth/#testing","title":"Testing","text":"<ul> <li>\u2705 Existing download unit tests pass</li> <li>\u2705 Debug log emitted when token is None</li> </ul>"},{"location":"archive/bugs/BUG-013-silent-zero-cost-on-missing-usage/","title":"BUG-013: Silent Zero-Cost Reporting on Missing Usage Data","text":""},{"location":"archive/bugs/BUG-013-silent-zero-cost-on-missing-usage/#severity-p1-high-priority-cost-tracking-integrity","title":"Severity: P1 (High Priority) - Cost Tracking Integrity","text":""},{"location":"archive/bugs/BUG-013-silent-zero-cost-on-missing-usage/#status-closed-fixed","title":"Status: Closed (Fixed)","text":""},{"location":"archive/bugs/BUG-013-silent-zero-cost-on-missing-usage/#description","title":"Description","text":"<p>Both OpenAI and Anthropic providers have defensive None checks for usage data that silently report 0 tokens and $0.00 cost if the SDK ever returns None usage. This breaks cost tracking without any error or warning.</p>"},{"location":"archive/bugs/BUG-013-silent-zero-cost-on-missing-usage/#current-code","title":"Current Code","text":"<pre><code># src/giant/llm/openai_client.py:204-208\nusage = response.usage\nprompt_tokens = usage.input_tokens if usage else 0\ncompletion_tokens = usage.output_tokens if usage else 0\ntotal_tokens = prompt_tokens + completion_tokens\n\n# src/giant/llm/anthropic_client.py:224-228\n# Same pattern with comment \"defensive None check for SDK edge cases\"\nusage = response.usage\nprompt_tokens = usage.input_tokens if usage else 0\ncompletion_tokens = usage.output_tokens if usage else 0\n</code></pre>"},{"location":"archive/bugs/BUG-013-silent-zero-cost-on-missing-usage/#why-this-is-bad","title":"Why This Is Bad","text":"<ol> <li>Silent Data Loss: If SDK returns None usage (API change, SDK bug, etc.), cost tracking silently breaks</li> <li>No Warning: User sees $0.00 cost with no indication something is wrong</li> <li>Budget Guardrails Fail: Spec-09's <code>budget_usd</code> feature depends on accurate cost tracking</li> <li>Production Money Burn: Could spend real money while reporting $0.00 spent</li> </ol>"},{"location":"archive/bugs/BUG-013-silent-zero-cost-on-missing-usage/#expected-behavior","title":"Expected Behavior","text":"<p>Either: 1. Raise Error: Usage data is required for cost tracking - fail loudly 2. Log Warning: Accept zero but log a clear warning that usage was missing</p>"},{"location":"archive/bugs/BUG-013-silent-zero-cost-on-missing-usage/#proposed-fix","title":"Proposed Fix","text":"<pre><code># Option A: Fail fast (implemented)\nusage = response.usage\nif usage is None:\n    raise LLMError(\n        \"API response missing usage data - cannot track costs\",\n        provider=\"openai\",\n        model=self.model,\n    )\nprompt_tokens = usage.input_tokens\ncompletion_tokens = usage.output_tokens\n\n# Option B: Warn but continue (if SDK None is expected edge case)\nusage = response.usage\nif usage is None:\n    logger.warning(\n        \"API response missing usage data, reporting 0 tokens\",\n        provider=\"openai\",\n        model=self.model,\n    )\n    prompt_tokens = 0\n    completion_tokens = 0\nelse:\n    prompt_tokens = usage.input_tokens\n    completion_tokens = usage.output_tokens\n</code></pre>"},{"location":"archive/bugs/BUG-013-silent-zero-cost-on-missing-usage/#impact","title":"Impact","text":"<ul> <li>P0 \u2192 P1: Downgraded from P0 because SDKs reliably return usage today</li> <li>Cost tracking integrity is critical for Spec-09's budget guardrails</li> <li>Budget overflow could cause unlimited spend if this triggers</li> </ul>"},{"location":"archive/bugs/BUG-013-silent-zero-cost-on-missing-usage/#code-location","title":"Code Location","text":"<ul> <li><code>src/giant/llm/openai_client.py</code></li> <li><code>src/giant/llm/anthropic_client.py</code></li> </ul>"},{"location":"archive/bugs/BUG-013-silent-zero-cost-on-missing-usage/#test-evidence","title":"Test Evidence","text":"<p>Added integration regression tests to ensure we fail loudly when usage is missing:</p> <ul> <li><code>tests/integration/llm/test_p1_high_priority.py</code></li> <li><code>TestP1_7_CostTracking.test_openai_missing_usage_raises</code></li> <li><code>TestP1_7_CostTracking.test_anthropic_missing_usage_raises</code></li> </ul>"},{"location":"archive/bugs/BUG-013-silent-zero-cost-on-missing-usage/#mitigation","title":"Mitigation","text":"<p>Live tests still help verify SDK/API behavior, but missing usage now fails fast to prevent silent under-reporting.</p>"},{"location":"archive/bugs/BUG-014-env-secrets-management/","title":"BUG-014: Environment Secrets Management Gap","text":""},{"location":"archive/bugs/BUG-014-env-secrets-management/#severity-p1-high-priority-security-configuration-integrity","title":"Severity: P1 (High Priority) - Security &amp; Configuration Integrity","text":""},{"location":"archive/bugs/BUG-014-env-secrets-management/#status-closed-fixed","title":"Status: Closed (Fixed)","text":""},{"location":"archive/bugs/BUG-014-env-secrets-management/#summary","title":"Summary","text":"<p>The project has a <code>.env.example</code> template but it's incomplete, and there's no documentation explaining: 1. Which API keys are required vs optional 2. How tests pick up API keys (shell env vs <code>.env</code> file) 3. What Google/Gemini integration status is 4. How to safely run live tests without accidentally hitting APIs</p> <p>This caused confusion when a live Anthropic test ran unexpectedly because <code>ANTHROPIC_API_KEY</code> was exported in the user's shell environment.</p>"},{"location":"archive/bugs/BUG-014-env-secrets-management/#current-state-analysis","title":"Current State Analysis","text":""},{"location":"archive/bugs/BUG-014-env-secrets-management/#1-envexample-is-incomplete","title":"1. <code>.env.example</code> Is Incomplete","text":"<pre><code># Current .env.example (only 4 lines)\nOPENAI_API_KEY=\nANTHROPIC_API_KEY=\nHUGGINGFACE_TOKEN=\nLOG_LEVEL=INFO\n</code></pre> <p>Missing: - <code>GOOGLE_API_KEY</code> - Gemini is in the model registry but no config support - Documentation comments explaining each key - Guidance on which keys are required for what</p>"},{"location":"archive/bugs/BUG-014-env-secrets-management/#2-model-registry-vs-config-mismatch","title":"2. Model Registry vs Config Mismatch","text":"Provider In Model Registry In Config Provider Client OpenAI <code>gpt-5.2</code> <code>OPENAI_API_KEY</code> <code>openai_client.py</code> Anthropic <code>claude-sonnet-4-5-20250929</code> <code>ANTHROPIC_API_KEY</code> <code>anthropic_client.py</code> Google <code>gemini-3-pro-preview</code> MISSING MISSING <p>Issue: Google/Gemini models are in <code>model_registry.py</code> and <code>pricing.py</code> but: - No <code>GOOGLE_API_KEY</code> in <code>config.py</code> - No <code>require_google_key()</code> method - No <code>google_client.py</code> provider implementation - This is scaffolding for future work, but it's misleading</p>"},{"location":"archive/bugs/BUG-014-env-secrets-management/#3-huggingface-token-usage","title":"3. HuggingFace Token Usage","text":"<pre><code># src/giant/data/download.py:36-41\ntoken = settings.HUGGINGFACE_TOKEN\nif token is None:\n    logger.debug(\n        \"HUGGINGFACE_TOKEN not set, using anonymous access. \"\n        \"Set token in .env for private/gated datasets.\"\n    )\n</code></pre> <p>Status: Optional. Only needed for gated/private datasets. MultiPathQA is public.</p>"},{"location":"archive/bugs/BUG-014-env-secrets-management/#4-test-environment-behavior","title":"4. Test Environment Behavior","text":"<p>The live tests use <code>os.getenv()</code> which reads from: 1. Shell environment variables (<code>~/.zshrc</code>, <code>~/.bashrc</code>, etc.) 2. <code>.env</code> file (via pydantic-settings)</p> <p>Problem: If you have <code>ANTHROPIC_API_KEY</code> exported in your shell (for other projects), the live tests run automatically and hit real APIs with real costs.</p> <pre><code># tests/integration/llm/test_p0_critical.py:1117-1119\n@pytest.mark.skipif(\n    not os.getenv(\"ANTHROPIC_API_KEY\"),  # Reads from shell OR .env\n    reason=\"ANTHROPIC_API_KEY not set\",\n)\n</code></pre>"},{"location":"archive/bugs/BUG-014-env-secrets-management/#what-we-need","title":"What We Need","text":""},{"location":"archive/bugs/BUG-014-env-secrets-management/#a-complete-envexample","title":"A. Complete <code>.env.example</code>","text":"<pre><code># =============================================================================\n# GIANT WSI + LLM Pipeline - Environment Configuration\n# =============================================================================\n# Copy to .env and fill in values. The .env file is gitignored.\n#\n# REQUIRED FOR LIVE TESTS:\n#   - OPENAI_API_KEY: Required for OpenAI live tests\n#   - ANTHROPIC_API_KEY: Required for Anthropic live tests\n#\n# OPTIONAL:\n#   - HUGGINGFACE_TOKEN: Only for gated/private datasets (MultiPathQA is public)\n#   - GOOGLE_API_KEY: Reserved for future Gemini integration (not implemented)\n#\n# =============================================================================\n\n# --- LLM Provider API Keys ---\n# Get from: https://platform.openai.com/api-keys\nOPENAI_API_KEY=\n\n# Get from: https://console.anthropic.com/settings/keys\nANTHROPIC_API_KEY=\n\n# Reserved for future Gemini integration (Spec-XX)\n# GOOGLE_API_KEY=\n\n# --- Data Access ---\n# Get from: https://huggingface.co/settings/tokens\n# Optional - only needed for gated/private HuggingFace datasets\nHUGGINGFACE_TOKEN=\n\n# --- Logging ---\n# Options: DEBUG, INFO, WARNING, ERROR\nLOG_LEVEL=INFO\n# Options: console, json\nLOG_FORMAT=console\n</code></pre>"},{"location":"archive/bugs/BUG-014-env-secrets-management/#b-configpy-updates-needed","title":"B. Config.py Updates Needed","text":"<ol> <li>Add <code>GOOGLE_API_KEY</code> placeholder (commented as future)</li> <li>OR remove Google from model_registry until implemented</li> <li>Document the precedence: shell env \u2192 .env file</li> </ol>"},{"location":"archive/bugs/BUG-014-env-secrets-management/#c-test-documentation","title":"C. Test Documentation","text":"<p>Update spec-08.5 to clearly explain: <pre><code>## Running Tests\n\n### Mock Tests (Default, No API Keys Needed)\n```bash\nuv run pytest tests/integration/llm/ -m mock\n</code></pre></p>"},{"location":"archive/bugs/BUG-014-env-secrets-management/#live-tests-requires-api-keys-in-env","title":"Live Tests (Requires API Keys in .env)","text":"<pre><code># Create .env from template\ncp .env.example .env\n# Edit .env with your keys\n\n# Run live tests (costs money!)\nuv run pytest tests/integration/llm/ -m live -v\n</code></pre>"},{"location":"archive/bugs/BUG-014-env-secrets-management/#warning-shell-environment","title":"WARNING: Shell Environment","text":"<p>If you have API keys exported in your shell (<code>~/.zshrc</code>), live tests will run automatically. Use <code>-m mock</code> to avoid this. ```</p>"},{"location":"archive/bugs/BUG-014-env-secrets-management/#spec-analysis-gemini-intent","title":"Spec Analysis - Gemini Intent","text":""},{"location":"archive/bugs/BUG-014-env-secrets-management/#what-the-specs-say","title":"What the Specs Say","text":"Document Gemini Status <code>spec-06-llm-provider.md</code> Gemini in pricing example, NOT in acceptance criteria <code>spec-08.5-llm-integration-checkpoint.md</code> P4-1: \"Gemini provider - Not in current spec - Document as future work\" <code>docs/models/model-registry.md</code> Gemini listed as approved model with pricing <code>src/giant/llm/model_registry.py</code> <code>GOOGLE_MODELS</code> defined with <code>gemini-3-pro-preview</code> <code>src/giant/llm/pricing.py</code> Gemini pricing defined <code>src/giant/config.py</code> NO GOOGLE_API_KEY <code>src/giant/llm/</code> NO google_client.py"},{"location":"archive/bugs/BUG-014-env-secrets-management/#conclusion","title":"Conclusion","text":"<p>Gemini was intentionally planned for the future (P4 priority) but the scaffolding is incomplete: - Model registry and pricing are ready - Config and provider implementation are missing</p>"},{"location":"archive/bugs/BUG-014-env-secrets-management/#decision-points","title":"Decision Points","text":""},{"location":"archive/bugs/BUG-014-env-secrets-management/#q1-what-to-do-about-googlegemini","title":"Q1: What to do about Google/Gemini?","text":"<p>Options: 1. Remove from model_registry - Clean up scaffolding until we implement it 2. Keep scaffolding, complete config - Add <code>GOOGLE_API_KEY</code> to config, keep provider as future 3. Implement now - Add <code>google_client.py</code> (scope creep for current checkpoint)</p> <p>Recommendation: Option 2 - Complete the config scaffolding, document provider as P4 future work</p> <p>Why: The model_registry and pricing are already there. Adding <code>GOOGLE_API_KEY</code> to config.py makes it consistent and ready for when we implement the provider. This is low-risk, high-clarity.</p>"},{"location":"archive/bugs/BUG-014-env-secrets-management/#q2-should-tests-only-read-from-env-not-shell","title":"Q2: Should tests ONLY read from <code>.env</code>, not shell?","text":"<p>Options: 1. Keep current behavior - Reads shell env (standard Python behavior) 2. Force .env only - Override with explicit <code>_env_file</code> loading in tests 3. Document clearly - Warn users about shell env in test docs</p> <p>Recommendation: Option 3 - Document clearly, don't fight Python conventions</p>"},{"location":"archive/bugs/BUG-014-env-secrets-management/#implementation-checklist","title":"Implementation Checklist","text":"<ul> <li>[x] Update <code>.env.example</code> with comprehensive comments</li> <li>[x] Update <code>docs/specs/spec-08.5-llm-integration-checkpoint.md</code> with test warnings</li> <li>[x] Decide on Google/Gemini status and document \u2192 Decision: Keep scaffolding, complete config</li> <li>[x] Add <code>GOOGLE_API_KEY</code> to <code>config.py</code> with <code>require_google_key()</code> method</li> <li>[x] Update <code>spec-06-llm-provider.md</code> to note Gemini as P4 future work</li> <li>[x] Consider: Should model_registry reject Google until implemented? \u2192 No, scaffolding is fine</li> <li>[x] Fix test skipif to detect keys from <code>.env</code> file (not just shell env)</li> <li>[x] Fix Anthropic JSON string parsing for nested action field</li> <li>[x] Fix OpenAI schema to not use <code>oneOf</code> (use flattened schema with nulls)</li> </ul>"},{"location":"archive/bugs/BUG-014-env-secrets-management/#code-locations","title":"Code Locations","text":"File Issue <code>.env.example</code> Incomplete, no comments <code>src/giant/config.py</code> Missing GOOGLE_API_KEY <code>src/giant/llm/model_registry.py</code> Has Google but no provider <code>src/giant/llm/pricing.py</code> Has Gemini pricing but unused <code>tests/integration/llm/test_p0_critical.py</code> Uses <code>os.getenv()</code> (shell + .env) <code>docs/specs/spec-08.5-llm-integration-checkpoint.md</code> Needs clearer test docs"},{"location":"archive/bugs/BUG-014-env-secrets-management/#risk-if-not-fixed","title":"Risk If Not Fixed","text":"<ol> <li>Cost Surprise: Users with shell-exported keys run live tests unknowingly</li> <li>Confusion: Google in registry but can't actually use it</li> <li>Onboarding Friction: New devs don't know which keys are needed</li> <li>Security: No guidance on key management best practices</li> </ol>"},{"location":"archive/bugs/BUG-015-visualizer-missing-images-and-overlays/","title":"BUG-015: Visualizer Missing Images / Overlays (Spec-12 Drift)","text":""},{"location":"archive/bugs/BUG-015-visualizer-missing-images-and-overlays/#severity-p3-low-priority-ux-debuggability","title":"Severity: P3 (Low Priority) - UX / Debuggability","text":""},{"location":"archive/bugs/BUG-015-visualizer-missing-images-and-overlays/#status-fixed-2025-12-19","title":"Status: Fixed (2025-12-19)","text":""},{"location":"archive/bugs/BUG-015-visualizer-missing-images-and-overlays/#description","title":"Description","text":"<p><code>giant visualize</code> generates an HTML report, but it does not display:</p> <ul> <li>The WSI thumbnail (with axis guides)</li> <li>The crop images from each step (<code>turn.image_base64</code>)</li> <li>Any overlay/highlight of the cropped region on the thumbnail</li> </ul> <p>Spec-12\u2019s \u201cVisualization\u201d design section describes these as core features for an \u201cinteractive HTML trajectory visualization\u201d.</p> <p>Today the visualizer is effectively a text-only timeline (reasoning + region numbers), which significantly reduces its value for debugging navigation correctness (coordinate mistakes, off-by-one crops, wrong tissue, etc.).</p>"},{"location":"archive/bugs/BUG-015-visualizer-missing-images-and-overlays/#evidence-current-behavior","title":"Evidence (Current Behavior)","text":"<ul> <li><code>src/giant/cli/visualizer.py</code>:</li> <li>Does not render any <code>&lt;img&gt;</code> tags</li> <li>Does not read <code>turn[\"image_base64\"]</code> for display (it only parses <code>region</code> and <code>reasoning</code>)</li> </ul>"},{"location":"archive/bugs/BUG-015-visualizer-missing-images-and-overlays/#expected-behavior","title":"Expected Behavior","text":"<p>The visualization should, at minimum:</p> <ol> <li>Render the initial thumbnail image (axis-guided).</li> <li>Render each crop image per step.</li> <li>Show the crop region coordinates (already present).</li> </ol> <p>Optionally (but spec-aligned):</p> <ul> <li>Draw the crop region on the thumbnail (SVG/canvas overlay).</li> <li>Provide navigation controls (carousel/stepper) and collapsible sections.</li> </ul>"},{"location":"archive/bugs/BUG-015-visualizer-missing-images-and-overlays/#impact","title":"Impact","text":"<ul> <li>Harder to validate agent correctness visually.</li> <li>Easy to miss geometry/coordinate bugs (the report lacks the actual evidence).</li> <li>Limits usefulness of saved trajectories for senior review/debugging.</li> </ul>"},{"location":"archive/bugs/BUG-015-visualizer-missing-images-and-overlays/#proposed-fix","title":"Proposed Fix","text":"<ol> <li>Embed images as data URLs:</li> <li>Thumbnail: <code>data:image/jpeg;base64,&lt;...&gt;</code></li> <li>Crop images: <code>data:image/jpeg;base64,&lt;...&gt;</code></li> <li>Add a size-aware strategy to avoid generating enormous HTML:</li> <li>Option A: Write images as separate <code>.jpg</code> files to an output directory and reference them.</li> <li>Option B: Allow <code>--inline-images/--no-inline-images</code> in the CLI.</li> <li>Add overlay rendering:</li> <li>Use an SVG overlay positioned over the thumbnail to draw the crop rectangle.</li> </ol>"},{"location":"archive/bugs/BUG-015-visualizer-missing-images-and-overlays/#testing-required","title":"Testing Required","text":"<ul> <li>Unit test: HTML includes <code>&lt;img&gt;</code> tags when <code>image_base64</code> is present.</li> <li>Unit test: Crop images and thumbnail are wired correctly per step.</li> <li>Regression test: Existing \u201cminimal\u201d trajectory shapes still render without exceptions.</li> </ul>"},{"location":"archive/bugs/BUG-015-visualizer-missing-images-and-overlays/#resolution","title":"Resolution","text":"<ul> <li>Implemented thumbnail + per-step image rendering and crop overlays in <code>src/giant/cli/visualizer.py</code>.</li> <li>Added trajectory metadata needed for overlays (<code>slide_width</code>, <code>slide_height</code>, <code>thumbnail_base64</code>) in <code>src/giant/agent/trajectory.py</code> and populated by the agent in <code>src/giant/agent/runner.py</code>.</li> <li>Verified by <code>tests/unit/cli/test_visualizer.py</code> (including <code>test_renders_images_and_overlays</code>).</li> </ul>"},{"location":"archive/bugs/BUG-016-agent-allows-crop-when-max-steps-is-1/","title":"BUG-016: Agent Executes Crop When <code>max_steps=1</code> (Contract Violation + Trajectory Inconsistency)","text":""},{"location":"archive/bugs/BUG-016-agent-allows-crop-when-max-steps-is-1/#severity-p2-high-priority-correctness-contract","title":"Severity: P2 (High Priority) - Correctness / Contract","text":""},{"location":"archive/bugs/BUG-016-agent-allows-crop-when-max-steps-is-1/#status-fixed-2025-12-19","title":"Status: Fixed (2025-12-19)","text":""},{"location":"archive/bugs/BUG-016-agent-allows-crop-when-max-steps-is-1/#description","title":"Description","text":"<p>When <code>AgentConfig(max_steps=1)</code>, the prompt explicitly marks step 1 as the final step and instructs the model to use <code>answer</code>.</p> <p>However, if the model returns a <code>crop</code> action anyway, <code>GIANTAgent</code> will:</p> <ol> <li>Execute the crop via <code>CropEngine.crop(...)</code> even though no crops are allowed.</li> <li>Add a crop turn to the trajectory.</li> <li>Then force an answer via <code>_force_final_answer()</code>.</li> </ol> <p>This violates the Spec-07/Spec-09 \u201cfinal step must answer\u201d contract and creates a subtle provenance issue: the forced-answer call does not include the final-step crop image in the user message (because the ContextManager step guard prevents adding a next-step user message), but the trajectory can still end up recording the crop image as the \u201cobservation\u201d image for the answer turn.</p>"},{"location":"archive/bugs/BUG-016-agent-allows-crop-when-max-steps-is-1/#reproduction-validated","title":"Reproduction (Validated)","text":"<p>With mocked components:</p> <ul> <li>Set <code>AgentConfig(max_steps=1)</code></li> <li>Make the LLM return <code>crop</code> on the first call and <code>answer</code> on the force-answer call</li> <li>Observe: <code>CropEngine.crop</code> is invoked and the trajectory contains 2 turns (crop + answer)</li> </ul>"},{"location":"archive/bugs/BUG-016-agent-allows-crop-when-max-steps-is-1/#expected-behavior","title":"Expected Behavior","text":"<p>If <code>current_step == max_steps</code> (no remaining steps):</p> <ul> <li>A <code>crop</code> action should be treated as out-of-contract.</li> <li>The agent should not execute the crop.</li> <li>The agent should immediately force an answer (or return an error if force-answer retries fail).</li> </ul>"},{"location":"archive/bugs/BUG-016-agent-allows-crop-when-max-steps-is-1/#code-location","title":"Code Location","text":"<ul> <li><code>src/giant/agent/runner.py</code></li> <li><code>_navigation_loop()</code> routes all <code>BoundingBoxAction</code> through <code>_handle_crop(...)</code> without checking if any crops are allowed for the current step.</li> <li><code>_force_final_answer()</code> uses <code>ContextManager.get_messages()</code>, which intentionally does not append a next-step user message once the step limit is reached.</li> </ul>"},{"location":"archive/bugs/BUG-016-agent-allows-crop-when-max-steps-is-1/#impact","title":"Impact","text":"<ul> <li>Violates the step contract for edge configs (and wastes time/resources executing a crop that the model won\u2019t actually get to observe in the forced-answer call).</li> <li>Can produce misleading trajectories (answer appears to be based on an unseen crop).</li> </ul>"},{"location":"archive/bugs/BUG-016-agent-allows-crop-when-max-steps-is-1/#proposed-fix","title":"Proposed Fix","text":"<p>In <code>_navigation_loop()</code>:</p> <ul> <li>Before calling <code>_handle_crop(...)</code>, add a guard:</li> <li>If <code>self._context.current_step &gt;= self.config.max_steps</code>: do not crop; go straight to <code>_force_final_answer()</code>.</li> </ul> <p>Optionally:</p> <ul> <li>Validate config early: reject <code>max_steps &lt; 2</code> for \u201cnavigation mode\u201d, or treat <code>max_steps=1</code> as \u201cthumbnail baseline\u201d behavior.</li> </ul>"},{"location":"archive/bugs/BUG-016-agent-allows-crop-when-max-steps-is-1/#testing-required","title":"Testing Required","text":"<ul> <li>New unit test: <code>max_steps=1</code> + model returns crop \u2192 crop engine is not called and agent forces answer.</li> </ul>"},{"location":"archive/bugs/BUG-016-agent-allows-crop-when-max-steps-is-1/#resolution","title":"Resolution","text":"<ul> <li>Added a final-step guard in <code>GIANTAgent._navigation_loop()</code> so <code>crop</code> actions are rejected on <code>current_step == max_steps</code> (forces answer immediately; no crop executed).</li> <li>Verified by <code>tests/unit/agent/test_runner.py::TestGIANTAgentLoopLimit::test_crop_at_max_steps_ignored</code>.</li> </ul>"},{"location":"archive/bugs/BUG-017-tcga-downloader-file-name-path-traversal-risk/","title":"BUG-017: TCGA Downloader Trusts Remote <code>file_name</code> for Filesystem Paths","text":""},{"location":"archive/bugs/BUG-017-tcga-downloader-file-name-path-traversal-risk/#severity-p4-future-security-hardening-defense-in-depth","title":"Severity: P4 (Future) - Security Hardening / Defense-in-Depth","text":""},{"location":"archive/bugs/BUG-017-tcga-downloader-file-name-path-traversal-risk/#status-fixed-2025-12-19","title":"Status: Fixed (2025-12-19)","text":""},{"location":"archive/bugs/BUG-017-tcga-downloader-file-name-path-traversal-risk/#description","title":"Description","text":"<p>The TCGA downloader writes files using the <code>file_name</code> returned by the GDC API:</p> <pre><code>dest_dir = out_dir / file.file_id\ndest_path = dest_dir / file.file_name\n</code></pre> <p><code>file_name</code> is remote input. If it contains path separators or traversal sequences (e.g. <code>../</code>), this could cause writes outside the intended directory.</p> <p>In practice, the GDC API is expected to return safe basenames, but we should still defensively validate/sanitize the filename since this code is a downloader operating on untrusted network responses.</p>"},{"location":"archive/bugs/BUG-017-tcga-downloader-file-name-path-traversal-risk/#code-location","title":"Code Location","text":"<ul> <li><code>src/giant/data/tcga.py:_download_gdc_file()</code></li> </ul>"},{"location":"archive/bugs/BUG-017-tcga-downloader-file-name-path-traversal-risk/#expected-behavior","title":"Expected Behavior","text":"<ul> <li>Reject (or sanitize) any <code>file_name</code> that is not a simple basename.</li> <li>Prevent absolute paths and any traversal (<code>..</code>) regardless of platform.</li> </ul>"},{"location":"archive/bugs/BUG-017-tcga-downloader-file-name-path-traversal-risk/#impact","title":"Impact","text":"<ul> <li>Low likelihood with a trusted upstream, but high impact if exploited (arbitrary file write within user permissions).</li> </ul>"},{"location":"archive/bugs/BUG-017-tcga-downloader-file-name-path-traversal-risk/#proposed-fix","title":"Proposed Fix","text":"<p>Add a filename validation step:</p> <ul> <li>Require <code>Path(file_name).name == file_name</code></li> <li>Reject absolute paths and any <code>..</code> parts</li> </ul> <p>Optionally, sanitize:</p> <ul> <li>Replace unsafe characters with <code>_</code> and store the original name in logs/metadata.</li> </ul>"},{"location":"archive/bugs/BUG-017-tcga-downloader-file-name-path-traversal-risk/#testing-required","title":"Testing Required","text":"<ul> <li>Unit test: <code>file_name=\"../evil.svs\"</code> raises <code>ValueError</code> (or is sanitized to a safe name).</li> </ul>"},{"location":"archive/bugs/BUG-017-tcga-downloader-file-name-path-traversal-risk/#resolution","title":"Resolution","text":"<ul> <li>Added basename/path-traversal validation before writing downloads in <code>src/giant/data/tcga.py</code>.</li> <li>Verified by <code>tests/unit/data/test_tcga.py::TestDownloadGdcFileSecurity</code>.</li> </ul>"},{"location":"archive/bugs/BUG-018-missing-conch-integration-GITHUB-33/","title":"BUG-018: Missing CONCH Tool Integration (Paper Ablation Gap)","text":""},{"location":"archive/bugs/BUG-018-missing-conch-integration-GITHUB-33/#severity-p3-paper-ablation-feature","title":"Severity: P3 (Paper ablation feature)","text":""},{"location":"archive/bugs/BUG-018-missing-conch-integration-GITHUB-33/#status-open-blocked-on-conch-access","title":"Status: Open (Blocked on CONCH access)","text":""},{"location":"archive/bugs/BUG-018-missing-conch-integration-GITHUB-33/#description","title":"Description","text":"<p>The GIANT paper includes an ablation study (Section 6.2) where GIANT is augmented with access to the CONCH pathology foundation model for localized image\u2013text retrieval. This expands the agent's action space from <code>{crop, answer}</code> to <code>{crop, answer, conch}</code>.</p> <p>This repository currently implements the baseline loop (crop/answer only), so we cannot reproduce the paper\u2019s \u201c+ CONCH\u201d ablation (Table 3) without implementing this optional tool.</p>"},{"location":"archive/bugs/BUG-018-missing-conch-integration-GITHUB-33/#paper-evidence-section-62-table-3","title":"Paper Evidence (Section 6.2, Table 3)","text":""},{"location":"archive/bugs/BUG-018-missing-conch-integration-GITHUB-33/#how-conch-integration-works","title":"How CONCH Integration Works","text":"<p>From <code>_literature/markdown/giant/giant.md</code> (lines 232-233):</p> <p>\"we augment GIANT with access to the CONCH pathology model, enabling the agent to choose at each step between continued navigation or invoking CONCH for localized image\u2013text retrieval.\"</p> <p>\"To use this tool, the agent supplies the current crop along with a set of textual hypotheses produced by the LMM. CONCH encodes the image and each hypothesis and returns cosine similarity scores that quantify their alignment, which GIANT can use for subsequent reasoning steps.\"</p>"},{"location":"archive/bugs/BUG-018-missing-conch-integration-GITHUB-33/#conch-tool-interface","title":"CONCH Tool Interface","text":"<p>Input: - <code>image</code>: The current crop (PIL Image or tensor) - <code>hypotheses</code>: List of textual hypotheses generated by the LLM (e.g., <code>[\"adenocarcinoma\", \"squamous cell carcinoma\", \"benign tissue\"]</code>)</p> <p>Output: - <code>scores</code>: List of cosine similarity scores between the image embedding and each hypothesis text embedding</p>"},{"location":"archive/bugs/BUG-018-missing-conch-integration-GITHUB-33/#paper-results-table-3","title":"Paper Results (Table 3)","text":"<p>Numbers below are taken from Table 3 in <code>_literature/markdown/giant/giant.md</code> (lines 264-270).</p> Benchmark GPT-5 + CONCH Difference GTEx 53.7% 63.8% +10.2% TCGA 32.3% 22.0% -10.3% SlideBench 58.9% 54.3% -4.6% PANDA 23.2% 25.5% +2.3% ExpertVQA 57.0% 60.9% +3.9%"},{"location":"archive/bugs/BUG-018-missing-conch-integration-GITHUB-33/#failure-mode-from-paper","title":"Failure Mode (from paper)","text":"<p>\"the limited improvement on complex tasks may partly stem from the agent generating incorrect hypotheses, using CONCH to reinforce them\"</p> <p>This is important: CONCH can amplify confirmation bias if the LLM generates bad hypotheses.</p>"},{"location":"archive/bugs/BUG-018-missing-conch-integration-GITHUB-33/#what-this-requires-behavior","title":"What This Requires (Behavior)","text":"<p>At each navigation step, the agent must be able to choose between: - <code>crop</code>: existing behavior (request next region) - <code>answer</code>: existing behavior (final answer) - <code>conch</code>: provide the current crop plus a set of LMM-generated hypotheses; receive similarity scores and incorporate them into subsequent reasoning</p>"},{"location":"archive/bugs/BUG-018-missing-conch-integration-GITHUB-33/#unknowns-not-specified-in-the-giant-paper","title":"Unknowns / Not Specified in the GIANT Paper","text":"<p>The GIANT paper does not specify: - CONCH preprocessing details (patch size, normalization, etc.) - embedding dimensionality - the exact number/formatting of hypotheses - how similarity scores are formatted back into the LMM context</p> <p>Implementation should therefore: - rely on CONCH\u2019s official preprocessing/inference utilities where possible - treat embeddings as opaque and only expose cosine similarity scores upstream</p>"},{"location":"archive/bugs/BUG-018-missing-conch-integration-GITHUB-33/#implementation-plan-aligned-with-this-codebase","title":"Implementation Plan (Aligned With This Codebase)","text":""},{"location":"archive/bugs/BUG-018-missing-conch-integration-GITHUB-33/#1-protocol-schemas","title":"1. Protocol + Schemas","text":"<p>This codebase models step output as <code>StepResponse(reasoning, action)</code> with a discriminated union in <code>src/giant/llm/protocol.py</code> (<code>action_type</code>).</p> <p>Add a third action model: - <code>ConchAction</code> with <code>action_type: Literal[\"conch\"]</code> and <code>hypotheses: list[str]</code></p> <p>Then update both schemas used for structured output: - <code>src/giant/llm/schemas.py</code> (<code>step_response_json_schema</code> and <code>step_response_json_schema_openai</code>) - <code>src/giant/llm/openai_client.py</code> (<code>_normalize_openai_response</code> to normalize the flattened action fields)</p>"},{"location":"archive/bugs/BUG-018-missing-conch-integration-GITHUB-33/#2-tool-interface","title":"2. Tool Interface","text":"<p>Create a minimal interface (exact implementation depends on CONCH artifacts/access):</p> <ul> <li><code>ConchTool.score_hypotheses(image, hypotheses) -&gt; list[float]</code></li> </ul> <p>The agent should never depend on CONCH embedding shapes; it only needs the list of float scores aligned to the input hypotheses.</p>"},{"location":"archive/bugs/BUG-018-missing-conch-integration-GITHUB-33/#3-agent-loop-context","title":"3. Agent Loop + Context","text":"<p>In <code>src/giant/agent/runner.py</code>: - add config flags to <code>AgentConfig</code> (defined in this file), e.g. <code>enable_conch: bool = False</code> - when <code>action_type == \"conch\"</code>, call the tool with the current observation image/crop and hypotheses - append a textual observation containing the scores (e.g., top-k sorted) to the next-step user message so the LMM can use the signal</p>"},{"location":"archive/bugs/BUG-018-missing-conch-integration-GITHUB-33/#4-trajectory-recording-optional-but-recommended","title":"4. Trajectory Recording (Optional but Recommended)","text":"<p>The paper discusses \u201cinspection of reasoning traces\u201d in the CONCH ablation (Sec 6.2), so recording CONCH invocations improves debuggability.</p> <p>Minimal requirement: persist hypotheses + scores in the saved trajectory JSON for later analysis/visualization.</p>"},{"location":"archive/bugs/BUG-018-missing-conch-integration-GITHUB-33/#external-dependency-notes","title":"External Dependency Notes","text":"<p>The paper identifies CONCH as a distinct pathology model ([16]). The public HuggingFace listing indicates the model is gated (<code>gated: manual</code>): - https://huggingface.co/api/models/MahmoodLab/CONCH</p> <p>This means \u201cpaper-faithful\u201d evaluation of the \u201c+ CONCH\u201d ablation will require obtaining access to CONCH weights or equivalent artifacts.</p>"},{"location":"archive/bugs/BUG-018-missing-conch-integration-GITHUB-33/#implementation-checklist-engineering","title":"Implementation Checklist (Engineering)","text":"<ul> <li>[ ] Add <code>ConchAction</code> to <code>src/giant/llm/protocol.py</code></li> <li>[ ] Update structured-output schemas in <code>src/giant/llm/schemas.py</code> and <code>src/giant/llm/openai_client.py</code></li> <li>[ ] Add <code>enable_conch</code> + related config to <code>AgentConfig</code> in <code>src/giant/agent/runner.py</code> (default disabled)</li> <li>[ ] Implement <code>ConchTool</code> behind an optional dependency / feature flag</li> <li>[ ] Feed CONCH scores into the next step context and persist in trajectories</li> <li>[ ] Add unit tests for parsing + agent behavior; add integration test gated on model access</li> </ul>"},{"location":"archive/bugs/BUG-018-missing-conch-integration-GITHUB-33/#references","title":"References","text":"<ul> <li>GIANT paper: <code>_literature/markdown/giant/giant.md</code> (Sec 6.2, Table 3)</li> <li>HuggingFace model listing (gated): https://huggingface.co/api/models/MahmoodLab/CONCH</li> </ul>"},{"location":"archive/bugs/BUG-019-silent-font-fallback-FIXED/","title":"BUG-019: Axis Guide Font Fallback Can Degrade Label Legibility","text":""},{"location":"archive/bugs/BUG-019-silent-font-fallback-FIXED/#severity-p3-robustness-reproducibility","title":"Severity: P3 (Robustness / Reproducibility)","text":""},{"location":"archive/bugs/BUG-019-silent-font-fallback-FIXED/#status-closed-fixed","title":"Status: Closed (Fixed)","text":""},{"location":"archive/bugs/BUG-019-silent-font-fallback-FIXED/#description","title":"Description","text":"<p>Axis guide labels are rendered via <code>PIL.ImageFont.truetype</code>. If neither <code>DejaVuSans.ttf</code> nor <code>Arial.ttf</code> is available, <code>AxisGuideGenerator._get_font()</code> logs a warning and falls back to <code>ImageFont.load_default()</code>.</p> <p>This fallback is not silent (it warns), but it can still materially degrade navigation because PIL\u2019s default bitmap font is often small/pixelated and ignores the configured <code>font_size</code>.</p>"},{"location":"archive/bugs/BUG-019-silent-font-fallback-FIXED/#why-this-matters","title":"Why This Matters","text":"<ul> <li>Model performance risk: If coordinates are hard to read on the thumbnail, the VLM may choose poor crops or fail to comply with coordinate constraints.</li> <li>Environment sensitivity: Minimal Docker images frequently lack TrueType fonts; behavior can vary across machines/CI.</li> </ul>"},{"location":"archive/bugs/BUG-019-silent-font-fallback-FIXED/#evidence","title":"Evidence","text":"<ul> <li><code>src/giant/geometry/overlay.py</code>: <code>_get_font()</code> tries DejaVuSans/Arial, then warns + returns <code>ImageFont.load_default()</code>.</li> <li>Historical context: <code>docs/bugs/archive/BUG-009-font-loading-silent-fallback.md</code> fixed the \u201cno warning\u201d issue; this bug is about the remaining quality/reproducibility gap.</li> </ul>"},{"location":"archive/bugs/BUG-019-silent-font-fallback-FIXED/#resolution","title":"Resolution","text":"<ul> <li>Added <code>OverlayStyle.strict_font_check</code> + <code>AxisGuideGenerator._get_font()</code> now raises when enabled and no TrueType font is available.</li> <li>Wired through <code>AgentConfig.strict_font_check</code> and CLI flags: <code>giant run --strict-font-check</code> / <code>giant benchmark --strict-font-check</code>.</li> <li>Added unit coverage: <code>tests/unit/geometry/test_overlay.py</code> asserts strict mode raises when fonts are unavailable.</li> </ul>"},{"location":"archive/bugs/BUG-019-silent-font-fallback-FIXED/#follow-ups-optional","title":"Follow-ups (Optional)","text":"<ul> <li>Bundle a small OSS TTF in-package and prefer it before system fonts to reduce environment variance further.</li> </ul>"},{"location":"archive/bugs/BUG-020-placeholder-prompts-GITHUB-34/","title":"BUG-020: Official System Prompts Not Incorporated (Supplementary Material Missing)","text":""},{"location":"archive/bugs/BUG-020-placeholder-prompts-GITHUB-34/#severity-p3-paper-faithfulness-reproducibility","title":"Severity: P3 (Paper faithfulness / reproducibility)","text":""},{"location":"archive/bugs/BUG-020-placeholder-prompts-GITHUB-34/#status-partially-resolved-paper-derived-prompts-implemented-awaiting-supplementary-material-for-verification","title":"Status: Partially Resolved (Paper-derived prompts implemented; awaiting Supplementary Material for verification)","text":""},{"location":"archive/bugs/BUG-020-placeholder-prompts-GITHUB-34/#description","title":"Description","text":"<p>The GIANT paper states that the system prompt text used for OpenAI and Anthropic models is included in the Supplementary Material, which is not present in this repo. Our prompt templates are therefore reverse-engineered placeholders, so we cannot claim a verbatim reproduction of the paper\u2019s prompting strategy.</p> <p>This bug is about paper reproducibility, not functional correctness of the agent loop.</p>"},{"location":"archive/bugs/BUG-020-placeholder-prompts-GITHUB-34/#paper-evidence-verbatim","title":"Paper Evidence (Verbatim)","text":"<p>The paper explicitly states the prompts are in the Supplementary Material:</p> <p>From <code>_literature/markdown/giant/giant.md</code> (line 142):</p> <p>\"The system prompt used for OpenAI and Anthropic models is included in the Supplementary Material.\"</p> <p>The Figure 4 caption also points to the Supplementary Material:</p> <p>From <code>_literature/markdown/giant/giant.md</code> (line 126):</p> <p>\"See Supplementary Material for prompts.\"</p>"},{"location":"archive/bugs/BUG-020-placeholder-prompts-GITHUB-34/#what-we-can-reliably-extract-from-the-paper-high-confidence","title":"What We Can Reliably Extract From the Paper (High Confidence)","text":"<p>Even without the Supplementary Material, the paper defines invariants that the prompts must communicate.</p>"},{"location":"archive/bugs/BUG-020-placeholder-prompts-GITHUB-34/#agent-loop-contract-algorithm-1","title":"Agent loop contract (Algorithm 1)","text":"<p>From <code>_literature/markdown/giant/giant.md</code> (lines 151-161):</p> <p><code>P0 = {q, nav instructions + \"at most T-1 crops\"};</code></p> <p><code>for t \u2190 1 to T\u22121 do</code></p> <p><code>(rt, at) \u2190 LMM(C) ; // at = (x, y, w, h)</code></p> <p>This implies the prompt (at minimum) must: - communicate a crop/step budget (\u201cat most T-1 crops\u201d) - require the model to output an action bounding box <code>(x, y, w, h)</code> per step - accumulate multimodal context across steps (image + prior reasoning/actions)</p>"},{"location":"archive/bugs/BUG-020-placeholder-prompts-GITHUB-34/#coordinate-system-axis-guides-sec-41","title":"Coordinate system + axis guides (Sec 4.1)","text":"<p>From <code>_literature/markdown/giant/giant.md</code> (line 134):</p> <p>\"To orient the model, the thumbnail is overlaid with four evenly spaced axis guides along each dimension, labeled with absolute level-0 pixel coordinates. At each step, the agent outputs a bounding box a\u1d57 = (x\u1d57, y\u1d57, w\u1d57, h\u1d57) in level-0 coordinates, specifying the next region of interest.\"</p> <p>This implies the prompt must explain Level-0 coordinate space and that the axis guides provide absolute pixel coordinates used to pick <code>(x, y, w, h)</code>.</p>"},{"location":"archive/bugs/BUG-020-placeholder-prompts-GITHUB-34/#step-limit-enforcement-fig-5-caption","title":"Step-limit enforcement (Fig 5 caption)","text":"<p>From <code>_literature/markdown/giant/giant.md</code> (line 200):</p> <p>\"We use a system prompt to enforce that the model provide its final response after a specific number of iterations, marking a trial incorrect if the model exceeds this limit after 3 retries.\"</p> <p>This implies the system prompt includes explicit \u201cfinal answer by iteration T\u201d enforcement and a retry policy for violations during evaluation.</p>"},{"location":"archive/bugs/BUG-020-placeholder-prompts-GITHUB-34/#what-we-cannot-reliably-reconstruct-must-not-guess","title":"What We Cannot Reliably Reconstruct (Must Not Guess)","text":"<p>Without the Supplementary Material we cannot verify, and therefore should not assert as fact: - the exact system prompt strings (verbatim wording) - whether prompts differ between OpenAI and Anthropic (beyond the paper\u2019s statement that both exist) - any required output formatting beyond the <code>(r\u1d57, a\u1d57)</code> contract (e.g., explicit JSON text included in the prompt) - any additional constraints not stated in the main paper text</p>"},{"location":"archive/bugs/BUG-020-placeholder-prompts-GITHUB-34/#current-state-in-this-repo","title":"Current State in This Repo","text":"<p>Code reference: <code>src/giant/prompts/templates.py</code></p> <p>Update (2025-12-20): Prompts have been systematically derived from the paper with full evidence documentation.</p> <ul> <li><code>docs/prompts/prompt-design.md</code>: Comprehensive evidence table mapping each prompt component to paper line numbers</li> <li><code>src/giant/prompts/templates.py</code>: Updated with paper citations and all high-confidence requirements implemented</li> </ul> <p>Implemented (High Confidence): - [x] Crop budget communicated (Algorithm 1, line 156) - [x] Level-0 coordinate system (Sec 4.1, line 134) - [x] Axis guides explanation (Sec 4.1, line 134) - [x] Output format (x, y, w, h) (Algorithm 1, line 159) - [x] Final answer enforcement (Fig 5 caption, line 200) - [x] Reasoning per step (Algorithm 1, line 159)</p> <p>Note: Output formatting is enforced structurally by the provider integrations (OpenAI uses strict JSON-schema structured output; Anthropic uses forced tool use). This improves parsing reliability, but may differ from how the paper authors enforced formatting in their experiments.</p>"},{"location":"archive/bugs/BUG-020-placeholder-prompts-GITHUB-34/#proposed-fix-ssot-reproducibility","title":"Proposed Fix (SSOT / Reproducibility)","text":"<ol> <li>Obtain the Supplementary Material prompt text(s) referenced by the paper (OpenAI + Anthropic variants, if different).</li> <li>Integrate the verbatim prompts into <code>src/giant/prompts/templates.py</code> (or a provider-specific prompt module).</li> <li>Add regression tests that lock the paper-required invariants (coordinate system, bbox format, crop limit, final-answer enforcement), and document any intentional divergence (e.g., structured-output enforcement).</li> <li>Keep a clear provenance note (source + date) to prevent future drift.</li> </ol>"},{"location":"archive/bugs/BUG-020-placeholder-prompts-GITHUB-34/#references","title":"References","text":"<ul> <li>GIANT paper: <code>_literature/markdown/giant/giant.md</code></li> <li>Fig 4 caption (prompts in Supplementary): line 126</li> <li>Axis guides + Level-0 bbox (Sec 4.1): line 134</li> <li>System prompt in Supplementary: line 142</li> <li>Algorithm 1 (GIANT): lines 151-161</li> <li>Fig 5 caption (step-limit enforcement): line 200</li> <li>Current templates: <code>src/giant/prompts/templates.py</code></li> </ul>"},{"location":"archive/bugs/BUG-021-prompt-template-edge-case-INVALID/","title":"BUG-021: Prompt Template Edge Case for <code>max_steps=1</code> (Not Reproducible)","text":""},{"location":"archive/bugs/BUG-021-prompt-template-edge-case-INVALID/#severity-p4-claim-inaccurate","title":"Severity: P4 (Claim inaccurate)","text":""},{"location":"archive/bugs/BUG-021-prompt-template-edge-case-INVALID/#status-closed-not-reproducible-in-current-implementation","title":"Status: Closed (Not reproducible in current implementation)","text":""},{"location":"archive/bugs/BUG-021-prompt-template-edge-case-INVALID/#description","title":"Description","text":"<p>The raw <code>INITIAL_USER_PROMPT</code> template includes a step-range instruction:</p> <p>\u201cFor Steps 1..{max_steps \u2212 1} you MUST use <code>crop</code>.\u201d</p> <p>If someone rendered that template directly with <code>max_steps=1</code>, it would read \u201cSteps 1..0\u201d, which is nonsensical.</p> <p>However, the runtime prompt construction uses <code>PromptBuilder</code>, which selects the final-step wording whenever <code>step == max_steps</code>. For <code>max_steps=1</code>, the very first message uses the final-step prompt (\u201cYou MUST use <code>answer</code> now\u201d), so the \u201cSteps 1..0\u201d text is never produced.</p>"},{"location":"archive/bugs/BUG-021-prompt-template-edge-case-INVALID/#evidence","title":"Evidence","text":"<ul> <li><code>src/giant/prompts/builder.py</code>: <code>_build_prompt_text()</code> checks <code>if step == max_steps</code> before the \u201cinitial step\u201d branch.</li> <li><code>src/giant/agent/context.py</code>: uses <code>PromptBuilder</code> for all message construction.</li> </ul>"},{"location":"archive/bugs/BUG-021-prompt-template-edge-case-INVALID/#resolution","title":"Resolution","text":"<p>No code change required. Spec-07 documents the <code>max_steps=1</code> edge case to prevent future confusion.</p>"},{"location":"archive/bugs/BUG-022-multipathqa-ux-FIXED/","title":"BUG-022: MultiPathQA Dataset Acquisition UX Gaps (Validation/Tooling)","text":""},{"location":"archive/bugs/BUG-022-multipathqa-ux-FIXED/#severity-p4-ux-tooling","title":"Severity: P4 (UX / Tooling)","text":""},{"location":"archive/bugs/BUG-022-multipathqa-ux-FIXED/#status-closed-fixed","title":"Status: Closed (Fixed)","text":""},{"location":"archive/bugs/BUG-022-multipathqa-ux-FIXED/#description","title":"Description","text":"<p>The paper describes MultiPathQA as being released publicly on HuggingFace. In practice (and in this repository), HuggingFace hosts the metadata CSV; the WSIs themselves must be obtained from TCGA / GTEx / PANDA sources and placed under <code>--wsi-root</code>.</p> <p>This is documented (e.g., Spec-10 explicitly calls out metadata-only download), but the CLI/tooling does not currently provide strong \u201cbatteries included\u201d helpers for: - validating a local <code>--wsi-root</code> against <code>MultiPathQA.csv</code>, - reporting which files are missing and where they\u2019re expected, - generating manifests / commands for common download workflows (e.g., <code>gdc-client</code> layout).</p>"},{"location":"archive/bugs/BUG-022-multipathqa-ux-FIXED/#why-this-matters","title":"Why This Matters","text":"<ul> <li>Reduces time-to-first-run for new users.</li> <li>Avoids confusing \u201cfile not found\u201d errors by catching missing/incorrect layout up front.</li> </ul>"},{"location":"archive/bugs/BUG-022-multipathqa-ux-FIXED/#evidence","title":"Evidence","text":"<ul> <li><code>src/giant/data/download.py</code> downloads MultiPathQA CSV metadata only (by design).</li> <li><code>docs/specs/spec-10-evaluation.md</code> notes WSIs may need to be acquired separately.</li> <li><code>src/giant/eval/runner.py</code> resolves <code>image_path</code> under <code>--wsi-root</code> and errors when missing.</li> </ul>"},{"location":"archive/bugs/BUG-022-multipathqa-ux-FIXED/#resolution","title":"Resolution","text":"<p>Implemented <code>giant check-data &lt;dataset&gt;</code> to validate local WSI availability before running benchmarks.</p> <p>Behavior: - Checks unique WSI files referenced by <code>MultiPathQA.csv</code> for the selected benchmark. - Exits <code>0</code> when all required WSIs are present; exits <code>1</code> when any are missing. - <code>--json</code> returns counts + a small set of missing examples; <code>-v</code> prints missing examples in text mode.</p> <p>Example: <code>giant check-data tcga --csv-path data/multipathqa/MultiPathQA.csv --wsi-root data/wsi -v</code></p>"},{"location":"archive/bugs/BUG-022-multipathqa-ux-FIXED/#follow-ups-optional","title":"Follow-ups (Optional)","text":"<ul> <li>Add a <code>--report &lt;path&gt;</code> option to write full missing manifests for bulk download workflows (e.g., TCGA <code>gdc-client</code>).</li> </ul>"},{"location":"archive/bugs/BUG-023-magic-numbers-overlay-FIXED/","title":"BUG-023: Axis Guide Labels Use \u201cK\u201d Abbreviation (Not Absolute Coordinates)","text":""},{"location":"archive/bugs/BUG-023-magic-numbers-overlay-FIXED/#severity-p2-paperspec-mismatch-impacting-navigation","title":"Severity: P2 (Paper/spec mismatch impacting navigation)","text":""},{"location":"archive/bugs/BUG-023-magic-numbers-overlay-FIXED/#status-closed-fixed","title":"Status: Closed (Fixed)","text":""},{"location":"archive/bugs/BUG-023-magic-numbers-overlay-FIXED/#description","title":"Description","text":"<p>The paper specifies that the thumbnail\u2019s axis guides are \u201clabeled with absolute level-0 pixel coordinates.\u201d Specs mirror this expectation (examples like <code>10000</code>, <code>20000</code>, <code>15000</code>).</p> <p>The implementation previously formatted larger coordinates using \u201cK\u201d notation (e.g., <code>15000 \u2192 \"15K\"</code>), controlled by hard-coded thresholds:</p> <pre><code>_COORD_THRESHOLD_LARGE = 10000  # 15000 -&gt; \"15K\"\n_COORD_THRESHOLD_MEDIUM = 1000  # 1500  -&gt; \"1.5K\"\n</code></pre> <p><code>OverlayStyle.num_guides=4</code> is correct and paper-faithful; the issue is specifically the label formatting (and, secondarily, the hard-coded formatting thresholds).</p>"},{"location":"archive/bugs/BUG-023-magic-numbers-overlay-FIXED/#why-this-matters","title":"Why This Matters","text":"<ul> <li>Prompt/visual mismatch: The system prompt and specs describe absolute coordinates (e.g., <code>10000</code>), but the image shows abbreviated labels (e.g., <code>10K</code>).</li> <li>Navigation accuracy: Abbreviations increase the risk the model outputs non-numeric coordinates (\u201c10K\u201d) or makes off-by-thousands errors.</li> <li>Reproducibility: Small presentation differences can materially change LMM behavior.</li> </ul>"},{"location":"archive/bugs/BUG-023-magic-numbers-overlay-FIXED/#evidence","title":"Evidence","text":"<ul> <li>Paper: <code>_literature/markdown/giant/giant.md</code> states labels are absolute level-0 pixel coordinates.</li> <li>Spec: <code>docs/specs/spec-03-coordinates.md</code> describes rendering labels like <code>\"15000\"</code>.</li> <li>Historical code: <code>src/giant/geometry/overlay.py</code> formatted coordinates with \u201cK\u201d abbreviations.</li> </ul>"},{"location":"archive/bugs/BUG-023-magic-numbers-overlay-FIXED/#resolution","title":"Resolution","text":"<ul> <li>Removed \u201cK\u201d abbreviations; axis labels now render as absolute integers (paper/spec-faithful).</li> <li>Added unit coverage in <code>tests/unit/geometry/test_overlay.py</code> to prevent regressions.</li> </ul>"},{"location":"archive/bugs/BUG-023-magic-numbers-overlay-FIXED/#follow-ups-optional","title":"Follow-ups (Optional)","text":"<ul> <li>If compact labels are desired for human debugging, make the behavior explicit and configurable (defaulting to paper-faithful absolute integers).</li> </ul>"},{"location":"archive/bugs/BUG-025-openai-assistant-message-content-type/","title":"BUG-025: OpenAI Responses API Rejects Multi-Turn Conversations","text":""},{"location":"archive/bugs/BUG-025-openai-assistant-message-content-type/#severity-p0-blocks-all-multi-step-inference-with-openai","title":"Severity: P0 (Blocks all multi-step inference with OpenAI)","text":""},{"location":"archive/bugs/BUG-025-openai-assistant-message-content-type/#status-fixed-role-aware-openai-content-typing","title":"Status: Fixed - role-aware OpenAI content typing","text":""},{"location":"archive/bugs/BUG-025-openai-assistant-message-content-type/#description","title":"Description","text":"<p>When running GIANT with OpenAI, the first navigation step succeeds, but all subsequent steps fail with:</p> <pre><code>Error code: 400 - {'error': {'message': \"Invalid value: 'input_text'. Supported values are: 'output_text' and 'refusal'.\", 'type': 'invalid_request_error', 'param': 'input[1].content[0]', 'code': 'invalid_value'}}\n</code></pre> <p>This blocks all multi-step GIANT inference with OpenAI.</p>"},{"location":"archive/bugs/BUG-025-openai-assistant-message-content-type/#root-cause","title":"Root Cause","text":"<p>Before the fix, in <code>src/giant/llm/converters.py</code>, <code>message_content_to_openai()</code> used <code>\"type\": \"input_text\"</code> for all text content, regardless of the message role:</p> <pre><code>def message_content_to_openai(content: MessageContent) -&gt; dict[str, Any]:\n    if content.type == \"text\":\n        if content.text is None:\n            raise ValueError(\"Text content requires 'text' field\")\n        return {\"type\": \"input_text\", \"text\": content.text}\n</code></pre> <p>The OpenAI Responses API has different content type requirements per role:</p> <ul> <li>User messages: <code>input_text</code>, <code>input_image</code></li> <li>Assistant messages: <code>output_text</code>, <code>refusal</code></li> </ul> <p>When the agent completes step 1 (crop action), the <code>ContextManager</code> builds an assistant message at <code>src/giant/agent/context.py:202-205</code>:</p> <pre><code>return Message(\n    role=\"assistant\",\n    content=[MessageContent(type=\"text\", text=text)],\n)\n</code></pre> <p>Before the fix, this assistant message gets converted via <code>message_to_openai()</code> without role-aware typing:</p> <pre><code>\"content\": [message_content_to_openai(c) for c in message.content],\n</code></pre> <p>Since <code>message_content_to_openai()</code> did not know the role, it used <code>input_text</code> for everything. OpenAI rejects this on step 2.</p>"},{"location":"archive/bugs/BUG-025-openai-assistant-message-content-type/#reproduction","title":"Reproduction","text":"<pre><code>source .venv/bin/activate &amp;&amp; source .env\ngiant run /path/to/any.svs -q \"What tissue is this?\" --provider openai\n</code></pre> <p>Expected: Multi-step navigation completes. Actual: Step 1 succeeds, step 2+ fails with 400 error.</p>"},{"location":"archive/bugs/BUG-025-openai-assistant-message-content-type/#test-gap","title":"Test Gap","text":"<p>Before the fix, <code>TestMessagesToOpenaiInput.test_preserves_order</code> in <code>tests/unit/llm/test_converters.py</code> only verified roles were preserved, not assistant content types:</p> <pre><code>def test_preserves_order(self) -&gt; None:\n    messages = [\n        Message(role=\"user\", ...),\n        Message(role=\"assistant\", ...),  # Uses input_text internally - NOT TESTED\n        Message(role=\"user\", ...),\n    ]\n    result = messages_to_openai_input(messages)\n    assert result[1][\"role\"] == \"assistant\"  # Only checks role, not content type\n</code></pre> <p>No test verifies that assistant messages use <code>output_text</code>.</p>"},{"location":"archive/bugs/BUG-025-openai-assistant-message-content-type/#fix-requirements","title":"Fix Requirements","text":"<ol> <li>Modify <code>message_content_to_openai()</code> to accept a <code>role</code> parameter</li> <li>Use <code>output_text</code> for <code>role=\"assistant\"</code>, <code>input_text</code> for <code>role=\"user\"</code></li> <li>Update <code>message_to_openai()</code> to pass the role</li> <li>Add test case verifying assistant messages use <code>output_text</code></li> <li>Add test case verifying assistant messages cannot contain images (only text/refusal allowed)</li> </ol>"},{"location":"archive/bugs/BUG-025-openai-assistant-message-content-type/#files-affected","title":"Files Affected","text":"<ul> <li><code>src/giant/llm/converters.py</code> - Core fix</li> <li><code>tests/unit/llm/test_converters.py</code> - Add missing test coverage</li> </ul>"},{"location":"archive/bugs/BUG-025-openai-assistant-message-content-type/#resolution","title":"Resolution","text":"<p>Implemented role-aware conversion for OpenAI Responses API:</p> <ul> <li><code>src/giant/llm/converters.py</code>: <code>message_content_to_openai(..., role=...)</code> now emits <code>output_text</code> for <code>role=\"assistant\"</code> and rejects images for assistant messages.</li> <li><code>tests/unit/llm/test_converters.py</code>: Added regression coverage for assistant <code>output_text</code> and unsupported assistant images.</li> </ul>"},{"location":"archive/bugs/BUG-025-openai-assistant-message-content-type/#references","title":"References","text":"<ul> <li>OpenAI Responses API docs</li> <li>Error observed during E2E testing on 2025-12-20</li> </ul>"},{"location":"archive/bugs/BUG-026-model-id-scattered-config-FIXED/","title":"BUG-026: Model ID Configuration Scattered Across Codebase","text":""},{"location":"archive/bugs/BUG-026-model-id-scattered-config-FIXED/#severity-p3-code-quality-maintainability","title":"Severity: P3 (Code Quality / Maintainability)","text":""},{"location":"archive/bugs/BUG-026-model-id-scattered-config-FIXED/#status-fixed","title":"Status: Fixed","text":""},{"location":"archive/bugs/BUG-026-model-id-scattered-config-FIXED/#description","title":"Description","text":"<p>Changing the default Anthropic model from <code>claude-opus-4-5-20251101</code> to <code>claude-sonnet-4-5-20250929</code> required modifying 17 files. This is an anti-pattern indicating scattered configuration that should be centralized.</p>"},{"location":"archive/bugs/BUG-026-model-id-scattered-config-FIXED/#evidence","title":"Evidence","text":"<p>Files touched for a single model change:</p> <p>Core Code (4 files): 1. <code>src/giant/llm/model_registry.py</code> - ANTHROPIC_MODELS set 2. <code>src/giant/llm/pricing.py</code> - pricing table 3. <code>src/giant/llm/anthropic_client.py</code> - default model attribute 4. <code>src/giant/llm/__init__.py</code> - fallback in factory function</p> <p>Documentation (5 files): 5. <code>docs/models/model-registry.md</code> - SSOT documentation 6. <code>CLAUDE.md</code> - project guidance 7. <code>docs/specs/spec-06-llm-provider.md</code> - spec examples 8. <code>docs/validation/e2e-validation-2025-12-20.md</code> - validation report 9. <code>docs/bugs/archive/BUG-014-env-secrets-management.md</code> - archived bug</p> <p>Tests (8 files): 10. <code>tests/unit/llm/test_model_registry.py</code> 11. <code>tests/unit/llm/test_pricing.py</code> 12. <code>tests/unit/llm/test_anthropic.py</code> 13. <code>tests/unit/llm/test_factory.py</code> 14. <code>tests/unit/llm/test_openai.py</code> 15. <code>tests/unit/cli/test_runners.py</code> 16. <code>tests/integration/llm/test_p0_critical.py</code> 17. <code>tests/integration/llm/test_p1_high_priority.py</code></p>"},{"location":"archive/bugs/BUG-026-model-id-scattered-config-FIXED/#root-cause-analysis","title":"Root Cause Analysis","text":""},{"location":"archive/bugs/BUG-026-model-id-scattered-config-FIXED/#whats-correct","title":"What's Correct","text":"<p>The <code>model_registry.py</code> file correctly serves as the validator for which models are allowed. This is good architecture.</p>"},{"location":"archive/bugs/BUG-026-model-id-scattered-config-FIXED/#whats-wrong","title":"What's Wrong","text":"<ol> <li>Default model defined in multiple places:</li> <li><code>anthropic_client.py:121</code> - <code>model: str = \"claude-sonnet-4-5-20250929\"</code></li> <li> <p><code>__init__.py:104</code> - <code>chosen_model = model or \"claude-sonnet-4-5-20250929\"</code></p> </li> <li> <p>Tests hardcode model IDs instead of using centralized fixtures or constants</p> </li> <li> <p>No single source of truth for defaults - The registry validates, but doesn't define defaults</p> </li> </ol>"},{"location":"archive/bugs/BUG-026-model-id-scattered-config-FIXED/#fix-implemented","title":"Fix Implemented","text":""},{"location":"archive/bugs/BUG-026-model-id-scattered-config-FIXED/#1-centralize-defaults-in-model_registrypy","title":"1. Centralize defaults in <code>model_registry.py</code>","text":"<pre><code># model_registry.py\n\n# Approved models (validation)\nANTHROPIC_MODELS: frozenset[str] = frozenset({\"claude-sonnet-4-5-20250929\"})\nOPENAI_MODELS: frozenset[str] = frozenset({\"gpt-5.2\"})\n\n# Default models (configuration SSOT)\nDEFAULT_ANTHROPIC_MODEL: str = \"claude-sonnet-4-5-20250929\"\nDEFAULT_OPENAI_MODEL: str = \"gpt-5.2\"\nDEFAULT_GOOGLE_MODEL: str = \"gemini-3-pro-preview\"\n</code></pre>"},{"location":"archive/bugs/BUG-026-model-id-scattered-config-FIXED/#2-use-defaults-everywhere-in-code","title":"2. Use defaults everywhere in code","text":"<pre><code># anthropic_client.py\nfrom giant.llm.model_registry import DEFAULT_ANTHROPIC_MODEL\n\n@dataclass\nclass AnthropicProvider:\n    model: str = DEFAULT_ANTHROPIC_MODEL  # Single import, no hardcoded string\n</code></pre> <pre><code># __init__.py\nfrom giant.llm.model_registry import DEFAULT_ANTHROPIC_MODEL, DEFAULT_OPENAI_MODEL\n\ndef create_provider(provider: str, model: str | None = None):\n    if provider == \"anthropic\":\n        chosen_model = model or DEFAULT_ANTHROPIC_MODEL  # Single import\n</code></pre>"},{"location":"archive/bugs/BUG-026-model-id-scattered-config-FIXED/#3-remove-hardcoded-ids-from-tests","title":"3. Remove hardcoded IDs from tests","text":"<p>Tests import <code>DEFAULT_ANTHROPIC_MODEL</code> / <code>DEFAULT_OPENAI_MODEL</code> directly so a future default-model update does not require editing test expectations that only assert \"default == default\".</p>"},{"location":"archive/bugs/BUG-026-model-id-scattered-config-FIXED/#4-result","title":"4. Result","text":"<p>After refactoring, a model change would only require updating:</p> <ol> <li><code>model_registry.py</code> - The SSOT</li> <li><code>pricing.py</code> - Pricing table (could also be data-driven)</li> <li>Documentation files (unavoidable for human-readable docs)</li> </ol> <p>This reduces \u201cstring scatter\u201d changes in tests to ~0 (pricing expectation changes may still be needed if the new default model has different costs).</p>"},{"location":"archive/bugs/BUG-026-model-id-scattered-config-FIXED/#impact-assessment","title":"Impact Assessment","text":"Current After Fix 17 files to change ~4 files to change Hardcoded strings everywhere Single import from registry Easy to miss a file Impossible to have inconsistency Tests break if constant missed Tests import registry defaults/constants"},{"location":"archive/bugs/BUG-026-model-id-scattered-config-FIXED/#priority-justification","title":"Priority Justification","text":"<p>P3 because: - Not a functional bug (system works correctly) - Not a security issue - Is a maintainability/DX issue that increases risk of future bugs - Would have prevented the near-miss where wrong model ID was almost used</p>"},{"location":"archive/bugs/BUG-026-model-id-scattered-config-FIXED/#references","title":"References","text":"<ul> <li>Commit that triggered this observation: <code>7ccb026</code> (17 files changed)</li> <li>Related: BUG-014 (env secrets management) - similar centralization issue</li> </ul>"},{"location":"archive/bugs/BUG-027-tcga-prompt-formatting-FIXED/","title":"BUG-027: CSV Options Parsed as Single-Element List (Python vs JSON)","text":""},{"location":"archive/bugs/BUG-027-tcga-prompt-formatting-FIXED/#severity-p1-high-causes-88-extraction-failures","title":"Severity: P1 (High - causes 88% extraction failures)","text":""},{"location":"archive/bugs/BUG-027-tcga-prompt-formatting-FIXED/#status-fixed","title":"Status: Fixed","text":""},{"location":"archive/bugs/BUG-027-tcga-prompt-formatting-FIXED/#description","title":"Description","text":"<p>When running the TCGA benchmark, answer extraction fails for 22 out of 25 questions (88%). The root cause is that <code>MultiPathQA.csv</code> stores options as Python list literals (single quotes), but the loader attempts <code>json.loads()</code> (double quotes required), which fails silently and the fallback produces incorrect data.</p>"},{"location":"archive/bugs/BUG-027-tcga-prompt-formatting-FIXED/#evidence","title":"Evidence","text":""},{"location":"archive/bugs/BUG-027-tcga-prompt-formatting-FIXED/#benchmark-results-2025-12-20","title":"Benchmark Results (2025-12-20)","text":"<pre><code>Results: n_total: 25, n_extraction_failures: 22\n</code></pre> <p>Only 3/25 answers were correctly extracted, leading to a meaningless 2.1% accuracy.</p>"},{"location":"archive/bugs/BUG-027-tcga-prompt-formatting-FIXED/#root-cause-demonstration","title":"Root Cause Demonstration","text":"<pre><code>options_str = \"['Glioblastoma multiforme', 'Ovarian', ...]\"\n\n# What the code does:\njson.loads(options_str)  # FAILS - single quotes not valid JSON\noptions_str.split(\"|\")   # Returns: [\"['Glioblastoma...']\"] (1 element!)\n\n# What it should do:\nast.literal_eval(options_str)  # Returns: ['Glioblastoma', 'Ovarian', ...] (30 elements)\n</code></pre>"},{"location":"archive/bugs/BUG-027-tcga-prompt-formatting-FIXED/#resulting-prompt","title":"Resulting Prompt","text":"<p>Actual (broken): <pre><code>Select from the following options:\n1. ['Glioblastoma multiforme', 'Ovarian serous cystadenocarcinoma', ...]\n</code></pre></p> <p>Expected (correct): <pre><code>Select from the following options:\n1. Glioblastoma multiforme\n2. Ovarian serous cystadenocarcinoma\n...\n30. Thymoma\n</code></pre></p>"},{"location":"archive/bugs/BUG-027-tcga-prompt-formatting-FIXED/#model-behavior","title":"Model Behavior","text":"<p>The model interprets <code>1. [list of 30 items]</code> as a single option containing a Python list, then outputs an index into that list (e.g., <code>{\"answer\": 19}</code>). The answer extraction fails because 19 &gt; 1 (apparent option count).</p>"},{"location":"archive/bugs/BUG-027-tcga-prompt-formatting-FIXED/#affected-code","title":"Affected Code","text":"<p><code>src/giant/eval/runner.py</code> (pre-fix):</p> <pre><code># Parse options if present\noptions = None\noptions_str = row.get(\"options\", \"\")\nif options_str:\n    try:\n        options = json.loads(options_str)  # FAILS for Python literals\n    except json.JSONDecodeError:\n        # Try splitting by common delimiters\n        options = [o.strip() for o in options_str.split(\"|\")]  # WRONG FALLBACK\n</code></pre>"},{"location":"archive/bugs/BUG-027-tcga-prompt-formatting-FIXED/#affected-benchmarks","title":"Affected Benchmarks","text":"<p>Pre-fix behavior:</p> Benchmark Options Format Current Behavior Status tcga Python list <code>['a', 'b']</code> 1-element list BROKEN (pre-fix) gtex Python list <code>['a', 'b']</code> 1-element list BROKEN (pre-fix) tcga_slidebench Python list <code>['2', '3']</code> 1-element list BROKEN (pre-fix) tcga_expert_vqa Python list <code>['Low', 'High']</code> 1-element list BROKEN (pre-fix) panda Empty N/A (no options) OK"},{"location":"archive/bugs/BUG-027-tcga-prompt-formatting-FIXED/#proposed-fix","title":"Proposed Fix","text":"<p>Add <code>ast.literal_eval</code> as secondary fallback in <code>src/giant/eval/runner.py</code>:</p> <pre><code>import ast\n\n# Parse options if present\noptions = None\noptions_str = row.get(\"options\", \"\")\nif options_str:\n    # Try JSON first (double quotes)\n    try:\n        options = json.loads(options_str)\n    except json.JSONDecodeError:\n        # Try Python literal (single quotes)\n        try:\n            options = ast.literal_eval(options_str)\n        except (ValueError, SyntaxError):\n            # Last resort: pipe-delimited\n            options = [o.strip() for o in options_str.split(\"|\")]\n</code></pre>"},{"location":"archive/bugs/BUG-027-tcga-prompt-formatting-FIXED/#impact-assessment","title":"Impact Assessment","text":"Metric Before Fix After Fix Extraction failures (TCGA) 88% ~0% Extraction failures (all benchmarks) Unknown ~0% Cost wasted on failed benchmark $0.99 $0"},{"location":"archive/bugs/BUG-027-tcga-prompt-formatting-FIXED/#testing-checklist","title":"Testing Checklist","text":"<ul> <li>[x] Add unit test: <code>test_options_parsing_python_literal()</code></li> <li>[x] Add unit test: <code>test_options_parsing_json()</code></li> <li>[x] Add unit test: <code>test_options_parsing_pipe_delimited()</code></li> <li>[ ] Verify all benchmarks load with correct option counts</li> <li>[ ] Re-run TCGA benchmark and confirm extraction rate improves</li> </ul>"},{"location":"archive/bugs/BUG-027-tcga-prompt-formatting-FIXED/#resolution","title":"Resolution","text":"<p>Fixed by parsing <code>options</code> using JSON first, then Python literal lists, and failing loudly on unparseable formats (instead of silently producing a 1-element list). Also updated prompt construction to always show options to the model when options exist.</p> <ul> <li>Code: <code>src/giant/eval/runner.py</code> (<code>BenchmarkRunner._parse_options</code>, <code>BenchmarkRunner._inject_options</code>)</li> <li>Tests: <code>tests/unit/eval/test_runner.py</code> (<code>test_parses_python_literal_options</code>)</li> </ul>"},{"location":"archive/bugs/BUG-027-tcga-prompt-formatting-FIXED/#references","title":"References","text":"<ul> <li>Failed benchmark run: <code>results/openai-benchmark-20251220-210900/</code></li> <li>CSV data: <code>data/multipathqa/MultiPathQA.csv</code></li> <li>Loader: <code>src/giant/eval/runner.py</code></li> </ul>"},{"location":"archive/bugs/BUG-028-missing-options-in-prompts-FIXED/","title":"BUG-028: Options Not Displayed in Prompts (tcga_slidebench, tcga_expert_vqa)","text":""},{"location":"archive/bugs/BUG-028-missing-options-in-prompts-FIXED/#severity-p2-medium-affects-2-benchmarks-accuracy-may-be-reduced","title":"Severity: P2 (Medium - affects 2 benchmarks, accuracy may be reduced)","text":""},{"location":"archive/bugs/BUG-028-missing-options-in-prompts-FIXED/#status-fixed","title":"Status: Fixed","text":""},{"location":"archive/bugs/BUG-028-missing-options-in-prompts-FIXED/#description","title":"Description","text":"<p>The <code>tcga_slidebench</code> and <code>tcga_expert_vqa</code> benchmarks have answer options in the CSV, but their prompt templates do not include the <code>{options}</code> placeholder. This means the model sees the question but not the available choices, forcing it to guess the answer format.</p>"},{"location":"archive/bugs/BUG-028-missing-options-in-prompts-FIXED/#evidence","title":"Evidence","text":""},{"location":"archive/bugs/BUG-028-missing-options-in-prompts-FIXED/#tcga_slidebench","title":"tcga_slidebench","text":"<p>Prompt in CSV: <pre><code>What is the secondary Gleason pattern observed in this case of prostate adenocarcinoma?\n</code></pre></p> <p>Options in CSV: <pre><code>['2', '3', '4', '5']\n</code></pre></p> <p>What the model sees: <pre><code>What is the secondary Gleason pattern observed in this case of prostate adenocarcinoma?\n</code></pre></p> <p>What the model should see: <pre><code>What is the secondary Gleason pattern observed in this case of prostate adenocarcinoma?\n\nSelect from the following options:\n1. 2\n2. 3\n3. 4\n4. 5\n\nPlease respond with the number of your answer.\n</code></pre></p>"},{"location":"archive/bugs/BUG-028-missing-options-in-prompts-FIXED/#tcga_expert_vqa","title":"tcga_expert_vqa","text":"<p>Prompt in CSV: <pre><code>What is the level of mitotic activity in the abnormal tissue?\n</code></pre></p> <p>Options in CSV: <pre><code>['Low', 'Medium', 'High', 'Cannot determine']\n</code></pre></p>"},{"location":"archive/bugs/BUG-028-missing-options-in-prompts-FIXED/#impact","title":"Impact","text":"<p>Without seeing the options, the model may: 1. Answer in free-form text that doesn't match extraction patterns 2. Use different terminology than the expected options 3. Provide correct diagnoses that don't match the expected answer format</p> <p>This likely reduces accuracy even when the model's reasoning is correct.</p>"},{"location":"archive/bugs/BUG-028-missing-options-in-prompts-FIXED/#affected-code","title":"Affected Code","text":"<p><code>src/giant/eval/runner.py</code> (pre-fix):</p> <pre><code># Build prompt (substitute {options} if needed)\nprompt = row.get(\"prompt\", row.get(\"question\", \"\"))\nif options and \"{options}\" in prompt:  # This check FAILS\n    formatted_options = \"\\n\".join(\n        f\"{i}. {opt}\" for i, opt in enumerate(options, start=1)\n    )\n    prompt = prompt.replace(\"{options}\", formatted_options)\n</code></pre> <p>The check <code>\"{options}\" in prompt</code> evaluates to <code>False</code>, so options are never added.</p>"},{"location":"archive/bugs/BUG-028-missing-options-in-prompts-FIXED/#proposed-fix","title":"Proposed Fix","text":""},{"location":"archive/bugs/BUG-028-missing-options-in-prompts-FIXED/#option-1-append-options-if-not-present-recommended","title":"Option 1: Append Options If Not Present (Recommended)","text":"<pre><code># Build prompt (substitute {options} if needed)\nprompt = row.get(\"prompt\", row.get(\"question\", \"\"))\nif options:\n    formatted_options = \"\\n\".join(\n        f\"{i}. {opt}\" for i, opt in enumerate(options, start=1)\n    )\n    if \"{options}\" in prompt:\n        prompt = prompt.replace(\"{options}\", formatted_options)\n    else:\n        # Append options if placeholder missing\n        prompt = (\n            f\"{prompt}\\n\\n\"\n            f\"Select from the following options:\\n{formatted_options}\\n\\n\"\n            f\"Please respond with the number of your answer.\"\n        )\n</code></pre>"},{"location":"archive/bugs/BUG-028-missing-options-in-prompts-FIXED/#option-2-fix-the-csv","title":"Option 2: Fix the CSV","text":"<p>Add <code>{options}</code> placeholder to all prompts in <code>MultiPathQA.csv</code>. This requires editing the source data.</p>"},{"location":"archive/bugs/BUG-028-missing-options-in-prompts-FIXED/#resolution","title":"Resolution","text":"<p>Fixed by always injecting options into the prompt when an <code>options</code> list is present:</p> <ul> <li>If <code>{options}</code> exists in the prompt, substitute it with a formatted list.</li> <li> <p>Otherwise, append a standardized options block and instruct the model to respond with a   1-based option index.</p> </li> <li> <p>Code: <code>src/giant/eval/runner.py</code> (<code>BenchmarkRunner._inject_options</code>)</p> </li> <li>Tests: <code>tests/unit/eval/test_runner.py</code> (<code>test_appends_options_when_placeholder_missing</code>)</li> </ul>"},{"location":"archive/bugs/BUG-028-missing-options-in-prompts-FIXED/#testing-checklist","title":"Testing Checklist","text":"<ul> <li>[x] Add unit test: prompts with <code>{options}</code> placeholder work correctly</li> <li>[x] Add unit test: prompts without <code>{options}</code> get options appended</li> <li>[ ] Verify <code>tcga_slidebench</code> and <code>tcga_expert_vqa</code> show options to model</li> <li>[ ] Run sample inference to confirm answer extraction works</li> </ul>"},{"location":"archive/bugs/BUG-028-missing-options-in-prompts-FIXED/#references","title":"References","text":"<ul> <li>Related: BUG-027 (options parsing issue - must fix first)</li> <li>CSV data: <code>data/multipathqa/MultiPathQA.csv</code></li> <li>Loader: <code>src/giant/eval/runner.py</code></li> </ul>"},{"location":"archive/bugs/BUG-029-low-tcga-accuracy-FIXED/","title":"BUG-029: Low TCGA Benchmark Accuracy Investigation","text":""},{"location":"archive/bugs/BUG-029-low-tcga-accuracy-FIXED/#severity-p3-investigation-partially-fixed","title":"Severity: P3 (Investigation - Partially Fixed)","text":""},{"location":"archive/bugs/BUG-029-low-tcga-accuracy-FIXED/#status-fixed-agentconfig-default-max_steps-aligned-to-paper","title":"Status: Fixed (AgentConfig default max_steps aligned to paper)","text":""},{"location":"archive/bugs/BUG-029-low-tcga-accuracy-FIXED/#observation","title":"Observation","text":"<p>After fixing BUG-027/028, a 5-item TCGA benchmark run showed: - 0% accuracy (0/5 correct) - 0% extraction failures (down from 88%)</p> <p>This initially seemed suspicious, but investigation reveals this is expected behavior.</p>"},{"location":"archive/bugs/BUG-029-low-tcga-accuracy-FIXED/#evidence","title":"Evidence","text":""},{"location":"archive/bugs/BUG-029-low-tcga-accuracy-FIXED/#test-run-configuration","title":"Test Run Configuration","text":"<pre><code>max_steps: 2\nitems: 5\nprovider: openai (gpt-5.2)\n</code></pre>"},{"location":"archive/bugs/BUG-029-low-tcga-accuracy-FIXED/#results","title":"Results","text":"Item Predicted Truth Correct TCGA-06-0875-01Z-00-DX1 4 (Lung squamous) 1 (GBM) No TCGA-06-0875-01Z-00-DX2 4 (Lung squamous) 1 (GBM) No TCGA-08-0386-01Z-00-DX1 19 1 (GBM) No TCGA-08-0348-01Z-00-DX1 26 1 (GBM) No TCGA-12-3648-01Z-00-DX1 19 1 (GBM) No"},{"location":"archive/bugs/BUG-029-low-tcga-accuracy-FIXED/#model-reasoning-turn-2","title":"Model Reasoning (Turn 2)","text":"<pre><code>\"On the zoomed region, tissue is composed of cohesive nests/sheets of\natypical epithelial cells with high cellularity and prominent hemorrhage.\nThere is suggestion of keratinization/whorled squamoid nests rather than\ngland formation (no clear acini/papillae/mucin). This fits best with\nsquamous cell carcinoma\"\n</code></pre> <p>The model IS analyzing the histopathology and making a reasoned (but wrong) diagnosis.</p>"},{"location":"archive/bugs/BUG-029-low-tcga-accuracy-FIXED/#root-cause-analysis","title":"Root Cause Analysis","text":""},{"location":"archive/bugs/BUG-029-low-tcga-accuracy-FIXED/#1-sample-size","title":"1. Sample Size","text":"<ul> <li>5 samples is statistically insignificant</li> <li>Random chance on 30-way classification: 3.3%</li> <li>0/5 is not surprising with small samples</li> </ul>"},{"location":"archive/bugs/BUG-029-low-tcga-accuracy-FIXED/#2-max_steps-configuration","title":"2. max_steps Configuration","text":"<p>From the GIANT paper:</p> <p>\"For more challenging benchmarks such as TCGA and PANDA, accuracy continues to increase up to 20 iterations. Across datasets, we found that the best-performing configuration used T = 20.\"</p> <p>We ran with <code>max_steps=2</code>. The paper shows accuracy improves significantly with more navigation steps.</p>"},{"location":"archive/bugs/BUG-029-low-tcga-accuracy-FIXED/#3-paper-reported-accuracy","title":"3. Paper-Reported Accuracy","text":"Mode TCGA Accuracy Thumbnail baseline 9.2% Patch baseline 12.8% GIANT (T=20) 32.3% Random chance 3.3% <p>Even the paper's best GIANT configuration only achieves 32.3% on this 30-way classification task.</p>"},{"location":"archive/bugs/BUG-029-low-tcga-accuracy-FIXED/#conclusion","title":"Conclusion","text":"<p>This is NOT a bug. The 0% accuracy is explained by:</p> <ol> <li>Small sample (n=5): 0/5 is within expected variance for 32% base rate</li> <li>Low max_steps (2 vs 20): Paper shows accuracy improves with more steps</li> <li>Hard task: 30-way cancer classification from histopathology is inherently difficult</li> </ol> <p>The fixes (BUG-027/028) are working correctly: - Extraction failures: 88% \u2192 0% - Model sees correct options (verified in trajectory) - Model outputs valid JSON answers - Model provides medical reasoning</p>"},{"location":"archive/bugs/BUG-029-low-tcga-accuracy-FIXED/#fix-applied","title":"Fix Applied","text":"<p>Root cause: <code>AgentConfig.max_steps</code> defaulted to 5 (library/default), not 20 as paper specifies.</p> <p>Changes: - <code>src/giant/agent/runner.py:129</code>: Changed <code>max_steps: int = 5</code> to <code>max_steps: int = 20</code> - Updated docstring examples in <code>context.py</code> and <code>builder.py</code></p> <p>After fix: Model took 4 navigation steps before answering (vs forced at step 2).</p>"},{"location":"archive/bugs/BUG-029-low-tcga-accuracy-FIXED/#remaining-accuracy-gap","title":"Remaining Accuracy Gap","text":"<p>Even with T=20, accuracy remains low on small samples. This is expected: - Paper reports 32.3% on TCGA (30-way classification) - Random chance is 3.3% - Model shows genuine pathology reasoning but arrives at wrong diagnoses</p> <p>Example (TCGA-06-0875-01Z-00-DX1): - Model observed: \"abundant melanin pigment\" \u2192 predicted Melanoma (14) - Truth: Glioblastoma (1) - This is a reasoning error, not a bug</p>"},{"location":"archive/bugs/BUG-029-low-tcga-accuracy-FIXED/#verification-checklist","title":"Verification Checklist","text":"<ul> <li>[x] Options correctly parsed (737/737)</li> <li>[x] Options correctly injected into prompts</li> <li>[x] Model sees numbered options (verified in trajectory)</li> <li>[x] Model outputs valid JSON answer format</li> <li>[x] Extraction correctly maps answer to label</li> <li>[x] Model reasoning shows actual pathology analysis</li> </ul>"},{"location":"archive/bugs/BUG-029-low-tcga-accuracy-FIXED/#references","title":"References","text":"<ul> <li>GIANT Paper: \"diagnostic accuracy improves rapidly over the first few   iterations and plateaus after approximately 10\u201315 steps\"</li> <li>Previous benchmark (pre-fix): 88% extraction failures</li> <li>Current benchmark (post-fix): 0% extraction failures</li> </ul>"},{"location":"archive/bugs/BUG-030-implementation-audit-GITHUB-35/","title":"BUG-030: Comprehensive Implementation Audit vs GIANT Paper","text":""},{"location":"archive/bugs/BUG-030-implementation-audit-GITHUB-35/#severity-p2-multiple-potential-issues","title":"Severity: P2 (Multiple Potential Issues)","text":""},{"location":"archive/bugs/BUG-030-implementation-audit-GITHUB-35/#status-investigation-complete-awaiting-senior-review","title":"Status: Investigation Complete - Awaiting Senior Review","text":""},{"location":"archive/bugs/BUG-030-implementation-audit-GITHUB-35/#summary","title":"Summary","text":"<p>A thorough codebase audit comparing our implementation against the GIANT paper has been conducted. This document catalogs all findings, both confirmed issues and verified-correct implementations, to establish a baseline for further investigation.</p>"},{"location":"archive/bugs/BUG-030-implementation-audit-GITHUB-35/#confirmed-issues","title":"Confirmed Issues","text":""},{"location":"archive/bugs/BUG-030-implementation-audit-GITHUB-35/#issue-1-missing-paper-prompts-high-priority","title":"ISSUE-1: Missing Paper Prompts (HIGH PRIORITY)","text":"<p>Evidence from Paper:</p> <p>\"The system prompt used for OpenAI and Anthropic models is included in the Supplementary Material.\" (Section 4.1, line 142)</p> <p>Current Implementation: - <code>src/giant/prompts/templates.py</code> uses a custom prompt (lines 24-63) - The prompt is labeled \"Paper-derived (pending Supplementary Material verification)\" - We do NOT have access to the exact prompts from the Supplementary Material</p> <p>Impact: Unknown - our prompts may differ significantly from what was used to achieve 32.3% accuracy.</p> <p>Status: Needs Supplementary Material access or author contact.</p>"},{"location":"archive/bugs/BUG-030-implementation-audit-GITHUB-35/#issue-2-prompt-format-differences-medium-priority","title":"ISSUE-2: Prompt Format Differences (MEDIUM PRIORITY)","text":"<p>Paper Specification (Algorithm 1): - Step 1 prompt: \"nav instructions + at most T-1 crops\" (line 156) - Action format: <code>(rt, at) \u2190 LMM(C)</code> where <code>at = (x, y, w, h)</code> (line 159)</p> <p>Current Implementation: Our prompt says: <pre><code>Navigation Budget: Step {step} of {max_steps}. You have at most {remaining_crops} crops remaining.\n</code></pre></p> <p>Potential Issues: 1. The paper says \"at most T-1 crops\" - we say \"remaining_crops\" which is <code>max_steps - step</code> 2. At step 1 with T=20, we show 19 remaining crops, which matches \"T-1\" \u2713 3. However, the paper does NOT specify whether steps are 1-indexed or 0-indexed</p> <p>Impact: Low - logic appears correct.</p>"},{"location":"archive/bugs/BUG-030-implementation-audit-GITHUB-35/#issue-3-tcga-specific-questions-format-high-priority","title":"ISSUE-3: TCGA-Specific Questions Format (HIGH PRIORITY)","text":"<p>Paper (Figure 3): - TCGA: 30-way classification with \"Balanced Accuracy\" metric - Questions appear to be simple prompts like \"What is the primary diagnosis?\"</p> <p>Current Implementation: - MultiPathQA provides per-row prompts and options in <code>data/multipathqa/MultiPathQA.csv</code>.   - For <code>tcga</code> and <code>gtex</code>, the CSV prompt includes an <code>{options}</code> placeholder and explicitly     asks for the number of the correct option (with an example JSON snippet like     <code>{\"answer\": YOUR_ANSWER}</code>).   - For <code>tcga_slidebench</code> and <code>tcga_expert_vqa</code>, the CSV prompt is usually just the question,     so our loader appends a numbered options block and asks the model to respond with the     1-based option index.</p> <p>Example (TCGA): <pre><code>What is the primary diagnosis for this histopathology image?\n\nSelect from the following options:\n1. Glioblastoma multiforme\n2. Ovarian serous cystadenocarcinoma\n...\n30. Thymoma\n\nPlease choose the number of the correct option.\n</code></pre></p> <p>Paper Evidence: - No explicit evidence of how options were presented - SlideChat paper uses similar multiple-choice format</p> <p>Impact: Medium - format may affect model reasoning.</p>"},{"location":"archive/bugs/BUG-030-implementation-audit-GITHUB-35/#issue-4-claude-image-size-confirmed-correct","title":"ISSUE-4: Claude Image Size (CONFIRMED CORRECT)","text":"<p>Paper (Table 1, footnote):</p> <p>\"Due to pricing differences with the Claude API for images, we provided cropped images as 500 px instead of 1000 px.\"</p> <p>Current Implementation: - <code>src/giant/config.py:79</code>: <code>IMAGE_SIZE_ANTHROPIC: int = 500</code> \u2713</p> <p>Status: CORRECT</p>"},{"location":"archive/bugs/BUG-030-implementation-audit-GITHUB-35/#issue-5-thumbnail-size-medium-priority","title":"ISSUE-5: Thumbnail Size (MEDIUM PRIORITY)","text":"<p>Paper Evidence: - Figure 2 caption: \"In the thumbnail setting, the model receives a 1024\u00d71024 px image\" - Section 4.1: \"overlaid with four evenly spaced axis guides\"</p> <p>Current Implementation: - <code>src/giant/config.py:65</code>: <code>THUMBNAIL_SIZE: int = 1024</code> \u2713 - <code>src/giant/geometry/overlay.py</code>: Adds axis guides \u2713</p> <p>Status: CORRECT</p>"},{"location":"archive/bugs/BUG-030-implementation-audit-GITHUB-35/#issue-6-crop-target-size-needs-verification","title":"ISSUE-6: Crop Target Size (NEEDS VERIFICATION)","text":"<p>Paper (Section 4.1, line 136):</p> <p>\"We choose the pyramid level that will render the crop to a target long side (default S=1000 px)\"</p> <p>Current Implementation: - <code>src/giant/config.py:62</code>: <code>WSI_LONG_SIDE_TARGET: int = 1000</code> \u2713 - <code>src/giant/config.py:78</code>: <code>IMAGE_SIZE_OPENAI: int = 1000</code> \u2713</p> <p>However: - For Anthropic, crops are 500px per IMAGE_SIZE_ANTHROPIC - This matches the paper footnote</p> <p>Status: CORRECT</p>"},{"location":"archive/bugs/BUG-030-implementation-audit-GITHUB-35/#issue-7-oversampling-bias-confirmed-correct","title":"ISSUE-7: Oversampling Bias (CONFIRMED CORRECT)","text":"<p>Paper (Section 4.1, line 136):</p> <p>\"biasing toward finer levels (oversampling bias 0.85)\"</p> <p>Current Implementation: - <code>src/giant/config.py:64</code>: <code>OVERSAMPLING_BIAS: float = 0.85</code> \u2713 - <code>src/giant/core/level_selector.py</code>: Implements bias correctly \u2713</p> <p>Status: CORRECT</p>"},{"location":"archive/bugs/BUG-030-implementation-audit-GITHUB-35/#issue-8-navigation-steps-t20-fixed-in-bug-029","title":"ISSUE-8: Navigation Steps T=20 (FIXED in BUG-029)","text":"<p>Paper (Section 5.2, line 214):</p> <p>\"Across datasets, we found that the best-performing configuration used T = 20.\"</p> <p>Current Implementation (after fix): - <code>src/giant/agent/runner.py:129</code>: <code>max_steps: int = 20</code> \u2713</p> <p>Status: FIXED</p>"},{"location":"archive/bugs/BUG-030-implementation-audit-GITHUB-35/#issue-9-bootstrap-replicates-confirmed-correct","title":"ISSUE-9: Bootstrap Replicates (CONFIRMED CORRECT)","text":"<p>Paper (Table 1):</p> <p>\"Std. dev. from 1000 bootstrap replicates\"</p> <p>Current Implementation: - <code>src/giant/config.py:72</code>: <code>BOOTSTRAP_REPLICATES: int = 1000</code> \u2713</p> <p>Status: CORRECT</p>"},{"location":"archive/bugs/BUG-030-implementation-audit-GITHUB-35/#issue-10-patch-baseline-configuration-confirmed-correct","title":"ISSUE-10: Patch Baseline Configuration (CONFIRMED CORRECT)","text":"<p>Paper (Section 4.2.1, line 184):</p> <p>\"Following SlideChat [8], we sample 30 random 224\u00d7224 patches\"</p> <p>Current Implementation: - <code>src/giant/vision/constants.py</code>: <code>N_PATCHES: int = 30</code> \u2713 - <code>src/giant/vision/constants.py</code>: <code>PATCH_SIZE: int = 224</code> \u2713</p> <p>Status: CORRECT</p>"},{"location":"archive/bugs/BUG-030-implementation-audit-GITHUB-35/#potential-issues-requiring-further-investigation","title":"Potential Issues Requiring Further Investigation","text":""},{"location":"archive/bugs/BUG-030-implementation-audit-GITHUB-35/#investigate-1-answer-extraction-logic","title":"INVESTIGATE-1: Answer Extraction Logic","text":"<p>Observation: The trajectory shows the model outputting <code>{\"answer\": 14}</code> for melanoma.</p> <p>Current Extraction: - <code>src/giant/eval/answer_extraction.py</code> looks for integers in text - The JSON answer format <code>{\"answer\": 14}</code> is parsed correctly</p> <p>But: - The paper doesn't specify expected answer format - We force JSON output via structured output schema - This may differ from how the paper evaluated responses</p> <p>Recommendation: Verify answer extraction matches paper methodology.</p>"},{"location":"archive/bugs/BUG-030-implementation-audit-GITHUB-35/#investigate-2-axis-guide-implementation","title":"INVESTIGATE-2: Axis Guide Implementation","text":"<p>Paper:</p> <p>\"four evenly spaced axis guides along each dimension, labeled with absolute level-0 pixel coordinates\"</p> <p>Current Implementation: - <code>src/giant/geometry/overlay.py</code> draws axis guides</p> <p>Concern: - Are our axis guides matching exactly what the paper used? - Font size, color, positioning could affect LLM's ability to read coordinates</p> <p>Recommendation: Visual comparison with paper figures needed.</p>"},{"location":"archive/bugs/BUG-030-implementation-audit-GITHUB-35/#investigate-3-clam-pipeline-for-baselines","title":"INVESTIGATE-3: CLAM Pipeline for Baselines","text":"<p>Paper (Section 4.2.1):</p> <p>\"we use the CLAM Python package to segment the tissue on the slide before patching\"</p> <p>Current Implementation: - <code>src/giant/vision/segmentation.py</code> uses custom Otsu-based segmentation - Does NOT use CLAM package</p> <p>Impact: For GIANT mode, this is irrelevant (we don't use CLAM). For baseline modes (thumbnail/patch), results may differ.</p> <p>Recommendation: Verify if custom segmentation is equivalent to CLAM.</p>"},{"location":"archive/bugs/BUG-030-implementation-audit-GITHUB-35/#investigate-4-model-temperaturesampling","title":"INVESTIGATE-4: Model Temperature/Sampling","text":"<p>Paper: No mention of temperature settings.</p> <p>Current Implementation: - Temperature not explicitly set in LLM clients - Defaults to provider defaults</p> <p>Recommendation: Verify paper used default temperature or specify.</p>"},{"location":"archive/bugs/BUG-030-implementation-audit-GITHUB-35/#investigate-5-early-stopping","title":"INVESTIGATE-5: Early Stopping","text":"<p>Paper (Algorithm 1):</p> <p>\"repeating until a step limit T or early stop\"</p> <p>Current Implementation: - Agent stops when model outputs <code>answer()</code> action - This matches paper behavior</p> <p>Status: Likely CORRECT</p>"},{"location":"archive/bugs/BUG-030-implementation-audit-GITHUB-35/#verified-correct-implementations","title":"Verified Correct Implementations","text":"Component Paper Spec Implementation Status Max steps (T) 20 <code>max_steps=20</code> \u2713 Fixed Crop size OpenAI (S) 1000px <code>IMAGE_SIZE_OPENAI=1000</code> \u2713 Crop size Anthropic 500px <code>IMAGE_SIZE_ANTHROPIC=500</code> \u2713 Thumbnail size 1024px <code>THUMBNAIL_SIZE=1024</code> \u2713 Oversampling bias 0.85 <code>OVERSAMPLING_BIAS=0.85</code> \u2713 Bootstrap replicates 1000 <code>BOOTSTRAP_REPLICATES=1000</code> \u2713 Patch size (baseline) 224px <code>PATCH_SIZE=224</code> \u2713 Patch count (baseline) 30 <code>N_PATCHES=30</code> \u2713 Lanczos resampling Yes PIL Lanczos \u2713 Balanced accuracy TCGA/PANDA/GTEx Implemented \u2713"},{"location":"archive/bugs/BUG-030-implementation-audit-GITHUB-35/#recommendations","title":"Recommendations","text":""},{"location":"archive/bugs/BUG-030-implementation-audit-GITHUB-35/#immediate-actions-before-more-testing","title":"Immediate Actions (Before More Testing)","text":"<ol> <li> <p>Get Supplementary Material: Contact paper authors or find supplementary material for exact prompts used.</p> </li> <li> <p>Verify Answer Format: Confirm the paper's expected answer format matches our JSON schema.</p> </li> <li> <p>Run Full TCGA Benchmark: With T=20 fixed, run full 221-item TCGA benchmark to compare against paper's 32.3%.</p> </li> </ol>"},{"location":"archive/bugs/BUG-030-implementation-audit-GITHUB-35/#medium-term-actions","title":"Medium-Term Actions","text":"<ol> <li> <p>Axis Guide Audit: Compare our axis guide rendering with paper figures visually.</p> </li> <li> <p>CLAM Integration: Consider adding CLAM segmentation for baseline mode parity.</p> </li> <li> <p>Temperature Testing: Test with explicit temperature=0 for reproducibility.</p> </li> </ol>"},{"location":"archive/bugs/BUG-030-implementation-audit-GITHUB-35/#test-commands-for-validation","title":"Test Commands for Validation","text":"<pre><code># Run TCGA benchmark with T=20 (paper configuration)\ngiant benchmark tcga --max-items=50 --provider=openai --model=gpt-5.2\n\n# Expected: ~32% balanced accuracy (paper reports 32.3% \u00b1 3.5%)\n# Current: 0% on small samples (need larger sample)\n\n# Run with verbose logging to verify prompts\nGIANT_LOG_LEVEL=DEBUG giant run /path/to/tcga/slide.svs -q \"What is the diagnosis?\"\n</code></pre>"},{"location":"archive/bugs/BUG-030-implementation-audit-GITHUB-35/#references","title":"References","text":"<ul> <li>GIANT Paper: \"Navigating Gigapixel Pathology Images with Large Multimodal Models\"</li> <li>Paper Section 4.1: Algorithm details</li> <li>Paper Section 5.2: Performance scaling with iterations</li> <li>Paper Table 1: Benchmark results</li> <li>Paper Footnote Table 1: Claude 500px image size</li> </ul>"},{"location":"archive/bugs/BUG-030-implementation-audit-GITHUB-35/#conclusion","title":"Conclusion","text":"<p>The implementation appears largely faithful to the paper specifications. The main concerns are:</p> <ol> <li>Prompts: We don't have access to the exact prompts used in the paper.</li> <li>Sample Size: Our tests used tiny samples (1-5 items) vs paper's full benchmarks.</li> <li>Model Versions: Paper used \"GPT-5\" - we use <code>gpt-5.2</code> which should be equivalent or better.</li> </ol> <p>Next Step: Run full TCGA benchmark (221 items) with T=20 to validate accuracy matches paper's 32.3%.</p>"},{"location":"archive/bugs/BUG-031-answer-extraction-multiple-integers-FIXED/","title":"BUG-031: Answer Extraction Fails When Prediction Contains Multiple Integers","text":""},{"location":"archive/bugs/BUG-031-answer-extraction-multiple-integers-FIXED/#severity-p2-evaluation-correctness-robustness","title":"Severity: P2 (Evaluation correctness / robustness)","text":""},{"location":"archive/bugs/BUG-031-answer-extraction-multiple-integers-FIXED/#status-fixed","title":"Status: Fixed","text":""},{"location":"archive/bugs/BUG-031-answer-extraction-multiple-integers-FIXED/#summary","title":"Summary","text":"<p>For multiple-choice benchmarks, <code>extract_label()</code> previously used the first integer found in <code>answer_text</code> to determine the selected option. If the prediction contained an out-of-range integer before the real option index (e.g., level-0 coordinates like <code>15000</code>), extraction would fail and incorrectly mark the item as an extraction failure.</p>"},{"location":"archive/bugs/BUG-031-answer-extraction-multiple-integers-FIXED/#root-cause","title":"Root Cause","text":"<p><code>src/giant/eval/answer_extraction.py</code> used: - <code>_INT_RE.search(text)</code> \u2192 first integer only - If that integer was out of range, it did not search for any subsequent integers.</p>"},{"location":"archive/bugs/BUG-031-answer-extraction-multiple-integers-FIXED/#fix","title":"Fix","text":"<p><code>src/giant/eval/answer_extraction.py</code> now iterates all integers in the prediction and selects the first integer that is within <code>1..len(options)</code>.</p>"},{"location":"archive/bugs/BUG-031-answer-extraction-multiple-integers-FIXED/#verification","title":"Verification","text":"<ul> <li>Added regression test: <code>tests/unit/eval/test_answer_extraction.py</code> (<code>test_multiple_integers_prefers_first_in_range</code>).</li> </ul>"},{"location":"archive/bugs/BUG-032-placeholder-api-keys-treated-as-configured-FIXED/","title":"BUG-032: Placeholder API Keys Treated as Configured (Leads to Repeated 401s)","text":""},{"location":"archive/bugs/BUG-032-placeholder-api-keys-treated-as-configured-FIXED/#severity-p3-developer-ux-cost-prevention","title":"Severity: P3 (Developer UX / cost-prevention)","text":""},{"location":"archive/bugs/BUG-032-placeholder-api-keys-treated-as-configured-FIXED/#status-fixed","title":"Status: Fixed","text":""},{"location":"archive/bugs/BUG-032-placeholder-api-keys-treated-as-configured-FIXED/#summary","title":"Summary","text":"<p><code>Settings.require_*_key()</code> treated any non-empty string as a configured secret. If a developer left a placeholder value (e.g., <code>\"your-key-here\"</code>) in <code>.env</code>, GIANT would attempt live API calls and fail with repeated <code>401 invalid_api_key</code> errors instead of failing fast with a clear configuration error.</p>"},{"location":"archive/bugs/BUG-032-placeholder-api-keys-treated-as-configured-FIXED/#root-cause","title":"Root Cause","text":"<p><code>src/giant/config.py:Settings._is_configured_secret()</code> only rejected <code>None</code> and blank strings.</p>"},{"location":"archive/bugs/BUG-032-placeholder-api-keys-treated-as-configured-FIXED/#fix","title":"Fix","text":"<p><code>src/giant/config.py</code> now rejects obvious placeholder key values (currently: strings containing <code>\"your-key\"</code> or <code>\"changeme\"</code>), causing <code>require_*_key()</code> to raise <code>ConfigError</code> before any API call is attempted.</p>"},{"location":"archive/bugs/BUG-032-placeholder-api-keys-treated-as-configured-FIXED/#verification","title":"Verification","text":"<ul> <li>Extended config unit tests in <code>tests/unit/test_config.py</code> to ensure placeholder keys are   rejected for OpenAI and Anthropic.</li> </ul>"},{"location":"archive/bugs/BUG-033-make-benchmark-uses-wrong-cli-args/","title":"BUG-033: <code>make benchmark</code> Uses Wrong CLI Args (Dataset vs Path)","text":""},{"location":"archive/bugs/BUG-033-make-benchmark-uses-wrong-cli-args/#severity-p0-blocks-benchmark-runs-via-makefile","title":"Severity: P0 (Blocks benchmark runs via Makefile)","text":""},{"location":"archive/bugs/BUG-033-make-benchmark-uses-wrong-cli-args/#status-fixed-2025-12-26","title":"Status: \u2705 Fixed (2025-12-26)","text":""},{"location":"archive/bugs/BUG-033-make-benchmark-uses-wrong-cli-args/#summary","title":"Summary","text":"<p><code>make benchmark</code> currently calls <code>uv run giant benchmark ./data/multipathqa --output-dir ./results</code>, but <code>giant benchmark</code> expects a dataset name (<code>tcga</code>, <code>gtex</code>, <code>panda</code>, <code>tcga_expert_vqa</code>, <code>tcga_slidebench</code>). This blocks the intended \u201cone-command\u201d reproduction workflow and causes confusing failures (often a <code>ConfigError</code> for missing API keys before the dataset validation error).</p>"},{"location":"archive/bugs/BUG-033-make-benchmark-uses-wrong-cli-args/#repro","title":"Repro","text":"<pre><code>make benchmark\n</code></pre> <p>Observed: - <code>dataset</code> is passed as <code>./data/multipathqa</code> (invalid). - If <code>OPENAI_API_KEY</code> is missing, the command fails before reporting the invalid dataset.</p>"},{"location":"archive/bugs/BUG-033-make-benchmark-uses-wrong-cli-args/#root-cause","title":"Root Cause","text":"<ul> <li>Drift between Spec-12 CLI (<code>giant benchmark &lt;dataset&gt;</code>) and the <code>Makefile</code>/docs example.</li> <li><code>src/giant/cli/runners.py:run_benchmark()</code> constructs an LLM provider (requiring API keys) before validating <code>dataset</code>.</li> </ul>"},{"location":"archive/bugs/BUG-033-make-benchmark-uses-wrong-cli-args/#proposed-fix","title":"Proposed Fix","text":"<ol> <li>Makefile</li> <li>Change <code>benchmark</code> target to accept a dataset name and include explicit <code>--csv-path</code>/<code>--wsi-root</code>.</li> <li>Recommended pattern:<ul> <li><code>DATASET ?= tcga</code></li> <li><code>uv run giant benchmark $(DATASET) --csv-path data/multipathqa/MultiPathQA.csv --wsi-root data/wsi --output-dir results</code></li> </ul> </li> <li> <p>Optionally add convenience targets: <code>benchmark-tcga</code>, <code>benchmark-gtex</code>, <code>benchmark-panda</code>, etc.</p> </li> <li> <p>Docs</p> </li> <li> <p>Update <code>docs/specs/spec-01-foundation.md</code> and <code>AGENTS.md</code> to match the corrected command.</p> </li> <li> <p>CLI Runner Fail-Fast</p> </li> <li>In <code>src/giant/cli/runners.py:run_benchmark()</code>, validate <code>dataset in BENCHMARK_TASKS</code> before <code>create_provider(...)</code> so invalid datasets error without requiring API keys.</li> </ol>"},{"location":"archive/bugs/BUG-033-make-benchmark-uses-wrong-cli-args/#acceptance-criteria","title":"Acceptance Criteria","text":"<ul> <li><code>make benchmark</code> passes a valid dataset name (default <code>tcga</code> or via <code>DATASET=...</code>).</li> <li><code>uv run giant benchmark not_a_dataset ...</code> fails with \u201cUnknown dataset \u2026\u201d even when API keys are missing.</li> </ul>"},{"location":"archive/bugs/BUG-033-make-benchmark-uses-wrong-cli-args/#test-plan","title":"Test Plan","text":"<ul> <li>Unit: add a test that calls the benchmark runner with an invalid dataset and asserts it raises a dataset error (not <code>ConfigError</code>).</li> <li>Manual: run <code>make --dry-run benchmark DATASET=tcga</code> (or add a <code>benchmark-help</code> / <code>benchmark-smoke</code> target) to confirm argument wiring.</li> </ul>"},{"location":"archive/bugs/BUG-034-default-tests-trigger-live-api/","title":"BUG-034: Default Test Targets Can Trigger Live API Calls","text":""},{"location":"archive/bugs/BUG-034-default-tests-trigger-live-api/#severity-p0-cost-accidental-network-calls","title":"Severity: P0 (Cost + accidental network calls)","text":""},{"location":"archive/bugs/BUG-034-default-tests-trigger-live-api/#status-fixed-2025-12-26","title":"Status: \u2705 Fixed (2025-12-26)","text":""},{"location":"archive/bugs/BUG-034-default-tests-trigger-live-api/#summary","title":"Summary","text":"<p><code>make test</code> (and therefore <code>make check</code>) runs <code>uv run pytest</code> without excluding <code>live</code>/<code>cost</code> tests. If a developer has <code>OPENAI_API_KEY</code> and/or <code>ANTHROPIC_API_KEY</code> configured (shell env or <code>.env</code>), pytest will run marked live tests and spend money.</p>"},{"location":"archive/bugs/BUG-034-default-tests-trigger-live-api/#repro","title":"Repro","text":"<pre><code># With real keys configured\nmake test\n</code></pre>"},{"location":"archive/bugs/BUG-034-default-tests-trigger-live-api/#root-cause","title":"Root Cause","text":"<ul> <li>Live tests in <code>tests/integration/llm/test_p0_critical.py</code> are only gated by \u201ckey is present\u201d, not by an explicit opt-in flag.</li> <li>Make targets do not exclude <code>live</code>/<code>cost</code> markers.</li> </ul>"},{"location":"archive/bugs/BUG-034-default-tests-trigger-live-api/#proposed-fix","title":"Proposed Fix","text":"<ol> <li>Safe defaults in Makefile</li> <li> <p>Update these targets to exclude live/cost by default:</p> <ul> <li><code>test</code>: <code>uv run pytest -m \"not live and not cost\"</code></li> <li><code>test-cov</code>: same marker exclusion plus coverage flags</li> <li><code>test-watch</code>: same marker exclusion</li> </ul> </li> <li> <p>Explicit opt-in targets</p> </li> <li>Add targets that intentionally run live tests:<ul> <li><code>test-live</code>: <code>uv run pytest -m \"live\"</code></li> <li>Optionally split by provider: <code>test-live-openai</code>, <code>test-live-anthropic</code></li> </ul> </li> <li> <p>Document these in <code>AGENTS.md</code> and/or <code>README.md</code>.</p> </li> <li> <p>Defense-in-depth: require an explicit env opt-in</p> </li> <li>Update live test <code>skipif</code> gates to require both a configured key and an explicit environment variable, e.g. <code>GIANT_RUN_LIVE_TESTS=1</code>.</li> <li>Keep <code>-m live</code> as a second explicit opt-in (marker selection).</li> </ol>"},{"location":"archive/bugs/BUG-034-default-tests-trigger-live-api/#acceptance-criteria","title":"Acceptance Criteria","text":"<ul> <li>With keys configured, <code>make test</code> and <code>make check</code> do not run any <code>live</code> or <code>cost</code> tests.</li> <li>Live tests only run when explicitly requested (e.g., <code>GIANT_RUN_LIVE_TESTS=1 uv run pytest -m live</code>).</li> </ul>"},{"location":"archive/bugs/BUG-034-default-tests-trigger-live-api/#test-plan","title":"Test Plan","text":"<ul> <li>Unit: add a small test for <code>_has_openai_key()</code>-style gating (or refactor to a shared helper) verifying that keys alone are insufficient without <code>GIANT_RUN_LIVE_TESTS=1</code>.</li> <li>Manual: set a dummy key in <code>.env</code>, run <code>make test</code> and confirm no network calls occur.</li> </ul>"},{"location":"archive/bugs/BUG-035-cli-tracebacks-leak-secrets/","title":"BUG-035: CLI Exceptions May Leak API Keys in Tracebacks","text":""},{"location":"archive/bugs/BUG-035-cli-tracebacks-leak-secrets/#severity-p1-security-secret-leakage","title":"Severity: P1 (Security / secret leakage)","text":""},{"location":"archive/bugs/BUG-035-cli-tracebacks-leak-secrets/#status-fixed-2025-12-26","title":"Status: \u2705 Fixed (2025-12-26)","text":""},{"location":"archive/bugs/BUG-035-cli-tracebacks-leak-secrets/#summary","title":"Summary","text":"<p>When the CLI crashes (e.g., misconfiguration, missing API key, invalid args), Typer/Rich pretty tracebacks can include locals, which may contain <code>Settings</code> objects and therefore API keys. This is especially risky when users paste logs into issues or share terminal output.</p>"},{"location":"archive/bugs/BUG-035-cli-tracebacks-leak-secrets/#repro","title":"Repro","text":"<ol> <li>Ensure at least one provider key is configured (e.g., <code>ANTHROPIC_API_KEY</code>).</li> <li>Trigger a CLI error (e.g., run a command that raises <code>ConfigError</code> for a different missing key).</li> <li>Observe that the traceback may print local variables and include the configured key value.</li> </ol>"},{"location":"archive/bugs/BUG-035-cli-tracebacks-leak-secrets/#root-cause","title":"Root Cause","text":"<ul> <li>Typer\u2019s Rich exception formatting can show locals by default.</li> <li><code>giant.config.Settings</code> stores secrets as plain <code>str | None</code>, so printing locals can expose them.</li> </ul>"},{"location":"archive/bugs/BUG-035-cli-tracebacks-leak-secrets/#proposed-fix","title":"Proposed Fix","text":"<ol> <li>Disable locals in pretty exceptions</li> <li> <p>In <code>src/giant/cli/main.py</code>, set Typer options:</p> <ul> <li><code>pretty_exceptions_show_locals=False</code></li> <li>Optionally <code>pretty_exceptions_short=True</code></li> </ul> </li> <li> <p>Defense-in-depth: mask secrets in <code>Settings</code></p> </li> <li>Consider switching key fields to <code>pydantic.SecretStr</code> (requires updating <code>require_*_key()</code> call sites to use <code>.get_secret_value()</code>).</li> <li> <p>Alternatively, keep string types but ensure any custom logging never prints key values.</p> </li> <li> <p>Regression test</p> </li> <li>Add a CLI test that:<ul> <li>Sets an environment variable like <code>ANTHROPIC_API_KEY=\"sk-ant-TESTSECRET\"</code>.</li> <li>Runs a CLI command expected to error.</li> <li>Asserts the output does not contain <code>TESTSECRET</code>.</li> </ul> </li> </ol>"},{"location":"archive/bugs/BUG-035-cli-tracebacks-leak-secrets/#acceptance-criteria","title":"Acceptance Criteria","text":"<ul> <li>CLI errors never print API key material (even when <code>-vv</code> is used).</li> <li>Logs and tracebacks remain actionable without showing locals that include secrets.</li> </ul>"},{"location":"archive/bugs/BUG-036-wsi-readme-verification-gdc-layout/","title":"BUG-036: <code>data/wsi/README.md</code> Verification Assumes Flat TCGA Layout","text":""},{"location":"archive/bugs/BUG-036-wsi-readme-verification-gdc-layout/#severity-p1-docs-cause-false-missing-data-signals","title":"Severity: P1 (Docs cause false \"missing data\" signals)","text":""},{"location":"archive/bugs/BUG-036-wsi-readme-verification-gdc-layout/#status-fixed-2025-12-26","title":"Status: \u2705 Fixed (2025-12-26)","text":""},{"location":"archive/bugs/BUG-036-wsi-readme-verification-gdc-layout/#summary","title":"Summary","text":"<p><code>data/wsi/README.md</code> includes a verification loop that checks for files at <code>data/wsi/tcga/&lt;filename&gt;.svs</code>. This fails when TCGA slides are downloaded using <code>gdc-client</code>, which typically stores files under <code>data/wsi/tcga/&lt;file_id&gt;/&lt;uuid-suffixed filename&gt;.svs</code> (the recommended layout in <code>docs/data-acquisition.md</code> and supported by <code>WSIPathResolver</code>).</p>"},{"location":"archive/bugs/BUG-036-wsi-readme-verification-gdc-layout/#repro","title":"Repro","text":"<ol> <li>Download TCGA slides using <code>gdc-client download -d data/wsi/tcga/</code> (default layout).</li> <li>Run the \u201cCheck against file lists\u201d loop in <code>data/wsi/README.md</code>.</li> <li>It reports many missing files even though <code>giant check-data tcga</code> can resolve them.</li> </ol>"},{"location":"archive/bugs/BUG-036-wsi-readme-verification-gdc-layout/#root-cause","title":"Root Cause","text":"<ul> <li>Documentation drift: the README uses a flat-path check, but the codebase supports and recommends the per-<code>file_id</code> directory structure.</li> </ul>"},{"location":"archive/bugs/BUG-036-wsi-readme-verification-gdc-layout/#proposed-fix","title":"Proposed Fix","text":"<ol> <li>Prefer resolver-based verification</li> <li> <p>Update <code>data/wsi/README.md</code> to recommend:</p> <ul> <li><code>uv run giant check-data tcga</code></li> <li><code>uv run giant check-data tcga_expert_vqa</code></li> <li><code>uv run giant check-data tcga_slidebench</code></li> </ul> </li> <li> <p>If keeping a shell-based loop</p> </li> <li> <p>Either remove the flat <code>-f tcga/$f</code> loop, or rewrite it to:</p> <ul> <li>Check for <code>tcga/&lt;file_id&gt;/*.svs</code> when <code>file_id</code> is available, or</li> <li>Use a stem glob (e.g., <code>TCGA-...-DX1.*.svs</code>) across subdirectories.</li> </ul> </li> <li> <p>Housekeeping</p> </li> <li>Update any \u201cSpec-12 will add a stable CLI\u201d language (CLI already exists).</li> </ol>"},{"location":"archive/bugs/BUG-036-wsi-readme-verification-gdc-layout/#acceptance-criteria","title":"Acceptance Criteria","text":"<ul> <li>Following <code>data/wsi/README.md</code> produces correct validation results for both:</li> <li>Flat layout (<code>tcga/&lt;filename&gt;.svs</code>), and</li> <li><code>gdc-client</code> layout (<code>tcga/&lt;file_id&gt;/&lt;uuid-suffixed filename&gt;.svs</code>).</li> </ul>"},{"location":"archive/bugs/BUG-037-data-acquisition-verification-no-pandas/","title":"BUG-037: <code>docs/data-acquisition.md</code> Verification Requires <code>pandas</code> (Not Installed)","text":""},{"location":"archive/bugs/BUG-037-data-acquisition-verification-no-pandas/#severity-p2-docs-friction-onboarding-failure","title":"Severity: P2 (Docs friction / onboarding failure)","text":""},{"location":"archive/bugs/BUG-037-data-acquisition-verification-no-pandas/#status-fixed-2025-12-26","title":"Status: \u2705 Fixed (2025-12-26)","text":""},{"location":"archive/bugs/BUG-037-data-acquisition-verification-no-pandas/#summary","title":"Summary","text":"<p><code>docs/data-acquisition.md</code> includes a verification snippet that imports <code>pandas</code>, but <code>pandas</code> is not listed in <code>pyproject.toml</code> dependencies. On a fresh setup (<code>uv sync</code>), the snippet fails and blocks users from validating their WSI downloads.</p>"},{"location":"archive/bugs/BUG-037-data-acquisition-verification-no-pandas/#repro","title":"Repro","text":"<pre><code>uv sync\npython -c \"import pandas as pd\"\n</code></pre>"},{"location":"archive/bugs/BUG-037-data-acquisition-verification-no-pandas/#root-cause","title":"Root Cause","text":"<ul> <li>Doc snippet was written assuming a data-science stack, but this repo intentionally avoids <code>pandas</code> as a hard dependency.</li> <li>The repo already provides a CLI-native verifier (<code>giant check-data</code>) that doesn\u2019t require extra packages.</li> </ul>"},{"location":"archive/bugs/BUG-037-data-acquisition-verification-no-pandas/#proposed-fix","title":"Proposed Fix","text":"<ol> <li>Replace the pandas-based snippet with CLI usage:</li> <li><code>uv run giant check-data tcga</code></li> <li><code>uv run giant check-data tcga_expert_vqa</code></li> <li><code>uv run giant check-data tcga_slidebench</code></li> <li><code>uv run giant check-data gtex</code></li> <li> <p><code>uv run giant check-data panda</code></p> </li> <li> <p>If a Python snippet is still desired, rewrite using stdlib:</p> </li> <li><code>csv.DictReader</code> + <code>pathlib.Path</code></li> <li>Use <code>giant.eval.wsi_resolver.WSIPathResolver</code> for parity with runtime behavior.</li> </ol>"},{"location":"archive/bugs/BUG-037-data-acquisition-verification-no-pandas/#acceptance-criteria","title":"Acceptance Criteria","text":"<ul> <li>All \u201cVerification\u201d instructions in <code>docs/data-acquisition.md</code> run on a fresh <code>uv sync</code> without extra installs.</li> <li>Verification instructions correctly handle <code>gdc-client</code> TCGA layout (per-<code>file_id</code> subdirectories).</li> </ul>"},{"location":"brainstorming/research-agent-sdks-vs-raw-api/","title":"Research: Agent SDKs vs Raw API Calls","text":"<p>Question: Should GIANT use an Agent SDK instead of hand-rolling LLM orchestration?</p>"},{"location":"brainstorming/research-agent-sdks-vs-raw-api/#tldr","title":"TL;DR","text":"<p>No, switching to an Agent SDK is not a root-cause fix for BUG-025 (the <code>input_text</code> vs <code>output_text</code> issue). It's an OpenAI Responses API content-type requirement; any wrapper still has to satisfy it.</p> <p>However, adopting an SDK could reduce future bugs and maintenance burden for: - Multi-turn conversation state management - Tool calling orchestration - Error handling and retries - Provider abstraction</p>"},{"location":"brainstorming/research-agent-sdks-vs-raw-api/#available-sdks-december-2025","title":"Available SDKs (December 2025)","text":""},{"location":"brainstorming/research-agent-sdks-vs-raw-api/#1-openai-agents-sdk","title":"1. OpenAI Agents SDK","text":"<ul> <li>Repo: openai/openai-agents-python</li> <li>Install: <code>pip install openai-agents</code></li> <li>What it does: Provides agent loop, handoffs, guardrails, sessions, tracing</li> <li>Key feature: Structured output via Pydantic models with <code>output_type</code> parameter</li> </ul> <pre><code>from agents import Agent, Runner\n\nagent = Agent(\n    name=\"GIANT\",\n    instructions=\"Navigate WSI slides...\",\n    tools=[crop_tool, answer_tool],\n    output_type=StepResponse,  # Pydantic model\n)\n\nresult = await Runner.run(agent, messages)\n</code></pre> <p>Pros: - Built-in agent loop (no manual orchestration) - Automatic structured output parsing - Provider-agnostic (supports 100+ LLMs) - Built-in tracing for debugging</p> <p>Cons: - Another dependency to maintain - May not fit our specific WSI navigation loop exactly - Learning curve</p>"},{"location":"brainstorming/research-agent-sdks-vs-raw-api/#2-claude-agent-sdk","title":"2. Claude Agent SDK","text":"<ul> <li>Repo: anthropics/claude-agent-sdk-python</li> <li>Install: <code>pip install claude-agent-sdk</code></li> <li>What it does: Same harness that powers Claude Code</li> <li>Architecture: Communicates via the (bundled) Claude Code CLI process</li> </ul> <pre><code>from claude_agent_sdk import ClaudeSDKClient\n\nclient = ClaudeSDKClient()\nresult = await client.query(\n    prompt=\"Navigate this slide...\",\n    tools=[crop_tool, answer_tool],\n)\n</code></pre> <p>Pros: - Battle-tested (powers Claude Code) - Built-in memory management for long-running tasks - MCP (Model Context Protocol) integration</p> <p>Cons: - Subprocess-based architecture (CLI orchestration) - Anthropic-specific (not provider-agnostic)</p>"},{"location":"brainstorming/research-agent-sdks-vs-raw-api/#3-langchain-langgraph","title":"3. LangChain / LangGraph","text":"<ul> <li>Install: <code>pip install langchain langgraph</code></li> <li>What it does: General-purpose LLM framework with agent orchestration</li> </ul> <p>Pros: - Mature ecosystem - Provider-agnostic - Graph-based workflow definition (LangGraph)</p> <p>Cons: - Heavy abstraction layer - Frequent breaking changes historically - Overkill for our simple loop</p>"},{"location":"brainstorming/research-agent-sdks-vs-raw-api/#4-llamaindex","title":"4. LlamaIndex","text":"<ul> <li>Focus: RAG and data retrieval, not agent orchestration</li> <li>Not relevant for GIANT's use case (we're doing navigation, not retrieval)</li> </ul>"},{"location":"brainstorming/research-agent-sdks-vs-raw-api/#what-we-hand-rolled-vs-what-sdks-provide","title":"What We Hand-Rolled vs What SDKs Provide","text":"Component GIANT (Current) OpenAI Agents SDK Claude Agent SDK Agent loop <code>GIANTAgent.run()</code> <code>Runner.run()</code> <code>client.query()</code> Multi-turn state <code>ContextManager</code> <code>Sessions</code> Built-in memory Structured output Manual JSON schema <code>output_type</code> Pydantic Tool responses Tool calling Manual in prompt <code>tools</code> parameter MCP tools Retries <code>tenacity</code> decorator Built-in Built-in Circuit breaker <code>CircuitBreaker</code> class Not included Not included Provider abstraction <code>LLMProvider</code> Protocol Provider-agnostic Anthropic-only Tracing <code>structlog</code> Built-in tracing Built-in"},{"location":"brainstorming/research-agent-sdks-vs-raw-api/#would-an-sdk-have-prevented-bug-025","title":"Would an SDK Have Prevented BUG-025?","text":"<p>Not inherently. BUG-025 is about the OpenAI Responses API requiring different content types for user vs assistant messages (<code>input_text</code> vs <code>output_text</code>). This is:</p> <ol> <li>A low-level API format issue</li> <li>Specific to the Responses API (not Chat Completions)</li> <li>Would still need to be handled correctly in any SDK</li> </ol> <p>An SDK could have avoided us writing the buggy conversion layer (because the SDK already implements the correct wire format), but the underlying constraint still exists and must be satisfied by the SDK/wrapper.</p>"},{"location":"brainstorming/research-agent-sdks-vs-raw-api/#recommendation","title":"Recommendation","text":""},{"location":"brainstorming/research-agent-sdks-vs-raw-api/#keep-current-architecture-for-now","title":"Keep Current Architecture (For Now)","text":"<p>Reasons: 1. We're 90% done - All 12 specs implemented, one P0 bug to fix 2. Simple loop - GIANT's navigation loop is straightforward (crop \u2192 observe \u2192 decide) 3. Provider flexibility - We need Anthropic, OpenAI, AND Gemini support 4. Control - Our custom implementation lets us optimize for WSI-specific needs</p>"},{"location":"brainstorming/research-agent-sdks-vs-raw-api/#consider-sdk-migration-later-if","title":"Consider SDK Migration Later If:","text":"<ol> <li>We add complex multi-agent coordination (e.g., CONCH tool)</li> <li>We need persistent memory across sessions</li> <li>Maintenance burden of raw API calls becomes excessive</li> <li>We want to leverage SDK-specific features (tracing, guardrails)</li> </ol>"},{"location":"brainstorming/research-agent-sdks-vs-raw-api/#immediate-action-fix-bug-025","title":"Immediate Action: Fix BUG-025","text":"<p>The fix is simple (pass <code>role</code> to <code>message_content_to_openai()</code>). No architectural change needed.</p>"},{"location":"brainstorming/research-agent-sdks-vs-raw-api/#references","title":"References","text":"<ul> <li>OpenAI Agents SDK</li> <li>OpenAI Agents SDK GitHub</li> <li>Building agents with the Claude Agent SDK</li> <li>Claude Agent SDK GitHub</li> <li>Comparing Open-Source AI Agent Frameworks</li> <li>LangChain vs LlamaIndex Comparison</li> </ul>"},{"location":"bugs/","title":"Bug Tracking - GIANT WSI + LLM Pipeline","text":""},{"location":"bugs/#summary","title":"Summary","text":"<p>This directory tracks bugs discovered during integration checkpoint audits.</p> <p>Archive: Fixed bugs are moved to <code>archive/</code> to keep the active list clean.</p>"},{"location":"bugs/#active-bugs","title":"Active Bugs","text":"<p>All bugs have been migrated to GitHub Issues for tracking:</p> <ul> <li>GitHub Issues</li> </ul> ID Severity Title GitHub Issue BUG-018 P3 Missing CONCH tool integration #33 BUG-020 P3 Official system prompts not incorporated #34 BUG-030 P2 Implementation audit findings #35"},{"location":"bugs/#local-audit-findings-not-yet-filed","title":"Local Audit Findings (Not Yet Filed)","text":"<p>No pending local audit findings. All benchmark-prep bugs (BUG-033 through BUG-037) have been fixed and archived.</p>"},{"location":"bugs/#archived-fixed-bugs","title":"Archived (Fixed) Bugs","text":"<p>See <code>archive/</code> for historical bugs that have been resolved:</p> ID Title Resolution BUG-037 Data acquisition verification requires <code>pandas</code> Fixed (use <code>giant check-data</code> CLI instead) BUG-036 WSI README verification assumes flat TCGA layout Fixed (recommend <code>giant check-data</code>, handles both layouts) BUG-035 CLI exceptions may leak API keys in tracebacks Fixed (<code>pretty_exceptions_show_locals=False</code>) BUG-034 Default tests trigger live API calls Fixed (exclude live/cost markers + <code>GIANT_RUN_LIVE_TESTS</code> gate) BUG-033 <code>make benchmark</code> uses wrong CLI args Fixed (Makefile uses <code>$(DATASET)</code> + fail-fast validation) BUG-032 Placeholder API keys treated as configured Fixed (reject obvious placeholder secrets early) BUG-031 Answer extraction fails with multiple integers Fixed (select first in-range option index) BUG-029 Low TCGA benchmark accuracy (investigation) Fixed (AgentConfig T=20 default aligned to paper) BUG-028 Options not displayed in prompts (slidebench, expert_vqa) Fixed (options appended when <code>{options}</code> missing) BUG-027 CSV options parsed as single-element list Fixed (Python literal parsing + fail-loud validation) BUG-026 Model ID configuration scattered across codebase Fixed (defaults centralized in <code>src/giant/llm/model_registry.py</code>) BUG-025 OpenAI Responses API rejects multi-turn conversations Fixed (assistant messages use <code>output_text</code>) BUG-023 Axis guide labels use \u201cK\u201d abbreviation Fixed (K-notation removed, strict integers used) BUG-022 MultiPathQA acquisition UX gaps Fixed (<code>giant check-data</code> command added) BUG-019 Axis guide font fallback degrades legibility Fixed (Strict font check added to overlay config) BUG-021 Prompt template edge case (max_steps=1) Not reproducible (PromptBuilder uses final-step prompt) BUG-017 TCGA downloader path traversal Path validation added BUG-016 Agent crop on max_steps=1 Step guard added BUG-015 Visualizer missing images/overlays Trajectory metadata + HTML/CSS updates BUG-001 Boundary Crop Behavior Documented + tested BUG-002 Spec Contradiction on Upsampling Spec-05.5 updated BUG-003 Huge Region No Protection Memory guard added BUG-004 Missing Integration Tests Integration tests added BUG-005 Single-Level Slide Untested Unit tests added BUG-007 Test Suite Mocked (claim inaccurate) Documentation corrected BUG-008 API Keys Silent None ConfigError added BUG-009 Font Loading Silent Fallback Warning log added BUG-010 MPP Nullable No Guards Archived (future-proofing note, no active bug) BUG-011 Unused GeometryValidator Fixed in Spec-09 (now used in agent runner) BUG-012 HF Download Silent Auth Debug log added BUG-013 Silent Zero-Cost on Missing Usage Data Fail fast on missing usage + tests BUG-014 Environment Secrets Management Gap .env docs + test fixes + schema fixes"},{"location":"bugs/#severity-definitions","title":"Severity Definitions","text":"<ul> <li>P0 (Critical): Blocks progress. Must fix immediately.</li> <li>P1 (High): Will cause production bugs. Should fix before next spec.</li> <li>P2 (Medium): Edge cases. Can document and address later.</li> <li>P3 (Low): Nice to have. Future optimization.</li> <li>P4 (Future): Scaffolding for upcoming specs.</li> </ul>"},{"location":"bugs/#checkpoint-history","title":"Checkpoint History","text":""},{"location":"bugs/#benchmark-prep-audit-2025-12-26","title":"Benchmark Prep Audit (2025-12-26)","text":"<p>Goal: Identify blockers and high-impact bugs before running the full MultiPathQA suite with an OpenAI API key (paper reproduction workflow).</p> <p>Paper anchor: GIANT algorithm expects <code>T=20</code>, <code>S=1000</code>, oversampling bias <code>0.85</code>, and navigation via level-0 coordinate axis guides (see <code>_literature/markdown/giant/giant.md</code> Section 4.1).</p> <p>Quick validations run (safe / no live API calls):</p> <ul> <li><code>make lint</code> / <code>make typecheck</code></li> <li><code>uv run pytest -m \"not live and not cost\"</code></li> <li><code>uv run giant check-data &lt;dataset&gt;</code> to confirm local WSI availability</li> </ul> <p>Findings (bugs identified and fixed):</p> <ul> <li>BUG-033 (P0): \u2705 Fixed. Makefile now uses <code>$(or $(DATASET),tcga)</code> + per-dataset targets (<code>benchmark-tcga</code>, etc.). CLI runner validates dataset before creating LLM provider (fail-fast).</li> <li>BUG-034 (P0): \u2705 Fixed. <code>make test</code> excludes <code>live</code>/<code>cost</code> markers. Live tests require explicit <code>GIANT_RUN_LIVE_TESTS=1</code> environment variable.</li> <li>BUG-035 (P1): \u2705 Fixed. Typer app sets <code>pretty_exceptions_show_locals=False</code>. Regression tests verify secrets don't leak.</li> <li>BUG-036 (P1): \u2705 Fixed. <code>data/wsi/README.md</code> now recommends <code>giant check-data</code> which handles both flat and gdc-client layouts.</li> <li>BUG-037 (P2): \u2705 Fixed. <code>docs/data-acquisition.md</code> verification section uses <code>giant check-data</code> CLI (no pandas dependency).</li> </ul> <p>Blocker status (data):</p> <ul> <li>TCGA WSIs are partially present; <code>giant check-data</code> currently reports missing files for <code>tcga</code>, <code>tcga_expert_vqa</code>, and <code>tcga_slidebench</code>.</li> <li>GTEx and PANDA WSIs are not present locally (<code>gtex</code>: missing 191/191, <code>panda</code>: missing 197/197), so full \u201call tasks\u201d paper reproduction is blocked until those files are acquired.</li> </ul>"},{"location":"bugs/#benchmark-execution-bug-hunt-2025-12-21","title":"Benchmark Execution Bug Hunt (2025-12-21)","text":"<p>Audited: Full OpenAI benchmark run on 25 available TCGA questions.</p> <p>Findings:</p> <ul> <li>88% answer extraction failure rate (22/25 questions)</li> <li>BUG-027 (P1): CSV options are Python list literals, but loader assumed JSON and produced a 1-element list fallback.</li> <li>BUG-028 (P2): <code>tcga_slidebench</code> and <code>tcga_expert_vqa</code> have options but prompts omit <code>{options}</code>, so the model never sees choices.</li> <li>Cost: $0.99 wasted on unusable benchmark results</li> </ul> <p>Status: Fixed in code + unit tests; re-run benchmark once WSIs are available.</p>"},{"location":"bugs/#e2e-validation-bug-hunt-2025-12-20","title":"E2E Validation Bug Hunt (2025-12-20)","text":"<p>Audited: Full E2E inference with real WSI and OpenAI API.</p> <p>Findings:</p> <ul> <li>1 P0 bug fixed (BUG-025): OpenAI Responses API rejects multi-turn conversations</li> <li>Root cause: <code>message_content_to_openai()</code> used <code>input_text</code> for all content, but assistant messages require <code>output_text</code></li> <li>Test gap: Added regression test for assistant message content types</li> <li>Additional test gaps identified but not blocking (circuit breaker cooldown, provider validation)</li> </ul>"},{"location":"bugs/#audit-2025-12-19-paper-gap-analysis","title":"Audit (2025-12-19) - Paper Gap Analysis","text":"<p>Audited: Full codebase vs GIANT paper and Specs.</p> <p>Findings: - 6 new bug reports filed (BUG-018 to BUG-023). - 3 bugs fixed immediately (BUG-019, BUG-022, BUG-023). - 1 bug invalid (BUG-021). - 2 reproducibility gaps remain open (BUG-018, BUG-020):   - BUG-018: CONCH tool ablation requires gated CONCH access to reproduce Table 3   - BUG-020: Paper\u2019s OpenAI/Anthropic system prompts are in Supplementary Material (not available here)</p>"},{"location":"bugs/#spec-085-llm-integration-checkpoint-2025-12-18","title":"Spec-08.5 LLM Integration Checkpoint (2025-12-18)","text":"<p>Audited: Specs 06-08 (LLM Provider, Navigation Prompts, Context Manager)</p> <p>Findings:</p> <ul> <li>61 integration tests passing (+ 2 new missing-usage tests)</li> <li>P0-2 requirements fully covered</li> <li>2 bugs documented + fixed (BUG-013, BUG-014)</li> <li>11 fixed bugs archived</li> <li>Fixed: Anthropic JSON string parsing, OpenAI oneOf schema issue</li> <li>Fixed: Test skipif now detects keys from .env file</li> </ul>"},{"location":"bugs/#spec-12-cli-merge-bug-housekeeping-2025-12-19","title":"Spec-12 CLI Merge + Bug Housekeeping (2025-12-19)","text":"<p>Audited: BUG-010, BUG-011 from deferred list</p> <p>Findings:</p> <ul> <li>BUG-010 (MPP nullable): Not a bug, just future-proofing note. Archived.</li> <li>BUG-011 (GeometryValidator unused): Fixed in Spec-09. Archived.</li> <li>Deferred list cleared (BUG-010/BUG-011 archived).</li> </ul>"},{"location":"bugs/#audit-bug-hunt-p0-p4-2025-12-19","title":"Audit Bug Hunt (P0-P4) (2025-12-19)","text":"<p>Audited: Spec-09 to Spec-12 integration surfaces (agent loop, eval, CLI, visualizer, download helpers).</p> <p>Findings:</p> <ul> <li>3 new bugs documented (BUG-015, BUG-016, BUG-017)</li> <li>No new P0/P1 blockers found</li> </ul>"},{"location":"bugs/#spec-055-wsi-integration-checkpoint-2025-12-17","title":"Spec-05.5 WSI Integration Checkpoint (2025-12-17)","text":"<p>Audited: Specs 01-05 (WSI data layer, cropping, levels)</p> <p>Findings:</p> <ul> <li>17 WSI integration tests added</li> <li>12 bugs documented</li> <li>9 bugs fixed</li> <li>2 deferred to Spec-09 (BUG-010, BUG-011) \u2014 now resolved</li> </ul>"},{"location":"concepts/algorithm/","title":"Navigation Algorithm","text":"<p>This page explains GIANT's core navigation algorithm, based on Algorithm 1 from the paper.</p>"},{"location":"concepts/algorithm/#overview","title":"Overview","text":"<p>GIANT navigates gigapixel images through an iterative process:</p> <ol> <li>Show the LLM a low-resolution thumbnail with coordinate guides</li> <li>LLM reasons about what to examine and outputs a crop action</li> <li>Extract the requested region at high resolution</li> <li>Repeat until the LLM has enough information to answer</li> </ol>"},{"location":"concepts/algorithm/#algorithm-1-giant-navigation","title":"Algorithm 1: GIANT Navigation","text":"<pre><code>Input: WSI W, question q, step limit T\nOutput: answer \u0177\n\n1.  I\u2080 \u2190 Thumbnail(W)                    # Generate thumbnail\n2.  I\u2080 \u2190 AddAxisGuides(I\u2080)               # Add coordinate markers\n3.  C \u2190 [(system_prompt, q, I\u2080)]         # Initialize context\n4.\n5.  for t = 1 to T-1 do                  # At most T-1 crops (paper)\n6.      (r\u209c, a\u209c) \u2190 LLM(C)                 # Get reasoning + action\n7.\n8.      if a\u209c.type == \"answer\" then\n9.          return a\u209c.text               # Early termination\n10.\n11.     if a\u209c.type == \"crop\" then\n12.         I\u209c \u2190 CropRegion(W, a\u209c, S)    # Extract region\n13.         C \u2190 C \u222a [(r\u209c, a\u209c, I\u209c)]        # Add to context\n14.\n15. end for\n16.\n17. \u0177 \u2190 ForceAnswer(C)                   # Final step: must answer (with retries)\n18. return \u0177\n</code></pre>"},{"location":"concepts/algorithm/#step-by-step-breakdown","title":"Step-by-Step Breakdown","text":""},{"location":"concepts/algorithm/#step-1-thumbnail-generation","title":"Step 1: Thumbnail Generation","text":"<pre><code># Generate thumbnail fitting within max_size\nthumbnail = reader.get_thumbnail((1024, 1024))\n</code></pre> <p>The thumbnail is a low-resolution overview of the entire slide. A 100,000 x 80,000 pixel slide becomes roughly 1024 x 820 pixels.</p>"},{"location":"concepts/algorithm/#step-2-axis-guides","title":"Step 2: Axis Guides","text":"<pre><code># Add Level-0 coordinate markers\nnavigable = overlay_service.create_navigable_thumbnail(thumbnail, metadata)\n</code></pre> <p>Red lines with pixel coordinate labels are overlaid:</p> <pre><code>     0        25000      50000      75000     100000\n     \u2502          \u2502          \u2502          \u2502          \u2502\n  \u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\n     \u2502          \u2502          \u2502          \u2502          \u2502\n     \u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502          \u2502\n     \u2502    \u2502  Tissue visible     \u2502     \u2502          \u2502\n25K \u2500\u253c\u2500\u2500\u2500\u2500\u2502  in this region     \u2502\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\n     \u2502    \u2502                     \u2502     \u2502          \u2502\n     \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502          \u2502\n     \u2502          \u2502          \u2502          \u2502          \u2502\n50K \u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\n</code></pre> <p>The LLM uses these markers to specify exact coordinates.</p>"},{"location":"concepts/algorithm/#step-3-context-initialization","title":"Step 3: Context Initialization","text":"<pre><code>context = ContextManager(\n    wsi_path=wsi_path,\n    question=question,\n    max_steps=20,\n)\n</code></pre> <p>The initial context includes: - System prompt (navigation instructions) - User question - Thumbnail image with axis guides</p>"},{"location":"concepts/algorithm/#steps-4-15-navigation-loop","title":"Steps 4-15: Navigation Loop","text":"<p>Each iteration:</p> <ol> <li>Build messages from context (system, user turns, assistant turns)</li> <li>Call LLM with multimodal input (text + images)</li> <li>Parse response into reasoning + action</li> <li>Execute action:</li> <li>If <code>crop</code>: Extract region, add to context</li> <li>If <code>answer</code>: Return immediately</li> </ol>"},{"location":"concepts/algorithm/#step-17-force-answer","title":"Step 17: Force Answer","text":"<p>If the LLM reaches the step limit without answering:</p> <pre><code>force_prompt = \"\"\"\nYou have reached the maximum number of navigation steps ({max_steps}).\nBased on all the regions you have examined, you MUST now provide your final answer.\n\"\"\"\n</code></pre> <p>The agent retries up to 3 times to get an answer action.</p>"},{"location":"concepts/algorithm/#key-parameters","title":"Key Parameters","text":"Parameter Default Description <code>T</code> (max_steps) 20 Maximum navigation steps <code>S</code> (target_size) 1000 (OpenAI) / 500 (Anthropic) Output crop long-side (provider-specific) Thumbnail size 1024 Maximum thumbnail dimension Max retries 3 Retries for invalid coordinates Oversampling bias 0.85 Bias toward finer pyramid levels"},{"location":"concepts/algorithm/#coordinate-system","title":"Coordinate System","text":"<p>All coordinates use Level-0 (full resolution) pixel space:</p> <ul> <li><code>x</code>: Horizontal position from left edge</li> <li><code>y</code>: Vertical position from top edge</li> <li><code>width</code>, <code>height</code>: Size of region to extract</li> </ul> <p>Example for a 100,000 x 80,000 slide: <pre><code>{\n  \"x\": 45000,      // 45% from left\n  \"y\": 20000,      // 25% from top\n  \"width\": 10000,  // 10% of slide width\n  \"height\": 10000  // 12.5% of slide height\n}\n</code></pre></p>"},{"location":"concepts/algorithm/#level-selection","title":"Level Selection","text":"<p>WSIs are stored as image pyramids with multiple resolution levels:</p> <pre><code>Level 0: 100,000 x 80,000 (full resolution)\nLevel 1:  50,000 x 40,000 (2x downsampled)\nLevel 2:  25,000 x 20,000 (4x downsampled)\nLevel 3:  12,500 x 10,000 (8x downsampled)\n</code></pre> <p>GIANT automatically selects the optimal level to: 1. Avoid upsampling (blurry results) 2. Minimize downsampling (preserve detail) 3. Output at target size (1000px)</p> <pre><code>from giant.core.level_selector import PyramidLevelSelector\nfrom giant.geometry import Region\nfrom giant.wsi import WSIReader\n\nwith WSIReader(\"slide.svs\") as reader:\n    metadata = reader.get_metadata()\n\nselector = PyramidLevelSelector()\nselected = selector.select_level(\n    region=Region(x=45000, y=20000, width=10000, height=10000),\n    metadata=metadata,\n    target_size=1000,\n    bias=0.85,\n)\n</code></pre>"},{"location":"concepts/algorithm/#error-handling","title":"Error Handling","text":""},{"location":"concepts/algorithm/#invalid-coordinates","title":"Invalid Coordinates","text":"<p>If the LLM provides out-of-bounds coordinates:</p> <ol> <li>Validate against slide dimensions</li> <li>Send error feedback to LLM</li> <li>Request corrected coordinates</li> <li>Retry up to <code>max_retries</code> times</li> </ol>"},{"location":"concepts/algorithm/#parse-errors","title":"Parse Errors","text":"<p>If the LLM output can't be parsed:</p> <ol> <li>Log the raw output</li> <li>Increment error counter</li> <li>Retry with same context</li> <li>Fail after <code>max_retries</code></li> </ol>"},{"location":"concepts/algorithm/#cost-optimization","title":"Cost Optimization","text":"<p>Each LLM call has a cost. GIANT optimizes by:</p> <ol> <li>Early termination: Answer as soon as evidence is sufficient</li> <li>Efficient context: Don't repeat full images in every turn</li> <li>Budget limits: Optional <code>--budget-usd</code> flag stops early</li> </ol>"},{"location":"concepts/algorithm/#visualization","title":"Visualization","text":"<p>After a run, visualize the trajectory:</p> <pre><code>giant visualize trajectory.json\n</code></pre> <p>Shows: - Thumbnail with all crop regions overlaid - Step-by-step reasoning - Final answer</p>"},{"location":"concepts/algorithm/#implementation-reference","title":"Implementation Reference","text":"Concept File Function/Class Agent loop <code>agent/runner.py</code> <code>GIANTAgent._navigation_loop</code> Context <code>agent/context.py</code> <code>ContextManager</code> Thumbnail <code>wsi/reader.py</code> <code>WSIReader.get_thumbnail</code> Axis guides <code>geometry/overlay.py</code> <code>AxisGuideGenerator</code> Cropping <code>core/crop_engine.py</code> <code>CropEngine.crop</code> Level selection <code>core/level_selector.py</code> <code>PyramidLevelSelector</code>"},{"location":"concepts/algorithm/#next-steps","title":"Next Steps","text":"<ul> <li>LLM Integration - How providers work</li> <li>Prompt Design - Navigation prompts</li> <li>Architecture - System design</li> </ul>"},{"location":"concepts/architecture/","title":"Architecture","text":"<p>GIANT follows a modular architecture with clear separation of concerns. This page describes the major components and how they interact.</p>"},{"location":"concepts/architecture/#high-level-design","title":"High-Level Design","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                           CLI Layer                             \u2502\n\u2502                    giant run / giant benchmark                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         Agent Layer                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                \u2502\n\u2502  \u2502GIANTAgent  \u2502 \u2502ContextManager \u2502 \u2502 Trajectory \u2502                \u2502\n\u2502  \u2502(runner.py) \u2502 \u2502  (context.py) \u2502 \u2502(trajectory)\u2502                \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n           \u25bc                  \u25bc                  \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502    WSI Layer      \u2502 \u2502   LLM Layer      \u2502 \u2502  Prompt Layer    \u2502\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502 \u2502 WSIReader     \u2502 \u2502 \u2502 \u2502 LLMProvider  \u2502 \u2502 \u2502 \u2502PromptBuilder \u2502 \u2502\n\u2502 \u2502 CropEngine    \u2502 \u2502 \u2502 \u2502 OpenAI       \u2502 \u2502 \u2502 \u2502 Templates    \u2502 \u2502\n\u2502 \u2502 Level Selector\u2502 \u2502 \u2502 \u2502 Anthropic    \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502 \u2502 OverlayGen    \u2502 \u2502 \u2502 \u2502ModelRegistry \u2502 \u2502\n\u2502 \u2502 AxisGuides    \u2502 \u2502 \u2502 \u2502 Pricing      \u2502 \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"concepts/architecture/#module-overview","title":"Module Overview","text":""},{"location":"concepts/architecture/#agent-layer-srcgiantagent","title":"Agent Layer (<code>src/giant/agent/</code>)","text":"<p>The orchestration layer that implements the navigation algorithm.</p> File Class Responsibility <code>runner.py</code> <code>GIANTAgent</code> Main agent loop, error handling, retry logic <code>context.py</code> <code>ContextManager</code> Multi-turn conversation state <code>trajectory.py</code> <code>Trajectory</code> Step recording for evaluation"},{"location":"concepts/architecture/#wsi-layer-srcgiantwsi-srcgiantcore","title":"WSI Layer (<code>src/giant/wsi/</code>, <code>src/giant/core/</code>)","text":"<p>Handles whole-slide image I/O and processing.</p> File Class Responsibility <code>reader.py</code> <code>WSIReader</code> OpenSlide wrapper, thumbnail generation <code>crop_engine.py</code> <code>CropEngine</code> Region extraction, level selection <code>level_selector.py</code> <code>PyramidLevelSelector</code> Optimal pyramid level algorithm"},{"location":"concepts/architecture/#geometry-layer-srcgiantgeometry","title":"Geometry Layer (<code>src/giant/geometry/</code>)","text":"<p>Coordinate systems and visual overlays.</p> File Class Responsibility <code>primitives.py</code> <code>Region</code>, <code>Point</code>, <code>Size</code> Value objects <code>overlay.py</code> <code>AxisGuideGenerator</code> Axis labels on thumbnails <code>validators.py</code> <code>GeometryValidator</code> Bounds checking"},{"location":"concepts/architecture/#llm-layer-srcgiantllm","title":"LLM Layer (<code>src/giant/llm/</code>)","text":"<p>Abstracts LLM API interactions.</p> File Class Responsibility <code>protocol.py</code> <code>LLMProvider</code> Protocol interface <code>openai_client.py</code> <code>OpenAIProvider</code> OpenAI Responses API <code>anthropic_client.py</code> <code>AnthropicProvider</code> Anthropic Messages API <code>model_registry.py</code> - Approved models, validation <code>pricing.py</code> - Cost calculation"},{"location":"concepts/architecture/#prompt-layer-srcgiantprompts","title":"Prompt Layer (<code>src/giant/prompts/</code>)","text":"<p>Prompt engineering and templates.</p> File Class Responsibility <code>builder.py</code> <code>PromptBuilder</code> Assembles prompts per step <code>templates.py</code> - System/user prompt templates"},{"location":"concepts/architecture/#evaluation-layer-srcgianteval","title":"Evaluation Layer (<code>src/giant/eval/</code>)","text":"<p>Benchmark running and metrics.</p> File Class Responsibility <code>runner.py</code> <code>BenchmarkRunner</code> Orchestrates benchmark runs <code>metrics.py</code> - Accuracy, balanced accuracy <code>answer_extraction.py</code> - Parse model answers <code>resumable.py</code> - Checkpoint/resume logic"},{"location":"concepts/architecture/#cli-layer-srcgiantcli","title":"CLI Layer (<code>src/giant/cli/</code>)","text":"<p>Command-line interface.</p> File Responsibility <code>main.py</code> Typer app, command definitions <code>runners.py</code> CLI command implementations <code>visualizer.py</code> HTML trajectory viewer"},{"location":"concepts/architecture/#data-flow","title":"Data Flow","text":""},{"location":"concepts/architecture/#single-inference","title":"Single Inference","text":"<pre><code>1. CLI receives: wsi_path, question, provider, model\n                    \u2502\n2. Create LLMProvider(model)\n                    \u2502\n3. Create GIANTAgent(wsi_path, question, provider)\n                    \u2502\n4. agent.run():\n   \u2502\n   \u251c\u2500\u25ba WSIReader opens slide\n   \u2502\n   \u251c\u2500\u25ba Generate thumbnail + axis guides\n   \u2502\n   \u2514\u2500\u25ba Navigation loop:\n       \u2502\n       \u251c\u2500\u25ba ContextManager builds messages\n       \u2502\n       \u251c\u2500\u25ba LLMProvider.generate_response()\n       \u2502\n       \u251c\u2500\u25ba Parse action (crop or answer)\n       \u2502\n       \u251c\u2500\u25ba If crop: CropEngine extracts region\n       \u2502             ContextManager.add_turn()\n       \u2502             Continue loop\n       \u2502\n       \u2514\u2500\u25ba If answer: Return RunResult\n                    \u2502\n5. Return answer, cost, trajectory\n</code></pre>"},{"location":"concepts/architecture/#benchmark-run","title":"Benchmark Run","text":"<pre><code>1. BenchmarkRunner loads MultiPathQA.csv\n                    \u2502\n2. Filter by benchmark_name\n                    \u2502\n3. For each item (with concurrency):\n   \u2502\n   \u251c\u2500\u25ba Resolve WSI path\n   \u2502\n   \u251c\u2500\u25ba Create GIANTAgent\n   \u2502\n   \u251c\u2500\u25ba Run agent, record result\n   \u2502\n   \u2514\u2500\u25ba Checkpoint progress\n                    \u2502\n4. Calculate metrics (balanced accuracy, etc.)\n                    \u2502\n5. Save results JSON\n</code></pre>"},{"location":"concepts/architecture/#design-patterns","title":"Design Patterns","text":""},{"location":"concepts/architecture/#protocol-pattern-llmprovider","title":"Protocol Pattern (LLMProvider)","text":"<p>All LLM providers implement the <code>LLMProvider</code> protocol:</p> <pre><code>class LLMProvider(Protocol):\n    async def generate_response(self, messages: list[Message]) -&gt; LLMResponse: ...\n    def get_model_name(self) -&gt; str: ...\n    def get_target_size(self) -&gt; int: ...\n</code></pre> <p>This allows swapping providers without changing agent code.</p>"},{"location":"concepts/architecture/#factory-pattern","title":"Factory Pattern","text":"<p>Providers are created via factory function:</p> <pre><code>from giant.llm import create_provider\nprovider = create_provider(\"openai\", model=\"gpt-5.2\")\n</code></pre>"},{"location":"concepts/architecture/#value-objects","title":"Value Objects","text":"<p>Geometry uses immutable value objects:</p> <pre><code>from giant.geometry import Region, Point, Size\n\nregion = Region(x=1000, y=2000, width=500, height=500)\n</code></pre>"},{"location":"concepts/architecture/#configuration","title":"Configuration","text":"<p>Configuration flows from:</p> <ol> <li>CLI arguments - User-provided options</li> <li>AgentConfig - Agent behavior settings</li> <li>Environment - API keys from <code>.env</code></li> <li>Model Registry - Approved models</li> </ol> <p>See Configuration Reference for details.</p>"},{"location":"concepts/architecture/#next-steps","title":"Next Steps","text":"<ul> <li>Algorithm - Navigation algorithm details</li> <li>LLM Integration - Provider implementations</li> <li>Project Structure - File organization</li> </ul>"},{"location":"concepts/llm-integration/","title":"LLM Integration","text":"<p>GIANT uses large multimodal models (LMMs) to analyze images and decide navigation actions. This page explains how LLM providers are integrated.</p>"},{"location":"concepts/llm-integration/#provider-architecture","title":"Provider Architecture","text":"<p>GIANT abstracts LLM interactions behind a protocol interface:</p> <pre><code>class LLMProvider(Protocol):\n    async def generate_response(self, messages: list[Message]) -&gt; LLMResponse:\n        \"\"\"Generate a response from the LLM.\"\"\"\n        ...\n\n    def get_model_name(self) -&gt; str:\n        \"\"\"Get the model identifier.\"\"\"\n        ...\n\n    def get_target_size(self) -&gt; int:\n        \"\"\"Get optimal image size for this provider.\"\"\"\n        ...\n</code></pre> <p>This allows swapping providers without changing agent code.</p>"},{"location":"concepts/llm-integration/#supported-providers","title":"Supported Providers","text":""},{"location":"concepts/llm-integration/#openai","title":"OpenAI","text":"<p>Uses the Responses API with structured outputs:</p> <pre><code>from giant.llm import create_provider\n\nprovider = create_provider(\"openai\", model=\"gpt-5.2\")\n</code></pre> <p>Features: - Native JSON schema enforcement via <code>response_format</code> - Image handling via base64 data URLs - Token and cost tracking from response metadata</p> <p>Target Size: 1000px (higher resolution for detail)</p>"},{"location":"concepts/llm-integration/#anthropic","title":"Anthropic","text":"<p>Uses the Messages API with tool use:</p> <pre><code>provider = create_provider(\"anthropic\", model=\"claude-sonnet-4-5-20250929\")\n</code></pre> <p>Features: - Tool use for structured output (<code>submit_step</code> tool) - Image handling via base64 content blocks - Token and cost tracking from response metadata</p> <p>Target Size: 500px (cost-optimized)</p>"},{"location":"concepts/llm-integration/#message-format","title":"Message Format","text":"<p>Internally, GIANT uses a unified message format:</p> <pre><code>from pydantic import BaseModel\nfrom typing import Literal\n\nclass MessageContent(BaseModel):\n    type: Literal[\"text\", \"image\"]\n    text: str | None = None\n    image_base64: str | None = None\n    media_type: str = \"image/jpeg\"\n\nclass Message(BaseModel):\n    role: Literal[\"system\", \"user\", \"assistant\"]\n    content: list[MessageContent]\n</code></pre> <p>Converters translate to provider-specific formats:</p> <pre><code># OpenAI format\n{\n    \"role\": \"user\",\n    \"content\": [\n        {\"type\": \"input_text\", \"text\": \"Analyze this image...\"},\n        {\"type\": \"input_image\", \"image_url\": \"data:image/jpeg;base64,...\"}\n    ]\n}\n\n# Anthropic format\n{\n    \"role\": \"user\",\n    \"content\": [\n        {\"type\": \"text\", \"text\": \"Analyze this image...\"},\n        {\"type\": \"image\", \"source\": {\"type\": \"base64\", \"data\": \"...\", \"media_type\": \"image/jpeg\"}}\n    ]\n}\n</code></pre>"},{"location":"concepts/llm-integration/#structured-output","title":"Structured Output","text":"<p>GIANT requires structured JSON responses:</p> <pre><code>class StepResponse(BaseModel):\n    reasoning: str\n    action: BoundingBoxAction | FinalAnswerAction\n\nclass BoundingBoxAction(BaseModel):\n    action_type: Literal[\"crop\"] = \"crop\"\n    x: int\n    y: int\n    width: int\n    height: int\n\nclass FinalAnswerAction(BaseModel):\n    action_type: Literal[\"answer\"] = \"answer\"\n    answer_text: str\n</code></pre>"},{"location":"concepts/llm-integration/#openai-json-schema","title":"OpenAI: JSON Schema","text":"<pre><code>response = client.responses.create(\n    model=\"gpt-5.2\",\n    input=messages,\n    text={\n        \"format\": {\n            \"type\": \"json_schema\",\n            \"name\": \"step_response\",\n            \"schema\": StepResponse.model_json_schema(),\n        }\n    }\n)\n</code></pre>"},{"location":"concepts/llm-integration/#anthropic-tool-use","title":"Anthropic: Tool Use","text":"<pre><code>response = client.messages.create(\n    model=\"claude-sonnet-4-5-20250929\",\n    messages=messages,\n    tools=[{\n        \"name\": \"submit_step\",\n        \"description\": \"Provide your response\",\n        \"input_schema\": StepResponse.model_json_schema(),\n    }],\n    tool_choice={\"type\": \"tool\", \"name\": \"submit_step\"},\n)\n</code></pre>"},{"location":"concepts/llm-integration/#model-registry","title":"Model Registry","text":"<p>Only approved models are allowed:</p> Provider Model ID Status OpenAI <code>gpt-5.2</code> Default Anthropic <code>claude-sonnet-4-5-20250929</code> Supported Google <code>gemini-3-pro-preview</code> Reserved (provider not yet implemented) <p>Models are validated at runtime:</p> <pre><code>from giant.llm.model_registry import validate_model_id\n\nvalidate_model_id(\"gpt-5.2\")  # OK\nvalidate_model_id(\"gpt-4o\")   # Raises ValueError\n</code></pre> <p>See Model Registry for details.</p>"},{"location":"concepts/llm-integration/#cost-tracking","title":"Cost Tracking","text":"<p>Each response includes usage and cost:</p> <pre><code>@dataclass\nclass TokenUsage:\n    prompt_tokens: int\n    completion_tokens: int\n    total_tokens: int\n    cost_usd: float\n</code></pre> <p>Costs are calculated using pricing tables:</p> <pre><code># Example pricing (per 1M tokens)\nPRICING = {\n    \"gpt-5.2\": {\"input\": 1.75, \"output\": 14.00},\n    \"claude-sonnet-4-5-20250929\": {\"input\": 3.00, \"output\": 15.00},\n    \"gemini-3-pro-preview\": {\"input\": 2.00, \"output\": 12.00},\n}\n</code></pre>"},{"location":"concepts/llm-integration/#error-handling","title":"Error Handling","text":""},{"location":"concepts/llm-integration/#llmerror","title":"LLMError","text":"<p>Base exception for API failures:</p> <pre><code>class LLMError(Exception):\n    \"\"\"Raised when API calls fail after retries.\"\"\"\n    provider: str | None\n    model: str | None\n    cause: Exception | None\n</code></pre>"},{"location":"concepts/llm-integration/#llmparseerror","title":"LLMParseError","text":"<p>When response can't be parsed:</p> <pre><code>class LLMParseError(LLMError):\n    \"\"\"Raised when output doesn't match expected schema.\"\"\"\n    raw_output: str | None\n</code></pre>"},{"location":"concepts/llm-integration/#circuit-breaker","title":"Circuit Breaker","text":"<p>Protects against cascading failures:</p> <pre><code>class CircuitBreakerOpenError(LLMError):\n    \"\"\"Raised when too many consecutive failures occur.\"\"\"\n    cooldown_remaining_seconds: float\n</code></pre>"},{"location":"concepts/llm-integration/#configuration","title":"Configuration","text":""},{"location":"concepts/llm-integration/#environment-variables","title":"Environment Variables","text":"<pre><code># Required for respective providers\nOPENAI_API_KEY=sk-...\nANTHROPIC_API_KEY=sk-ant-...\n</code></pre>"},{"location":"concepts/llm-integration/#provider-specific-settings","title":"Provider-Specific Settings","text":"Provider Target Size Notes OpenAI 1000px Higher resolution, higher cost Anthropic 500px Cost-optimized, still effective"},{"location":"concepts/llm-integration/#adding-new-providers","title":"Adding New Providers","text":"<p>To add a new LLM provider:</p> <ol> <li>Create client class implementing <code>LLMProvider</code> protocol</li> <li>Add converter functions for message format</li> <li>Add to <code>create_provider()</code> factory</li> <li>Add model to registry with pricing</li> <li>Add tests</li> </ol> <p>Example skeleton:</p> <pre><code>class NewProvider:\n    def __init__(self, model: str):\n        validate_model_id(model, provider=\"newprovider\")\n        self.model = model\n        self.client = NewProviderClient()\n\n    async def generate_response(self, messages: list[Message]) -&gt; LLMResponse:\n        # Convert messages\n        # Call API\n        # Parse response\n        # Return LLMResponse\n        ...\n\n    def get_model_name(self) -&gt; str:\n        return self.model\n\n    def get_target_size(self) -&gt; int:\n        return 1000\n</code></pre>"},{"location":"concepts/llm-integration/#next-steps","title":"Next Steps","text":"<ul> <li>Prompt Design - Navigation prompts</li> <li>Model Registry - Approved models</li> <li>Configuring Providers - Setup guide</li> </ul>"},{"location":"concepts/overview/","title":"What is GIANT?","text":"<p>GIANT (Gigapixel Image Agent for Navigating Tissue) is an agentic system that uses large language models (LLMs) to autonomously navigate whole-slide images (WSIs) for pathology analysis.</p>"},{"location":"concepts/overview/#the-problem","title":"The Problem","text":"<p>Whole-slide images are massive - often 100,000+ pixels on each side, resulting in gigapixel-scale images. A typical WSI can be 50,000 x 80,000 pixels or larger.</p> <p>This creates fundamental challenges:</p> <ol> <li>Too large for direct analysis: LLMs have input size limits (~1-2K pixels typically)</li> <li>Information overload: Most of the slide is background or irrelevant tissue</li> <li>Multi-scale features: Diagnosis requires both architectural patterns (low magnification) and cellular details (high magnification)</li> </ol>"},{"location":"concepts/overview/#the-solution","title":"The Solution","text":"<p>GIANT treats WSI analysis as a navigation problem. Instead of trying to analyze the entire slide at once, an LLM-powered agent:</p> <ol> <li>Starts with a thumbnail - A low-resolution overview with coordinate axis guides</li> <li>Iteratively zooms in - Selects regions of interest based on the question</li> <li>Accumulates evidence - Remembers what it has seen across navigation steps</li> <li>Provides an answer - When sufficient evidence is gathered</li> </ol> <p>This mimics how pathologists work: scan at low power, identify regions of interest, zoom in for cellular detail.</p>"},{"location":"concepts/overview/#key-innovations","title":"Key Innovations","text":""},{"location":"concepts/overview/#axis-guides","title":"Axis Guides","text":"<p>The thumbnail is overlaid with coordinate markers showing Level-0 pixel positions. This allows the LLM to specify exact crop coordinates using natural language reasoning:</p> <p>\"I can see a suspicious region around coordinates (45000, 32000). Let me zoom in there...\"</p>"},{"location":"concepts/overview/#multi-turn-context","title":"Multi-turn Context","text":"<p>The agent maintains conversation history, remembering: - Previously examined regions - Observations and reasoning at each step - The original question being answered</p>"},{"location":"concepts/overview/#structured-actions","title":"Structured Actions","text":"<p>The LLM outputs structured JSON actions:</p> <pre><code>{\n  \"reasoning\": \"The thumbnail shows a dark region that may be tumor...\",\n  \"action\": {\n    \"action_type\": \"crop\",\n    \"x\": 45000,\n    \"y\": 32000,\n    \"width\": 10000,\n    \"height\": 10000\n  }\n}\n</code></pre> <p>Or when ready to answer:</p> <pre><code>{\n  \"reasoning\": \"Based on the cellular morphology observed...\",\n  \"action\": {\n    \"action_type\": \"answer\",\n    \"answer_text\": \"This is adenocarcinoma, moderately differentiated.\"\n  }\n}\n</code></pre>"},{"location":"concepts/overview/#supported-tasks","title":"Supported Tasks","text":"<p>GIANT can answer various pathology questions:</p> Task Type Example Question Classification \"What organ is this tissue from?\" Diagnosis \"What type of cancer is present?\" Grading \"What is the Gleason grade?\" VQA \"Are there mitotic figures visible?\""},{"location":"concepts/overview/#architecture-overview","title":"Architecture Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    GIANTAgent                       \u2502\n\u2502                                                     \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u2502\n\u2502  \u2502WSIReader \u2502\u2500\u2500\u2500\u25b6\u2502CropEngine\u2502\u2500\u2500\u2500\u25b6\u2502OverlayGen\u2502       \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502\n\u2502        \u2502              \u2502               \u2502             \u2502\n\u2502        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2502\n\u2502                       \u2502                             \u2502\n\u2502                       \u25bc                             \u2502\n\u2502              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                       \u2502\n\u2502              \u2502ContextManager\u2502                       \u2502\n\u2502              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                       \u2502\n\u2502                       \u2502                             \u2502\n\u2502                       \u25bc                             \u2502\n\u2502              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                       \u2502\n\u2502              \u2502 LLMProvider  \u2502\u25c0\u2500\u2500\u25b6 OpenAI/Anthropic  \u2502\n\u2502              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                       \u2502\n\u2502                       \u2502                             \u2502\n\u2502                       \u25bc                             \u2502\n\u2502              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                       \u2502\n\u2502              \u2502  Trajectory  \u2502\u2500\u2500\u2500\u25b6 Evaluation        \u2502\n\u2502              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"concepts/overview/#research-origin","title":"Research Origin","text":"<p>GIANT is based on the paper:</p> <p>GIANT: Gigapixel Image Agent for Navigating Tissue arXiv:2511.19652</p> <p>This implementation reproduces and extends the paper's methodology, achieving competitive results on the MultiPathQA benchmark.</p>"},{"location":"concepts/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Architecture - Detailed system design</li> <li>Algorithm - The core navigation algorithm</li> <li>Quickstart - Try it yourself</li> </ul>"},{"location":"development/contributing/","title":"Contributing","text":"<p>Thank you for your interest in contributing to GIANT! This guide covers how to set up your development environment and submit contributions.</p>"},{"location":"development/contributing/#development-setup","title":"Development Setup","text":""},{"location":"development/contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11+</li> <li>OpenSlide library</li> <li>uv package manager (recommended)</li> <li>Git</li> </ul>"},{"location":"development/contributing/#clone-and-install","title":"Clone and Install","text":"<pre><code># Clone the repository\ngit clone https://github.com/The-Obstacle-Is-The-Way/gigapixel-goblin.git\ncd gigapixel-goblin\n\n# Install with dev dependencies\nuv sync --dev\n\n# Activate environment\nsource .venv/bin/activate\n\n# Verify installation\ngiant version\npytest tests/unit -x\n</code></pre>"},{"location":"development/contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"development/contributing/#1-create-a-branch","title":"1. Create a Branch","text":"<pre><code>git checkout -b feature/your-feature-name\n# or\ngit checkout -b fix/your-bug-fix\n</code></pre>"},{"location":"development/contributing/#2-make-changes","title":"2. Make Changes","text":"<p>Follow the coding standards below.</p>"},{"location":"development/contributing/#3-run-tests","title":"3. Run Tests","text":"<pre><code># Unit tests (fast)\npytest tests/unit\n\n# With coverage\npytest tests/unit --cov=giant --cov-report=term-missing\n\n# Specific module\npytest tests/unit/llm/\n</code></pre>"},{"location":"development/contributing/#4-run-linting","title":"4. Run Linting","text":"<pre><code># Check code style\nruff check .\n\n# Auto-fix issues\nruff check . --fix\n\n# Format code\nruff format .\n\n# Type checking\nmypy src/giant\n</code></pre>"},{"location":"development/contributing/#5-commit-changes","title":"5. Commit Changes","text":"<pre><code>git add .\ngit commit -m \"feat: add new feature description\"\n</code></pre> <p>Follow Conventional Commits:</p> Prefix Use For <code>feat:</code> New features <code>fix:</code> Bug fixes <code>docs:</code> Documentation <code>refactor:</code> Code refactoring <code>test:</code> Test changes <code>chore:</code> Maintenance"},{"location":"development/contributing/#6-push-and-create-pr","title":"6. Push and Create PR","text":"<pre><code>git push -u origin feature/your-feature-name\n</code></pre> <p>Then create a Pull Request on GitHub.</p>"},{"location":"development/contributing/#coding-standards","title":"Coding Standards","text":""},{"location":"development/contributing/#python-style","title":"Python Style","text":"<ul> <li>Follow PEP 8 with Ruff enforcement</li> <li>Maximum line length: 88 characters</li> <li>Use type hints for all public functions</li> <li>Docstrings for all public classes and functions</li> </ul>"},{"location":"development/contributing/#example","title":"Example","text":"<pre><code>\"\"\"Module docstring explaining purpose.\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from pathlib import Path\n\n\n@dataclass\nclass ExampleClass:\n    \"\"\"Class docstring.\n\n    Attributes:\n        name: Description of attribute.\n        value: Description of attribute.\n    \"\"\"\n\n    name: str\n    value: int\n\n    def process(self, input_path: Path) -&gt; str:\n        \"\"\"Process the input.\n\n        Args:\n            input_path: Path to input file.\n\n        Returns:\n            Processed result string.\n\n        Raises:\n            ValueError: If input is invalid.\n        \"\"\"\n        if not input_path.exists():\n            raise ValueError(f\"Path does not exist: {input_path}\")\n        return f\"Processed: {self.name}\"\n</code></pre>"},{"location":"development/contributing/#imports","title":"Imports","text":"<pre><code># Standard library\nimport os\nfrom pathlib import Path\n\n# Third-party\nimport numpy as np\nfrom pydantic import BaseModel\n\n# Local\nfrom giant.geometry import Region\nfrom giant.llm import LLMProvider\n</code></pre>"},{"location":"development/contributing/#error-handling","title":"Error Handling","text":"<ul> <li>Use specific exception types</li> <li>Never catch bare <code>Exception</code> unless re-raising</li> <li>Include context in error messages</li> </ul> <pre><code># Good\ntry:\n    result = process(data)\nexcept ValueError as e:\n    logger.error(\"Processing failed: %s\", e)\n    raise ProcessingError(f\"Failed to process {data.id}\") from e\n\n# Bad\ntry:\n    result = process(data)\nexcept:\n    pass\n</code></pre>"},{"location":"development/contributing/#test-guidelines","title":"Test Guidelines","text":""},{"location":"development/contributing/#test-driven-development","title":"Test-Driven Development","text":"<p>This project follows TDD:</p> <ol> <li>Write a failing test</li> <li>Implement the feature</li> <li>Refactor</li> </ol>"},{"location":"development/contributing/#test-organization","title":"Test Organization","text":"<pre><code>tests/\n\u251c\u2500\u2500 unit/           # Fast, isolated tests\n\u2502   \u2514\u2500\u2500 module/\n\u2502       \u2514\u2500\u2500 test_feature.py\n\u2514\u2500\u2500 integration/    # Tests requiring external resources\n    \u2514\u2500\u2500 module/\n        \u2514\u2500\u2500 test_integration.py\n</code></pre>"},{"location":"development/contributing/#test-naming","title":"Test Naming","text":"<pre><code>def test_feature_does_expected_thing() -&gt; None:\n    \"\"\"Test that feature does X when Y.\"\"\"\n    ...\n\ndef test_feature_raises_on_invalid_input() -&gt; None:\n    \"\"\"Test that feature raises ValueError for invalid input.\"\"\"\n    ...\n</code></pre>"},{"location":"development/contributing/#fixtures","title":"Fixtures","text":"<p>Use pytest fixtures for common setup:</p> <pre><code># conftest.py\n@pytest.fixture\ndef sample_region() -&gt; Region:\n    return Region(x=100, y=200, width=50, height=50)\n\n# test_feature.py\ndef test_crop_uses_region(sample_region: Region) -&gt; None:\n    result = crop(sample_region)\n    assert result is not None\n</code></pre>"},{"location":"development/contributing/#mocking","title":"Mocking","text":"<p>Use mocks for external dependencies:</p> <pre><code>from unittest.mock import AsyncMock, MagicMock\n\ndef test_agent_calls_provider(mocker: MockerFixture) -&gt; None:\n    mock_provider = MagicMock(spec=LLMProvider)\n    mock_provider.generate_response = AsyncMock(return_value=mock_response)\n\n    agent = GIANTAgent(provider=mock_provider, ...)\n    result = asyncio.run(agent.run())\n\n    mock_provider.generate_response.assert_called_once()\n</code></pre>"},{"location":"development/contributing/#documentation","title":"Documentation","text":""},{"location":"development/contributing/#docstrings","title":"Docstrings","text":"<p>Use Google-style docstrings:</p> <pre><code>def function(arg1: str, arg2: int = 10) -&gt; dict[str, int]:\n    \"\"\"Short description.\n\n    Longer description if needed. Can span multiple lines\n    and include examples.\n\n    Args:\n        arg1: Description of arg1.\n        arg2: Description of arg2. Defaults to 10.\n\n    Returns:\n        Dictionary mapping strings to integers.\n\n    Raises:\n        ValueError: If arg1 is empty.\n\n    Example:\n        &gt;&gt;&gt; function(\"test\", 20)\n        {\"test\": 20}\n    \"\"\"\n</code></pre>"},{"location":"development/contributing/#mkdocs-documentation","title":"MkDocs Documentation","text":"<p>Documentation lives in <code>docs/</code>. To preview:</p> <pre><code># Install mkdocs\npip install mkdocs-material\n\n# Serve locally\nmkdocs serve\n</code></pre> <p>Then open http://localhost:8000.</p>"},{"location":"development/contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":""},{"location":"development/contributing/#before-submitting","title":"Before Submitting","text":"<ul> <li>[ ] All tests pass (<code>pytest tests/unit</code>)</li> <li>[ ] No linting errors (<code>ruff check .</code>)</li> <li>[ ] Type checking passes (<code>mypy src/giant</code>)</li> <li>[ ] Code is formatted (<code>ruff format .</code>)</li> <li>[ ] Docstrings added for new code</li> <li>[ ] Tests added for new features</li> </ul>"},{"location":"development/contributing/#pr-description","title":"PR Description","text":"<p>Include:</p> <ol> <li>Summary - What the change does</li> <li>Motivation - Why it's needed</li> <li>Test plan - How it was tested</li> </ol>"},{"location":"development/contributing/#review-process","title":"Review Process","text":"<ol> <li>Automated checks must pass</li> <li>At least one maintainer review</li> <li>Address feedback</li> <li>Merge when approved</li> </ol>"},{"location":"development/contributing/#bug-reports","title":"Bug Reports","text":""},{"location":"development/contributing/#where-to-report","title":"Where to Report","text":"<ul> <li>GitHub Issues: https://github.com/The-Obstacle-Is-The-Way/gigapixel-goblin/issues</li> </ul>"},{"location":"development/contributing/#bug-report-template","title":"Bug Report Template","text":"<pre><code>## Description\nBrief description of the bug.\n\n## Steps to Reproduce\n1. Step one\n2. Step two\n3. Step three\n\n## Expected Behavior\nWhat should happen.\n\n## Actual Behavior\nWhat actually happens.\n\n## Environment\n- OS:\n- Python version:\n- GIANT version:\n- OpenSlide version:\n</code></pre>"},{"location":"development/contributing/#questions","title":"Questions?","text":"<ul> <li>Check existing issues</li> <li>Read the documentation</li> <li>Open a new issue with the <code>question</code> label</li> </ul>"},{"location":"development/contributing/#see-also","title":"See Also","text":"<ul> <li>Testing</li> <li>Project Structure</li> <li>Architecture</li> </ul>"},{"location":"development/testing/","title":"Testing","text":"<p>This guide covers GIANT's testing practices and conventions.</p>"},{"location":"development/testing/#test-organization","title":"Test Organization","text":"<pre><code>tests/\n\u251c\u2500\u2500 conftest.py             # Shared fixtures\n\u251c\u2500\u2500 unit/                   # Unit tests\n\u2502   \u251c\u2500\u2500 agent/\n\u2502   \u251c\u2500\u2500 llm/\n\u2502   \u251c\u2500\u2500 core/\n\u2502   \u251c\u2500\u2500 geometry/\n\u2502   \u251c\u2500\u2500 eval/\n\u2502   \u2514\u2500\u2500 cli/\n\u2514\u2500\u2500 integration/            # Integration tests\n    \u251c\u2500\u2500 wsi/\n    \u2514\u2500\u2500 llm/\n</code></pre>"},{"location":"development/testing/#running-tests","title":"Running Tests","text":""},{"location":"development/testing/#unit-tests-fast","title":"Unit Tests (Fast)","text":"<pre><code># All unit tests\npytest tests/unit\n\n# Stop on first failure\npytest tests/unit -x\n\n# Specific module\npytest tests/unit/llm/\n\n# Single test file\npytest tests/unit/llm/test_openai_client.py\n\n# Single test\npytest tests/unit/llm/test_openai_client.py::test_generate_response\n</code></pre>"},{"location":"development/testing/#with-coverage","title":"With Coverage","text":"<pre><code># Generate coverage report\npytest tests/unit --cov=giant --cov-report=term-missing\n\n# HTML coverage report\npytest tests/unit --cov=giant --cov-report=html\nopen htmlcov/index.html\n</code></pre>"},{"location":"development/testing/#integration-tests","title":"Integration Tests","text":"<pre><code># Requires real WSI files\npytest tests/integration/wsi/\n\n# Requires API keys (costs money!)\nGIANT_RUN_LIVE_TESTS=1 pytest tests/integration/llm/\n</code></pre>"},{"location":"development/testing/#test-markers","title":"Test Markers","text":""},{"location":"development/testing/#available-markers","title":"Available Markers","text":"Marker Description Default <code>@pytest.mark.cost</code> Requires live API, costs money Skipped <code>@pytest.mark.integration</code> Requires real resources Included <code>@pytest.mark.live</code> Requires live external service Skipped"},{"location":"development/testing/#usage","title":"Usage","text":"<pre><code>import pytest\n\n@pytest.mark.cost\nasync def test_real_api_call():\n    \"\"\"This test calls a real API and costs money.\"\"\"\n    ...\n\n@pytest.mark.integration\ndef test_with_real_wsi():\n    \"\"\"This test requires real WSI files.\"\"\"\n    ...\n</code></pre>"},{"location":"development/testing/#running-marked-tests","title":"Running Marked Tests","text":"<pre><code># Skip cost tests (default)\npytest tests/\n\n# Include cost tests (requires GIANT_RUN_LIVE_TESTS=1)\nGIANT_RUN_LIVE_TESTS=1 pytest -m cost\n\n# Only integration tests\npytest -m integration\n\n# Exclude integration tests\npytest -m \"not integration\"\n</code></pre>"},{"location":"development/testing/#writing-tests","title":"Writing Tests","text":""},{"location":"development/testing/#test-structure","title":"Test Structure","text":"<pre><code>\"\"\"Tests for giant.module.feature module.\"\"\"\n\nimport pytest\nfrom giant.module.feature import FeatureClass\n\n\nclass TestFeatureClass:\n    \"\"\"Tests for FeatureClass.\"\"\"\n\n    def test_basic_functionality(self) -&gt; None:\n        \"\"\"Test that basic feature works correctly.\"\"\"\n        # Arrange\n        obj = FeatureClass(name=\"test\")\n\n        # Act\n        result = obj.process()\n\n        # Assert\n        assert result == \"expected\"\n\n    def test_raises_on_invalid_input(self) -&gt; None:\n        \"\"\"Test that invalid input raises ValueError.\"\"\"\n        obj = FeatureClass(name=\"\")\n\n        with pytest.raises(ValueError) as exc_info:\n            obj.process()\n\n        assert \"empty\" in str(exc_info.value)\n</code></pre>"},{"location":"development/testing/#async-tests","title":"Async Tests","text":"<pre><code>import pytest\n\n@pytest.mark.asyncio\nasync def test_async_function() -&gt; None:\n    \"\"\"Test async function.\"\"\"\n    result = await async_function()\n    assert result is not None\n</code></pre>"},{"location":"development/testing/#parametrized-tests","title":"Parametrized Tests","text":"<pre><code>import pytest\n\n@pytest.mark.parametrize(\"input,expected\", [\n    (\"case1\", \"result1\"),\n    (\"case2\", \"result2\"),\n    (\"case3\", \"result3\"),\n])\ndef test_multiple_cases(input: str, expected: str) -&gt; None:\n    \"\"\"Test multiple input cases.\"\"\"\n    assert process(input) == expected\n</code></pre>"},{"location":"development/testing/#fixtures","title":"Fixtures","text":""},{"location":"development/testing/#shared-fixtures-conftestpy","title":"Shared Fixtures (<code>conftest.py</code>)","text":"<pre><code># tests/conftest.py\nimport pytest\nfrom giant.geometry import Region\n\n@pytest.fixture\ndef sample_region() -&gt; Region:\n    \"\"\"Create a sample Region for testing.\"\"\"\n    return Region(x=100, y=200, width=50, height=50)\n\n@pytest.fixture\ndef mock_wsi_path(tmp_path):\n    \"\"\"Create a mock WSI path.\"\"\"\n    wsi = tmp_path / \"test.svs\"\n    wsi.touch()\n    return wsi\n</code></pre>"},{"location":"development/testing/#module-specific-fixtures","title":"Module-Specific Fixtures","text":"<pre><code># tests/unit/llm/conftest.py\nimport pytest\nfrom unittest.mock import MagicMock, AsyncMock\nfrom giant.llm.protocol import LLMResponse, StepResponse\n\n@pytest.fixture\ndef mock_llm_response() -&gt; LLMResponse:\n    \"\"\"Create a mock LLM response.\"\"\"\n    return LLMResponse(\n        step_response=StepResponse(\n            reasoning=\"Test reasoning\",\n            action={\"action_type\": \"answer\", \"answer_text\": \"Test answer\"}\n        ),\n        usage=TokenUsage(\n            prompt_tokens=100,\n            completion_tokens=50,\n            total_tokens=150,\n            cost_usd=0.01,\n        ),\n        model=\"gpt-5.2\",\n        latency_ms=500.0,\n    )\n</code></pre>"},{"location":"development/testing/#mocking","title":"Mocking","text":""},{"location":"development/testing/#basic-mocking","title":"Basic Mocking","text":"<pre><code>from unittest.mock import MagicMock, patch\n\ndef test_with_mock():\n    mock_client = MagicMock()\n    mock_client.call.return_value = \"result\"\n\n    result = function_under_test(client=mock_client)\n\n    mock_client.call.assert_called_once()\n    assert result == \"result\"\n</code></pre>"},{"location":"development/testing/#async-mocking","title":"Async Mocking","text":"<pre><code>from unittest.mock import AsyncMock\n\n@pytest.mark.asyncio\nasync def test_async_with_mock():\n    mock_provider = MagicMock()\n    mock_provider.generate_response = AsyncMock(return_value=mock_response)\n\n    result = await agent.run(provider=mock_provider)\n\n    mock_provider.generate_response.assert_called()\n</code></pre>"},{"location":"development/testing/#patching","title":"Patching","text":"<pre><code>from unittest.mock import patch\n\ndef test_with_patch():\n    with patch(\"giant.module.external_function\") as mock_func:\n        mock_func.return_value = \"mocked\"\n\n        result = function_that_calls_external()\n\n        assert result == \"mocked\"\n</code></pre>"},{"location":"development/testing/#using-pytest-mock","title":"Using pytest-mock","text":"<pre><code>def test_with_mocker(mocker):\n    mock_func = mocker.patch(\"giant.module.function\")\n    mock_func.return_value = \"mocked\"\n\n    result = function_under_test()\n\n    assert result == \"mocked\"\n</code></pre>"},{"location":"development/testing/#test-coverage","title":"Test Coverage","text":""},{"location":"development/testing/#coverage-requirements","title":"Coverage Requirements","text":"<ul> <li>Minimum: 90% (enforced in CI)</li> <li>Target: 95%+</li> </ul>"},{"location":"development/testing/#checking-coverage","title":"Checking Coverage","text":"<pre><code># Terminal report\npytest tests/unit --cov=giant --cov-report=term-missing\n\n# HTML report\npytest tests/unit --cov=giant --cov-report=html\n\n# Fail if below threshold\npytest tests/unit --cov=giant --cov-fail-under=90\n</code></pre>"},{"location":"development/testing/#ignoring-coverage","title":"Ignoring Coverage","text":"<p>For code that can't be tested:</p> <pre><code>if TYPE_CHECKING:  # pragma: no cover\n    from expensive.import import Type\n</code></pre>"},{"location":"development/testing/#test-data","title":"Test Data","text":""},{"location":"development/testing/#sample-wsi-files","title":"Sample WSI Files","text":"<p>For integration tests, download OpenSlide test data:</p> <pre><code>mkdir -p tests/integration/wsi/data\ncurl -L -o tests/integration/wsi/data/CMU-1-Small-Region.svs \\\n    https://openslide.cs.cmu.edu/download/openslide-testdata/Aperio/CMU-1-Small-Region.svs\n</code></pre>"},{"location":"development/testing/#using-test-data","title":"Using Test Data","text":"<pre><code>import pytest\nfrom pathlib import Path\n\n@pytest.fixture\ndef test_wsi_path() -&gt; Path:\n    \"\"\"Path to test WSI file.\"\"\"\n    path = Path(\"tests/integration/wsi/data/CMU-1-Small-Region.svs\")\n    if not path.exists():\n        pytest.skip(\"Test WSI not available\")\n    return path\n</code></pre>"},{"location":"development/testing/#common-patterns","title":"Common Patterns","text":""},{"location":"development/testing/#testing-exceptions","title":"Testing Exceptions","text":"<pre><code>def test_raises_on_error():\n    with pytest.raises(ValueError) as exc_info:\n        function_that_raises()\n\n    assert \"specific message\" in str(exc_info.value)\n</code></pre>"},{"location":"development/testing/#testing-warnings","title":"Testing Warnings","text":"<pre><code>def test_emits_warning():\n    with pytest.warns(DeprecationWarning):\n        deprecated_function()\n</code></pre>"},{"location":"development/testing/#temporary-files","title":"Temporary Files","text":"<pre><code>def test_with_temp_file(tmp_path):\n    test_file = tmp_path / \"test.json\"\n    test_file.write_text('{\"key\": \"value\"}')\n\n    result = load_json(test_file)\n\n    assert result[\"key\"] == \"value\"\n</code></pre>"},{"location":"development/testing/#ci-integration","title":"CI Integration","text":"<p>Tests run automatically on:</p> <ul> <li>Pull requests</li> <li>Pushes to main/dev branches</li> </ul> <p>CI configuration enforces:</p> <ul> <li>All unit tests pass</li> <li>Coverage &gt;= 90%</li> <li>No linting errors</li> <li>Type checking passes</li> </ul>"},{"location":"development/testing/#troubleshooting","title":"Troubleshooting","text":""},{"location":"development/testing/#test-not-found","title":"Test Not Found","text":"<pre><code># Ensure correct path\npytest tests/unit/module/test_file.py -v\n\n# Check for import errors\npython -c \"import tests.unit.module.test_file\"\n</code></pre>"},{"location":"development/testing/#fixture-not-found","title":"Fixture Not Found","text":"<pre><code># List available fixtures\npytest --fixtures\n\n# Check conftest.py location\n</code></pre>"},{"location":"development/testing/#async-test-issues","title":"Async Test Issues","text":"<pre><code># Ensure pytest-asyncio is installed\npip install pytest-asyncio\n\n# Check marker is present\n@pytest.mark.asyncio\nasync def test_async(): ...\n</code></pre>"},{"location":"development/testing/#see-also","title":"See Also","text":"<ul> <li>Contributing</li> <li>Project Structure</li> <li>Architecture</li> </ul>"},{"location":"getting-started/first-benchmark/","title":"Your First Benchmark","text":"<p>Run GIANT on the MultiPathQA benchmark to reproduce paper results.</p>"},{"location":"getting-started/first-benchmark/#prerequisites","title":"Prerequisites","text":"<ul> <li>Installation completed</li> <li>API key configured</li> <li>WSI files downloaded (see Data Acquisition)</li> </ul>"},{"location":"getting-started/first-benchmark/#download-benchmark-metadata","title":"Download Benchmark Metadata","text":"<pre><code># Download the MultiPathQA CSV (questions, answers, metadata)\ngiant download multipathqa\n</code></pre> <p>This creates <code>data/multipathqa/MultiPathQA.csv</code> containing 934 questions across 5 benchmarks.</p>"},{"location":"getting-started/first-benchmark/#check-your-data","title":"Check Your Data","text":"<p>Before running benchmarks, verify WSI files are available:</p> <pre><code># Check which files you have\ngiant check-data gtex\ngiant check-data tcga\ngiant check-data panda\n</code></pre> <p>Example output: <pre><code>All WSIs present for gtex: 191/191 under data/wsi\n</code></pre></p>"},{"location":"getting-started/first-benchmark/#run-a-subset","title":"Run a Subset","text":"<p>Start with a small subset to verify everything works:</p> <pre><code># Run on 5 GTEx items (organ classification)\ngiant benchmark gtex \\\n    --provider openai \\\n    --max-items 5 \\\n    -v\n</code></pre> <p>To see machine-readable output (recommended), add <code>--json</code>:</p> <pre><code>giant benchmark gtex --provider openai --max-items 5 --json | jq\n</code></pre>"},{"location":"getting-started/first-benchmark/#full-benchmark-run","title":"Full Benchmark Run","text":"<p>For complete benchmark runs:</p> <pre><code># Full GTEx (191 items)\ngiant benchmark gtex --provider openai -v\n\n# TCGA cancer diagnosis (221 items)\ngiant benchmark tcga --provider openai -v\n\n# With concurrency for faster runs\ngiant benchmark gtex --concurrency 4 -v\n\n# Resume interrupted runs\ngiant benchmark gtex --resume -v\n</code></pre>"},{"location":"getting-started/first-benchmark/#understanding-results","title":"Understanding Results","text":"<p>Results are saved to <code>results/</code> with:</p> File Contents <code>*_results.json</code> Full results with predictions <code>checkpoints/*.checkpoint.json</code> Resume state for interrupted runs <code>trajectories/*.json</code> Per-item navigation trajectories"},{"location":"getting-started/first-benchmark/#metrics","title":"Metrics","text":"Metric Description <code>balanced_accuracy</code> Accuracy weighted by class frequency <code>accuracy</code> Simple accuracy <code>bootstrap_ci</code> 95% confidence interval"},{"location":"getting-started/first-benchmark/#compare-to-paper","title":"Compare to Paper","text":"Benchmark Our Result Paper (GIANT x1) Paper (GIANT x5) GTEx (20-way) 67.6% 53.7% 60.7% TCGA (30-way) TBD 32.3% 29.3% PANDA (6-way) TBD 23.2% 25.4%"},{"location":"getting-started/first-benchmark/#cost-estimates","title":"Cost Estimates","text":"<p>Costs depend on provider/model, prompt length, and how many steps each item takes. For safe estimation:</p> <ol> <li>Run a small sample: <code>giant benchmark &lt;dataset&gt; --max-items 5 --json</code></li> <li>Extrapolate from <code>total_cost</code></li> <li>Use <code>--budget-usd</code> as a guardrail on full runs</li> </ol>"},{"location":"getting-started/first-benchmark/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/first-benchmark/#wsi-file-not-found","title":"\"WSI file not found\"","text":"<pre><code># Check your WSI directory structure\ngiant check-data gtex -v\n</code></pre> <p>See Data Acquisition for download instructions.</p>"},{"location":"getting-started/first-benchmark/#api-rate-limits","title":"API Rate Limits","text":"<p>Reduce concurrency: <pre><code>giant benchmark gtex --concurrency 1 -v\n</code></pre></p>"},{"location":"getting-started/first-benchmark/#resume-after-errors","title":"Resume After Errors","text":"<p>Runs automatically checkpoint. Resume with: <pre><code>giant benchmark gtex --resume -v\n</code></pre></p>"},{"location":"getting-started/first-benchmark/#next-steps","title":"Next Steps","text":"<ul> <li>Running Benchmarks Guide - Advanced options</li> <li>Benchmark Results - Our official results</li> <li>Visualizing Trajectories - Inspect agent behavior</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This guide walks you through setting up GIANT on your local machine.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11+ (tested with 3.11, 3.12)</li> <li>OpenSlide library (for reading whole-slide images)</li> <li>uv package manager (recommended) or pip</li> </ul>"},{"location":"getting-started/installation/#system-dependencies","title":"System Dependencies","text":""},{"location":"getting-started/installation/#macos","title":"macOS","text":"<pre><code># Install OpenSlide\nbrew install openslide\n\n# Install uv (recommended package manager)\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre>"},{"location":"getting-started/installation/#ubuntudebian","title":"Ubuntu/Debian","text":"<pre><code># Install OpenSlide\nsudo apt-get install openslide-tools python3-openslide\n\n# Install uv\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre>"},{"location":"getting-started/installation/#windows","title":"Windows","text":"<pre><code># OpenSlide Windows binaries: https://openslide.org/download/\n# Add OpenSlide bin directory to PATH\n\n# Install uv\npowershell -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n</code></pre>"},{"location":"getting-started/installation/#clone-and-install","title":"Clone and Install","text":"<pre><code># Clone the repository\ngit clone https://github.com/The-Obstacle-Is-The-Way/gigapixel-goblin.git\ncd gigapixel-goblin\n\n# Install dependencies with uv (recommended)\nuv sync\n\n# Activate the virtual environment\nsource .venv/bin/activate\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<pre><code># Check GIANT version\ngiant version\n\n# Verify OpenSlide is working\npython -c \"import openslide; print(f'OpenSlide version: {openslide.__version__}')\"\n\n# Run unit tests\nuv run pytest tests/unit -x\n</code></pre>"},{"location":"getting-started/installation/#api-keys","title":"API Keys","text":"<p>GIANT requires API keys for LLM providers. Create a <code>.env</code> file in the project root:</p> <pre><code># .env\nOPENAI_API_KEY=sk-...\nANTHROPIC_API_KEY=sk-ant-...\n</code></pre> <p>See Configuring Providers for detailed setup instructions.</p>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quickstart - Run your first inference</li> <li>Data Acquisition - Download WSI files for benchmarks</li> <li>Configuring Providers - Set up OpenAI/Anthropic</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quickstart","text":"<p>Run your first GIANT inference on a whole-slide image.</p>"},{"location":"getting-started/quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>Installation completed</li> <li>OpenAI or Anthropic API key configured in <code>.env</code></li> <li>A WSI file (<code>.svs</code>, <code>.tiff</code>, or <code>.ndpi</code> format)</li> </ul>"},{"location":"getting-started/quickstart/#download-a-test-slide","title":"Download a Test Slide","text":"<p>For testing, download a small WSI from OpenSlide's test data:</p> <pre><code>mkdir -p data/test\ncurl -L -o data/test/CMU-1-Small-Region.svs \\\n    https://openslide.cs.cmu.edu/download/openslide-testdata/Aperio/CMU-1-Small-Region.svs\n</code></pre>"},{"location":"getting-started/quickstart/#run-inference","title":"Run Inference","text":""},{"location":"getting-started/quickstart/#basic-usage","title":"Basic Usage","text":"<pre><code># Activate environment and load API keys\nsource .venv/bin/activate\nsource .env\n\n# Run GIANT on a WSI with a question\ngiant run data/test/CMU-1-Small-Region.svs \\\n    -q \"What type of tissue is shown in this slide?\"\n</code></pre>"},{"location":"getting-started/quickstart/#expected-output","title":"Expected Output","text":"<pre><code>Answer: This slide shows squamous epithelial tissue...\nCost: $0.0234\nTurns: 3\n</code></pre>"},{"location":"getting-started/quickstart/#cli-options","title":"CLI Options","text":"<pre><code># Use Anthropic instead of OpenAI\ngiant run slide.svs -q \"Question?\" --provider anthropic\n\n# Limit navigation steps\ngiant run slide.svs -q \"Question?\" --max-steps 5\n\n# Set a cost budget (USD)\ngiant run slide.svs -q \"Question?\" --budget-usd 0.10\n\n# Save the navigation trajectory\ngiant run slide.svs -q \"Question?\" --output trajectory.json\n\n# Multiple runs with majority voting\ngiant run slide.svs -q \"Question?\" --runs 3\n\n# JSON output for scripting\ngiant run slide.svs -q \"Question?\" --json\n</code></pre>"},{"location":"getting-started/quickstart/#understanding-the-output","title":"Understanding the Output","text":"<p>GIANT returns:</p> Field Description <code>answer</code> The model's response to your question <code>cost</code> Total API cost in USD <code>turns</code> Number of navigation steps taken <code>agreement</code> (with <code>--runs &gt; 1</code>) Fraction of runs that agreed"},{"location":"getting-started/quickstart/#visualize-navigation","title":"Visualize Navigation","text":"<p>After running with <code>--output</code>, visualize the agent's trajectory:</p> <pre><code>giant visualize trajectory.json --open\n</code></pre> <p>This opens an interactive HTML viewer showing:</p> <ul> <li>Initial thumbnail with axis guides</li> <li>Each cropped region the agent examined</li> <li>The agent's reasoning at each step</li> <li>Final answer</li> </ul>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>First Benchmark - Run on real benchmark data</li> <li>Algorithm Explanation - Understand how GIANT navigates</li> <li>CLI Reference - All command options</li> </ul>"},{"location":"guides/configuring-providers/","title":"Configuring Providers","text":"<p>This guide covers setting up LLM providers for GIANT.</p>"},{"location":"guides/configuring-providers/#overview","title":"Overview","text":"<p>GIANT currently supports these LLM providers:</p> Provider Model Status OpenAI <code>gpt-5.2</code> Default Anthropic <code>claude-sonnet-4-5-20250929</code> Supported <p><code>gemini-3-pro-preview</code> is present in the model registry for future work, but the Google/Gemini provider is not implemented in the CLI yet.</p>"},{"location":"guides/configuring-providers/#api-key-configuration","title":"API Key Configuration","text":""},{"location":"guides/configuring-providers/#environment-variables","title":"Environment Variables","text":"<p>Create a <code>.env</code> file in the project root:</p> <pre><code># .env\nOPENAI_API_KEY=sk-proj-...\nANTHROPIC_API_KEY=sk-ant-api03-...\n</code></pre> <p>Load before running:</p> <pre><code>source .env\ngiant run slide.svs -q \"Question?\"\n</code></pre>"},{"location":"guides/configuring-providers/#shell-export","title":"Shell Export","text":"<p>Alternatively, export directly:</p> <pre><code>export OPENAI_API_KEY=sk-proj-...\ngiant run slide.svs -q \"Question?\"\n</code></pre>"},{"location":"guides/configuring-providers/#security-notes","title":"Security Notes","text":"<ol> <li>Never commit <code>.env</code> files - Add to <code>.gitignore</code></li> <li>Avoid placeholder values - Keys containing <code>your-key</code> or <code>changeme</code> are treated as not configured</li> <li>Use environment-specific keys - Separate keys for dev/test/prod</li> <li>Rotate compromised keys - If exposed, regenerate immediately</li> </ol>"},{"location":"guides/configuring-providers/#openai-setup","title":"OpenAI Setup","text":""},{"location":"guides/configuring-providers/#getting-an-api-key","title":"Getting an API Key","text":"<ol> <li>Go to OpenAI Platform</li> <li>Create an account or sign in</li> <li>Navigate to API Keys</li> <li>Create a new secret key</li> <li>Copy and save securely</li> </ol>"},{"location":"guides/configuring-providers/#configuration","title":"Configuration","text":"<pre><code># .env\nOPENAI_API_KEY=sk-proj-abc123...\n</code></pre>"},{"location":"guides/configuring-providers/#usage","title":"Usage","text":"<pre><code>giant run slide.svs -q \"Question?\" --provider openai --model gpt-5.2\n</code></pre>"},{"location":"guides/configuring-providers/#pricing","title":"Pricing","text":"Model Input (per 1M) Output (per 1M) gpt-5.2 $1.75 $14.00"},{"location":"guides/configuring-providers/#target-image-size","title":"Target Image Size","text":"<p>OpenAI provider uses 1000px target size for higher resolution analysis.</p>"},{"location":"guides/configuring-providers/#anthropic-setup","title":"Anthropic Setup","text":""},{"location":"guides/configuring-providers/#getting-an-api-key_1","title":"Getting an API Key","text":"<ol> <li>Go to Anthropic Console</li> <li>Create an account or sign in</li> <li>Navigate to API Keys</li> <li>Create a new key</li> <li>Copy and save securely</li> </ol>"},{"location":"guides/configuring-providers/#configuration_1","title":"Configuration","text":"<pre><code># .env\nANTHROPIC_API_KEY=sk-ant-api03-abc123...\n</code></pre>"},{"location":"guides/configuring-providers/#usage_1","title":"Usage","text":"<pre><code>giant run slide.svs -q \"Question?\" --provider anthropic --model claude-sonnet-4-5-20250929\n</code></pre>"},{"location":"guides/configuring-providers/#pricing_1","title":"Pricing","text":"Model Input (per 1M) Output (per 1M) claude-sonnet-4-5-20250929 $3.00 $15.00"},{"location":"guides/configuring-providers/#target-image-size_1","title":"Target Image Size","text":"<p>Anthropic provider uses 500px target size (cost-optimized).</p>"},{"location":"guides/configuring-providers/#model-registry","title":"Model Registry","text":"<p>Only approved models are allowed. This ensures:</p> <ol> <li>Consistency - Reproducible results</li> <li>Cost control - Known pricing</li> <li>Compatibility - Tested integration</li> </ol>"},{"location":"guides/configuring-providers/#approved-models","title":"Approved Models","text":"<pre><code>APPROVED_MODELS = {\n    \"gpt-5.2\",                      # OpenAI\n    \"claude-sonnet-4-5-20250929\",   # Anthropic\n    \"gemini-3-pro-preview\",         # Reserved (Google/Gemini provider not yet implemented)\n}\n</code></pre>"},{"location":"guides/configuring-providers/#validation","title":"Validation","text":"<p>Invalid models are rejected at runtime:</p> <pre><code>giant run slide.svs -q \"?\" --model gpt-4o\n# Error: Model 'gpt-4o' is not approved. Allowed for openai approved models: gpt-5.2. See docs/models/model-registry.md.\n</code></pre> <p>See Model Registry for the full list.</p>"},{"location":"guides/configuring-providers/#provider-comparison","title":"Provider Comparison","text":"Feature OpenAI Anthropic Default model gpt-5.2 claude-sonnet-4-5-20250929 Target size 1000px 500px Structured output JSON schema via <code>responses.create</code> Tool use via <code>submit_step</code> Image cost model Flat per image Pixel-based"},{"location":"guides/configuring-providers/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/configuring-providers/#api-key-not-set","title":"\"API key not set\"","text":"<pre><code>Error: OpenAI API key not configured. Set it in .env file or OPENAI_API_KEY environment variable.\n</code></pre> <p>Fix: Ensure the key is exported: <pre><code>source .env\necho $OPENAI_API_KEY  # Should print your key\n</code></pre></p>"},{"location":"guides/configuring-providers/#invalid-api-key","title":"\"Invalid API key\"","text":"<pre><code>Error: openai.AuthenticationError: Invalid API key\n</code></pre> <p>Fix: Check your key is correct and has not expired.</p>"},{"location":"guides/configuring-providers/#placeholder-key-treated-as-not-configured","title":"\"Placeholder key treated as not configured\"","text":"<p>If your key contains obvious placeholder strings (e.g., <code>your-key</code>, <code>changeme</code>), GIANT treats it as missing and raises the same configuration error as an unset key.</p>"},{"location":"guides/configuring-providers/#model-not-approved","title":"\"Model not approved\"","text":"<pre><code>Error: Model 'gpt-4o' not approved\n</code></pre> <p>Fix: Use an approved model from the registry.</p>"},{"location":"guides/configuring-providers/#rate-limits","title":"Rate Limits","text":"<p>If you hit rate limits:</p> <ol> <li>Reduce concurrency: <code>--concurrency 1</code></li> <li>Add delays between requests (not currently configurable)</li> <li>Upgrade your API tier</li> </ol>"},{"location":"guides/configuring-providers/#programmatic-usage","title":"Programmatic Usage","text":"<pre><code>from giant.llm import create_provider\n\n# OpenAI\nprovider = create_provider(\"openai\", model=\"gpt-5.2\")\n\n# Anthropic\nprovider = create_provider(\"anthropic\", model=\"claude-sonnet-4-5-20250929\")\n\n# Use with agent\nfrom giant.agent import GIANTAgent, AgentConfig\n\nagent = GIANTAgent(\n    wsi_path=\"slide.svs\",\n    question=\"What tissue is this?\",\n    llm_provider=provider,\n    config=AgentConfig(max_steps=10),\n)\nresult = await agent.run()\n</code></pre>"},{"location":"guides/configuring-providers/#next-steps","title":"Next Steps","text":"<ul> <li>Model Registry - All approved models</li> <li>Running Inference - Use configured provider</li> <li>LLM Integration - Technical details</li> </ul>"},{"location":"guides/running-benchmarks/","title":"Running Benchmarks","text":"<p>This guide covers running GIANT on the MultiPathQA benchmark suite.</p>"},{"location":"guides/running-benchmarks/#prerequisites","title":"Prerequisites","text":"<ol> <li>Installation completed</li> <li>API key configured in <code>.env</code></li> <li>WSI files downloaded (see Data Acquisition)</li> <li>MultiPathQA metadata downloaded:    <pre><code>giant download multipathqa\n</code></pre></li> </ol>"},{"location":"guides/running-benchmarks/#basic-usage","title":"Basic Usage","text":"<pre><code>giant benchmark &lt;dataset&gt; [options]\n</code></pre>"},{"location":"guides/running-benchmarks/#available-datasets","title":"Available Datasets","text":"Dataset Task Items WSI Source <code>gtex</code> Organ Classification (20-way) 191 GTEx <code>tcga</code> Cancer Diagnosis (30-way) 221 TCGA <code>panda</code> Prostate Grading (6-way) 197 PANDA <code>tcga_expert_vqa</code> Pathologist-Authored VQA 128 TCGA <code>tcga_slidebench</code> SlideBench VQA 197 TCGA"},{"location":"guides/running-benchmarks/#example","title":"Example","text":"<pre><code># Full GTEx benchmark\ngiant benchmark gtex --provider openai -v\n</code></pre>"},{"location":"guides/running-benchmarks/#command-options","title":"Command Options","text":""},{"location":"guides/running-benchmarks/#data-options","title":"Data Options","text":"Option Default Description <code>--csv-path</code> <code>data/multipathqa/MultiPathQA.csv</code> Path to benchmark CSV <code>--wsi-root</code> <code>data/wsi</code> Root directory for WSI files <code>-o, --output-dir</code> <code>results</code> Output directory"},{"location":"guides/running-benchmarks/#provider-options","title":"Provider Options","text":"Option Default Description <code>-p, --provider</code> <code>openai</code> LLM provider <code>--model</code> <code>gpt-5.2</code> Model ID"},{"location":"guides/running-benchmarks/#run-options","title":"Run Options","text":"Option Default Description <code>-T, --max-steps</code> <code>20</code> Max navigation steps per item <code>-r, --runs</code> <code>1</code> Runs per item (for majority voting) <code>-c, --concurrency</code> <code>4</code> Max concurrent API calls <code>--max-items</code> <code>0</code> (all) Limit items to process <code>--budget-usd</code> <code>0</code> (disabled) Total cost budget <code>--skip-missing/--no-skip-missing</code> <code>--skip-missing</code> Skip missing WSI files <code>--resume/--no-resume</code> <code>--resume</code> Resume from checkpoint"},{"location":"guides/running-benchmarks/#output-options","title":"Output Options","text":"Option Default Description <code>--json</code> False JSON output for scripting <code>-v, --verbose</code> 0 Verbosity level"},{"location":"guides/running-benchmarks/#workflow-examples","title":"Workflow Examples","text":""},{"location":"guides/running-benchmarks/#quick-validation-5-items","title":"Quick Validation (5 items)","text":"<pre><code>giant benchmark gtex \\\n    --max-items 5 \\\n    --provider openai \\\n    -v\n</code></pre>"},{"location":"guides/running-benchmarks/#full-benchmark-with-resume","title":"Full Benchmark with Resume","text":"<pre><code># Start benchmark (may take hours)\ngiant benchmark tcga --provider openai -v\n\n# If interrupted, resume:\ngiant benchmark tcga --resume -v\n</code></pre>"},{"location":"guides/running-benchmarks/#high-concurrency","title":"High Concurrency","text":"<pre><code># Faster but higher API load\ngiant benchmark gtex --concurrency 8 -v\n</code></pre>"},{"location":"guides/running-benchmarks/#multiple-runs-majority-voting","title":"Multiple Runs (Majority Voting)","text":"<pre><code># 3 runs per item, report majority vote\ngiant benchmark gtex --runs 3 -v\n</code></pre>"},{"location":"guides/running-benchmarks/#cost-limited-run","title":"Cost-Limited Run","text":"<pre><code># Stop when budget is exhausted\ngiant benchmark tcga --budget-usd 10.00 -v\n</code></pre>"},{"location":"guides/running-benchmarks/#output-structure","title":"Output Structure","text":"<p>Results are saved to <code>--output-dir</code> (default: <code>results/</code>):</p> <pre><code>results/\n\u251c\u2500\u2500 gtex_giant_openai_gpt-5.2_results.json    # Full results\n\u251c\u2500\u2500 checkpoints/\n\u2502   \u2514\u2500\u2500 gtex_giant_openai_gpt-5.2.checkpoint.json  # Resume state\n\u2514\u2500\u2500 trajectories/\n    \u251c\u2500\u2500 GTEX-OIZH-0626_run0.json              # Per-item trajectories\n    \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"guides/running-benchmarks/#results-json","title":"Results JSON","text":"<pre><code>{\n  \"run_id\": \"gtex_giant_openai_gpt-5.2\",\n  \"benchmark_name\": \"gtex\",\n  \"model_name\": \"gpt-5.2\",\n  \"config\": {\n    \"mode\": \"giant\",\n    \"max_steps\": 20,\n    \"runs_per_item\": 1,\n    \"max_concurrent\": 4,\n    \"max_items\": null,\n    \"skip_missing_wsis\": true\n  },\n  \"results\": [\n    {\n      \"item_id\": \"GTEX-OIZH-0626\",\n      \"prediction\": \"Heart\",\n      \"predicted_label\": 1,\n      \"truth_label\": 1,\n      \"correct\": true,\n      \"cost_usd\": 0.0378,\n      \"total_tokens\": 1234,\n      \"trajectory_file\": \"results/trajectories/GTEX-OIZH-0626_run0.json\",\n      \"error\": null\n    },\n    ...\n  ],\n  \"metrics\": {\n    \"metric_type\": \"balanced_accuracy\",\n    \"bootstrap_mean\": 0.676,\n    \"bootstrap_std\": 0.031,\n    \"bootstrap_ci_lower\": 0.614,\n    \"bootstrap_ci_upper\": 0.735,\n    \"n_replicates\": 1000\n  },\n  \"total_cost_usd\": 7.21,\n  \"total_tokens\": 1234567,\n  \"timestamp\": \"2025-12-27T00:00:00Z\"\n}\n</code></pre>"},{"location":"guides/running-benchmarks/#metrics","title":"Metrics","text":"Metric Description <code>metric_type</code> <code>\"balanced_accuracy\"</code> for classification, <code>\"accuracy\"</code> for VQA <code>bootstrap_mean</code> Bootstrap mean of the metric <code>bootstrap_std</code> Bootstrap standard deviation <code>bootstrap_ci_lower</code> / <code>bootstrap_ci_upper</code> 95% bootstrap confidence interval <p>Classification tasks (<code>tcga</code>, <code>gtex</code>, <code>panda</code>) use balanced accuracy (per the paper). VQA tasks use accuracy.</p>"},{"location":"guides/running-benchmarks/#cost-estimates","title":"Cost Estimates","text":"<p>Costs vary significantly by provider/model and how many steps the agent uses per item. For a safe estimate:</p> <ol> <li>Run a small sample: <code>giant benchmark &lt;dataset&gt; --max-items 5 --json</code></li> <li>Extrapolate from <code>total_cost</code></li> <li>Use <code>--budget-usd</code> on longer runs</li> </ol>"},{"location":"guides/running-benchmarks/#checkpoint-and-resume","title":"Checkpoint and Resume","text":"<p>Benchmarks automatically checkpoint progress:</p> <ol> <li>After each item completes</li> <li>On graceful shutdown (Ctrl+C)</li> </ol> <p>To resume an interrupted run:</p> <pre><code>giant benchmark gtex --resume -v\n</code></pre> <p>The checkpoint file contains: - Completed item indices - Partial results - Run configuration</p>"},{"location":"guides/running-benchmarks/#handling-missing-files","title":"Handling Missing Files","text":"<p>By default, missing WSI files are skipped:</p> <pre><code># Skip missing (default)\ngiant benchmark gtex --skip-missing -v\n\n# Fail on missing\ngiant benchmark gtex --no-skip-missing -v\n</code></pre> <p>Check which files are missing:</p> <pre><code>giant check-data gtex -v\n</code></pre>"},{"location":"guides/running-benchmarks/#comparing-to-paper-results","title":"Comparing to Paper Results","text":"Benchmark Our Result Paper (GIANT x1) Paper (GIANT x5) GTEx 67.6% 53.7% 60.7% TCGA TBD 32.3% 29.3% PANDA TBD 23.2% 25.4% Expert VQA TBD 57.0% 62.5% SlideBench TBD 58.9% 59.4% <p>See Benchmark Results for detailed analysis.</p>"},{"location":"guides/running-benchmarks/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/running-benchmarks/#wsi-file-not-found","title":"\"WSI file not found\"","text":"<p>Check data availability: <pre><code>giant check-data gtex -v\n</code></pre></p> <p>See Data Acquisition for download instructions.</p>"},{"location":"guides/running-benchmarks/#rate-limits","title":"Rate Limits","text":"<p>Reduce concurrency: <pre><code>giant benchmark gtex --concurrency 1 -v\n</code></pre></p>"},{"location":"guides/running-benchmarks/#memory-issues","title":"Memory Issues","text":"<p>Large WSIs can consume memory. Try: <pre><code># Process one at a time\ngiant benchmark gtex --concurrency 1 -v\n</code></pre></p>"},{"location":"guides/running-benchmarks/#checkpoint-corruption","title":"Checkpoint Corruption","text":"<p>Delete the checkpoint and restart: <pre><code>rm results/checkpoints/gtex_*.checkpoint.json\ngiant benchmark gtex -v\n</code></pre></p>"},{"location":"guides/running-benchmarks/#next-steps","title":"Next Steps","text":"<ul> <li>Visualizing Trajectories - Inspect results</li> <li>Benchmark Results - Official results</li> <li>CLI Reference - All options</li> </ul>"},{"location":"guides/running-inference/","title":"Running Inference","text":"<p>This guide covers running GIANT on single whole-slide images.</p>"},{"location":"guides/running-inference/#basic-usage","title":"Basic Usage","text":"<pre><code>giant run &lt;wsi_path&gt; -q \"&lt;question&gt;\"\n</code></pre>"},{"location":"guides/running-inference/#example","title":"Example","text":"<pre><code>giant run data/wsi/tcga/TCGA-02-0266-01Z-00-DX1.svs \\\n    -q \"What type of cancer is shown in this slide?\"\n</code></pre>"},{"location":"guides/running-inference/#command-options","title":"Command Options","text":""},{"location":"guides/running-inference/#required-arguments","title":"Required Arguments","text":"Argument Description <code>WSI_PATH</code> Path to the WSI file (<code>.svs</code>, <code>.tiff</code>, <code>.ndpi</code>) <code>-q, --question</code> Question to answer about the slide"},{"location":"guides/running-inference/#provider-options","title":"Provider Options","text":"Option Default Description <code>-p, --provider</code> <code>openai</code> LLM provider (<code>openai</code>, <code>anthropic</code>) <code>--model</code> <code>gpt-5.2</code> Model ID (see Model Registry) <pre><code># Use Anthropic\ngiant run slide.svs -q \"Question?\" --provider anthropic\n\n# Specify model explicitly\ngiant run slide.svs -q \"Question?\" --provider openai --model gpt-5.2\n</code></pre>"},{"location":"guides/running-inference/#navigation-options","title":"Navigation Options","text":"Option Default Description <code>-T, --max-steps</code> <code>20</code> Maximum navigation steps <code>--budget-usd</code> <code>0</code> (disabled) Cost limit in USD <code>-m, --mode</code> <code>giant</code> Evaluation mode <pre><code># Limit to 5 navigation steps\ngiant run slide.svs -q \"Question?\" --max-steps 5\n\n# Set cost budget ($0.50 max)\ngiant run slide.svs -q \"Question?\" --budget-usd 0.50\n</code></pre>"},{"location":"guides/running-inference/#output-options","title":"Output Options","text":"Option Default Description <code>-o, --output</code> None Save trajectory to JSON <code>--json</code> False Output as JSON (for scripting) <code>-v, --verbose</code> 0 Increase verbosity (<code>-v</code>, <code>-vv</code>, <code>-vvv</code>) <pre><code># Save trajectory for visualization\ngiant run slide.svs -q \"Question?\" -o trajectory.json\n\n# JSON output for scripting\ngiant run slide.svs -q \"Question?\" --json | jq '.answer'\n\n# Verbose logging\ngiant run slide.svs -q \"Question?\" -vv\n</code></pre>"},{"location":"guides/running-inference/#multiple-runs","title":"Multiple Runs","text":"Option Default Description <code>-r, --runs</code> <code>1</code> Number of runs for majority voting <pre><code># Run 3 times, take majority vote\ngiant run slide.svs -q \"Question?\" --runs 3\n</code></pre> <p>With <code>--runs &gt; 1</code>, output includes: - <code>answer</code>: Most common answer - <code>agreement</code>: Fraction of runs that agreed</p>"},{"location":"guides/running-inference/#evaluation-modes","title":"Evaluation Modes","text":"<p>GIANT supports three evaluation modes:</p> Mode Description <code>giant</code> Full agentic navigation (default) <code>thumbnail</code> Single thumbnail, no cropping <code>patch</code> Random patch sampling (CLAM-style) <pre><code># Compare methods\ngiant run slide.svs -q \"Question?\" --mode giant\ngiant run slide.svs -q \"Question?\" --mode thumbnail\ngiant run slide.svs -q \"Question?\" --mode patch\n</code></pre>"},{"location":"guides/running-inference/#output-format","title":"Output Format","text":""},{"location":"guides/running-inference/#standard-output","title":"Standard Output","text":"<pre><code>Answer: This slide shows adenocarcinoma of the lung...\nCost: $0.0432\nTurns: 4\n</code></pre>"},{"location":"guides/running-inference/#json-output-json","title":"JSON Output (<code>--json</code>)","text":"<pre><code>{\n  \"success\": true,\n  \"answer\": \"This slide shows adenocarcinoma of the lung...\",\n  \"total_cost\": 0.0432,\n  \"agreement\": 1.0,\n  \"turns\": 4\n}\n</code></pre>"},{"location":"guides/running-inference/#trajectory-file-output","title":"Trajectory File (<code>--output</code>)","text":"<pre><code>{\n  \"wsi_path\": \"/path/to/slide.svs\",\n  \"question\": \"What type of cancer...\",\n  \"turns\": [\n    {\n      \"step_index\": 0,\n      \"image_base64\": \"...\",\n      \"response\": {\n        \"reasoning\": \"I see a tissue sample with...\",\n        \"action\": {\"action_type\": \"crop\", \"x\": 45000, ...}\n      },\n      \"region\": {\"x\": 45000, \"y\": 32000, ...}\n    },\n    ...\n  ],\n  \"answer\": \"This slide shows adenocarcinoma...\",\n  \"success\": true,\n  \"total_cost\": 0.0432\n}\n</code></pre>"},{"location":"guides/running-inference/#error-handling","title":"Error Handling","text":""},{"location":"guides/running-inference/#exit-codes","title":"Exit Codes","text":"Code Meaning <code>0</code> Success <code>1</code> Failure (error message in stderr)"},{"location":"guides/running-inference/#common-errors","title":"Common Errors","text":"<p>WSI file not found: <pre><code>Error: Path 'slide.svs' does not exist.\n</code></pre></p> <p>OpenSlide can't read file: <pre><code>Error: openslide.OpenSlideError: Cannot open ...\n</code></pre></p> <p>API key not configured: <pre><code>Error: OpenAI API key not configured. Set it in .env file or OPENAI_API_KEY environment variable.\n</code></pre></p> <p>Budget exceeded: If <code>--budget-usd</code> is exceeded, GIANT forces an answer and exits non-zero (the run artifact includes <code>error_message: \"Budget exceeded\"</code>).</p>"},{"location":"guides/running-inference/#examples","title":"Examples","text":""},{"location":"guides/running-inference/#cancer-diagnosis","title":"Cancer Diagnosis","text":"<pre><code>giant run data/wsi/tcga/TCGA-02-0266-01Z-00-DX1.svs \\\n    -q \"What type of cancer is present in this slide? Options: Lung Adenocarcinoma, Breast Invasive Carcinoma, Kidney Renal Clear Cell Carcinoma\"\n</code></pre>"},{"location":"guides/running-inference/#tissue-classification","title":"Tissue Classification","text":"<pre><code>giant run data/wsi/gtex/GTEX-OIZH-0626.tiff \\\n    -q \"What organ is this tissue from? Options: Heart, Lung, Liver, Kidney, Brain\"\n</code></pre>"},{"location":"guides/running-inference/#grading","title":"Grading","text":"<pre><code>giant run data/wsi/panda/abc123.tiff \\\n    -q \"What is the ISUP grade group for this prostate biopsy? Options: 0, 1, 2, 3, 4, 5\"\n</code></pre>"},{"location":"guides/running-inference/#quick-test-cost-limited","title":"Quick Test (Cost-Limited)","text":"<pre><code>giant run slide.svs -q \"What tissue is this?\" \\\n    --max-steps 3 \\\n    --budget-usd 0.10 \\\n    -v\n</code></pre>"},{"location":"guides/running-inference/#visualizing-results","title":"Visualizing Results","text":"<p>After running with <code>--output</code>:</p> <pre><code>giant visualize trajectory.json --open\n</code></pre> <p>See Visualizing Trajectories for details.</p>"},{"location":"guides/running-inference/#next-steps","title":"Next Steps","text":"<ul> <li>Running Benchmarks - Batch evaluation</li> <li>Configuring Providers - API setup</li> <li>CLI Reference - All options</li> </ul>"},{"location":"guides/visualizing-trajectories/","title":"Visualizing Trajectories","text":"<p>This guide covers using GIANT's trajectory visualization tools.</p>"},{"location":"guides/visualizing-trajectories/#overview","title":"Overview","text":"<p>After running inference with <code>--output</code>, you can visualize the agent's navigation:</p> <pre><code># Run with trajectory output\ngiant run slide.svs -q \"Question?\" -o trajectory.json\n\n# Visualize\ngiant visualize trajectory.json\n</code></pre>"},{"location":"guides/visualizing-trajectories/#basic-usage","title":"Basic Usage","text":"<pre><code>giant visualize &lt;trajectory_path&gt; [options]\n</code></pre>"},{"location":"guides/visualizing-trajectories/#options","title":"Options","text":"Option Default Description <code>-o, --output</code> Auto-generated Output HTML file path <code>--open/--no-open</code> <code>--open</code> Open in browser <code>-v, --verbose</code> 0 Verbosity level <code>--json</code> False JSON output"},{"location":"guides/visualizing-trajectories/#examples","title":"Examples","text":"<pre><code># Open in browser (default)\ngiant visualize trajectory.json\n\n# Save without opening\ngiant visualize trajectory.json --no-open -o my_viz.html\n\n# Specify output path\ngiant visualize trajectory.json -o results/viz/run1.html\n</code></pre>"},{"location":"guides/visualizing-trajectories/#visualization-features","title":"Visualization Features","text":""},{"location":"guides/visualizing-trajectories/#thumbnail-view","title":"Thumbnail View","text":"<p>The initial view shows the full slide thumbnail with:</p> <ul> <li>Axis guides - Red lines with coordinate labels</li> <li>Crop overlays - Rectangles showing examined regions</li> <li>Region numbers - Step index for each crop</li> </ul>"},{"location":"guides/visualizing-trajectories/#step-by-step-view","title":"Step-by-Step View","text":"<p>Each navigation step displays:</p> Element Description Image The cropped region shown to the model Reasoning Model's chain-of-thought explanation Action Crop coordinates or final answer Region Level-0 bounding box (when available)"},{"location":"guides/visualizing-trajectories/#final-answer","title":"Final Answer","text":"<p>The visualization highlights:</p> <ul> <li>Total navigation steps</li> <li>Final answer text</li> <li>Total cost</li> </ul>"},{"location":"guides/visualizing-trajectories/#trajectory-file-format","title":"Trajectory File Format","text":"<p>GIANT can visualize two related JSON formats:</p> <ol> <li>Run artifacts from <code>giant run --output</code> (trajectory + run metadata like <code>success</code> and <code>total_cost</code>)</li> <li>Raw trajectories from <code>giant benchmark</code> under <code>results/trajectories/</code> (trajectory only)</li> </ol> <pre><code>{\n  \"wsi_path\": \"/path/to/slide.svs\",\n  \"question\": \"What type of cancer is shown?\",\n  \"slide_width\": 100000,\n  \"slide_height\": 80000,\n  \"thumbnail_base64\": \"...\",\n  \"turns\": [\n    {\n      \"step_index\": 0,\n      \"image_base64\": \"...\",\n      \"response\": {\n        \"reasoning\": \"I observe a tissue section with...\",\n        \"action\": {\n          \"action_type\": \"crop\",\n          \"x\": 45000,\n          \"y\": 32000,\n          \"width\": 10000,\n          \"height\": 10000\n        }\n      },\n      \"region\": {\n        \"x\": 45000,\n        \"y\": 32000,\n        \"width\": 10000,\n        \"height\": 10000\n      }\n    },\n    {\n      \"step_index\": 1,\n      \"image_base64\": \"...\",\n      \"response\": {\n        \"reasoning\": \"At higher magnification, I can see...\",\n        \"action\": {\n          \"action_type\": \"answer\",\n          \"answer_text\": \"This is adenocarcinoma.\"\n        }\n      },\n      \"region\": {\n        \"x\": 45000,\n        \"y\": 32000,\n        \"width\": 10000,\n        \"height\": 10000\n      }\n    }\n  ],\n  \"answer\": \"This is adenocarcinoma.\",\n  \"success\": true,\n  \"total_cost\": 0.0432,\n  \"mode\": \"giant\",\n  \"provider\": \"openai\",\n  \"model\": \"gpt-5.2\"\n}\n</code></pre>"},{"location":"guides/visualizing-trajectories/#batch-visualization","title":"Batch Visualization","text":"<p>For benchmark runs, trajectories are saved per-item:</p> <pre><code>results/trajectories/\n\u251c\u2500\u2500 GTEX-OIZH-0626_run0.json\n\u251c\u2500\u2500 GTEX-ABCD-1234_run0.json\n\u2514\u2500\u2500 ...\n</code></pre> <p>These files contain the raw <code>Trajectory</code> (no per-run <code>total_cost</code> metadata). The visualizer will use <code>final_answer</code> if present.</p> <p>Visualize individual items:</p> <pre><code>giant visualize results/trajectories/GTEX-OIZH-0626_run0.json\n</code></pre>"},{"location":"guides/visualizing-trajectories/#batch-script","title":"Batch Script","text":"<pre><code># Visualize all trajectories\nfor f in results/trajectories/*.json; do\n    giant visualize \"$f\" --no-open -o \"${f%.json}.html\"\ndone\n</code></pre>"},{"location":"guides/visualizing-trajectories/#programmatic-access","title":"Programmatic Access","text":"<pre><code>from giant.cli.visualizer import create_trajectory_html\nfrom pathlib import Path\n\nhtml_path = create_trajectory_html(\n    trajectory_path=Path(\"trajectory.json\"),\n    output_path=Path(\"output.html\"),\n    open_browser=False,\n)\nprint(f\"Saved to: {html_path}\")\n</code></pre>"},{"location":"guides/visualizing-trajectories/#understanding-the-visualization","title":"Understanding the Visualization","text":""},{"location":"guides/visualizing-trajectories/#successful-navigation","title":"Successful Navigation","text":"<p>A good trajectory shows:</p> <ol> <li>Purposeful exploration - Model zooms into relevant regions</li> <li>Progressive refinement - Each step adds information</li> <li>Clear reasoning - Explanations reference visual features</li> <li>Confident answer - Final answer with supporting evidence</li> </ol>"},{"location":"guides/visualizing-trajectories/#common-patterns","title":"Common Patterns","text":"Pattern Meaning Few steps, quick answer Obvious case or thumbnail sufficient Many steps, gradual zoom Complex case requiring detail Multiple regions Model sampling different areas Error retries Invalid coordinates corrected"},{"location":"guides/visualizing-trajectories/#failure-modes","title":"Failure Modes","text":"Pattern Issue Random exploration Model not understanding task Repeated same region Model stuck in loop No answer at limit Insufficient evidence gathered Invalid coordinates Model confused by axis guides"},{"location":"guides/visualizing-trajectories/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/visualizing-trajectories/#trajectory-file-not-found","title":"\"Trajectory file not found\"","text":"<p>Ensure you ran inference with <code>--output</code>:</p> <pre><code>giant run slide.svs -q \"?\" -o trajectory.json\n</code></pre>"},{"location":"guides/visualizing-trajectories/#missing-images-in-visualization","title":"\"Missing images in visualization\"","text":"<p>The trajectory must include <code>thumbnail_base64</code> and per-turn <code>image_base64</code>. Older trajectories may lack these fields.</p>"},{"location":"guides/visualizing-trajectories/#browser-doesnt-open","title":"\"Browser doesn't open\"","text":"<p>Use explicit browser:</p> <pre><code>giant visualize traj.json --no-open -o viz.html\nopen viz.html  # macOS\nxdg-open viz.html  # Linux\n</code></pre>"},{"location":"guides/visualizing-trajectories/#next-steps","title":"Next Steps","text":"<ul> <li>Running Inference - Generate trajectories</li> <li>Algorithm - Navigation algorithm</li> <li>Benchmark Results - Example trajectories</li> </ul>"},{"location":"models/model-registry/","title":"GIANT Model Registry","text":"<p>Single Source of Truth for LLM model identifiers used in GIANT</p> <p>This diverges from the original paper to use current frontier models (Dec 2025).</p>"},{"location":"models/model-registry/#approved-models","title":"Approved Models","text":"<p>These are the only model IDs allowed by <code>src/giant/llm/model_registry.py</code>.</p> Provider API Model ID Runtime Support Notes OpenAI <code>gpt-5.2</code> Supported Default OpenAI model Anthropic <code>claude-sonnet-4-5-20250929</code> Supported Default Anthropic model Google <code>gemini-3-pro-preview</code> Planned Model ID is reserved; Google/Gemini provider is not implemented yet"},{"location":"models/model-registry/#pricing-usd-per-1m-tokens","title":"Pricing (USD per 1M tokens)","text":"Model Input Output Image Cost <code>claude-sonnet-4-5-20250929</code> $3.00 $15.00 Pixel-based (~$0.00048/1K px) <code>gemini-3-pro-preview</code> $2.00 $12.00 Included in token count <code>gpt-5.2</code> $1.75 $14.00 Flat-rate per image ($0.00255/image) <p>Notes: - The Anthropic image cost in code is <code>$0.00048 / 1K pixels</code> (i.e., <code>$0.48 / 1M pixels</code>). - Pricing and image-cost formulas are implemented in <code>src/giant/llm/pricing.py</code>.</p>"},{"location":"models/model-registry/#why-these-ids","title":"Why These IDs?","text":"<p>GIANT pins model IDs for reproducibility and to ensure pricing is known. If you need to update pricing or supported models, update <code>docs/models/model-registry.md</code> and <code>src/giant/llm/pricing.py</code>, and keep <code>src/giant/llm/model_registry.py</code> aligned.</p>"},{"location":"models/model-registry/#code-usage","title":"Code Usage","text":"<pre><code>from giant.llm import create_provider\nfrom giant.llm.model_registry import validate_model_id\n\n# OpenAI\nprovider = create_provider(\"openai\", model=\"gpt-5.2\")\n\n# Anthropic\nprovider = create_provider(\"anthropic\", model=\"claude-sonnet-4-5-20250929\")\n\n# Google/Gemini (reserved model ID; provider not implemented)\nvalidate_model_id(\"gemini-3-pro-preview\", provider=\"google\")  # OK\n</code></pre>"},{"location":"models/model-registry/#sources","title":"Sources","text":"<ul> <li>Anthropic Claude Sonnet 4.5</li> <li>Google Gemini 3 Pro</li> <li>OpenAI GPT-5.2</li> </ul>"},{"location":"prompts/prompt-design/","title":"GIANT Prompt Design","text":"<p>Status: Paper-derived (pending Supplementary Material verification)</p> <p>This document captures all prompt requirements extractable from the GIANT paper. When the authors release official prompts, we will compare and update.</p> <p>Enhancement Note: Section \"2025 Pathology VLM Best Practices\" contains domain enhancements beyond the paper. These are clearly marked and separable.</p>"},{"location":"prompts/prompt-design/#paper-evidence-summary","title":"Paper Evidence Summary","text":"Requirement Source Line Confidence Crop budget communicated Algorithm 1 156 High Level-0 coordinate system Sec 4.1 134 High Axis guides explained Sec 4.1 134 High Output format (x, y, w, h) Algorithm 1 159 High Final answer enforcement Fig 5 caption 200 High Reasoning per step Algorithm 1 159 High Thumbnail size 1024px Sec 4.2.1 183 High"},{"location":"prompts/prompt-design/#required-prompt-components-high-confidence","title":"Required Prompt Components (High Confidence)","text":""},{"location":"prompts/prompt-design/#1-crop-budget","title":"1. Crop Budget","text":"<p>Evidence (Algorithm 1, line 156): <pre><code>P0 = {q, nav instructions + \"at most T-1 crops\"};\n</code></pre></p> <p>The initial prompt must tell the model how many crops it can make.</p> <p>Implementation: <pre><code>You have at most {max_steps - 1} crops to explore before providing your final answer.\n</code></pre></p>"},{"location":"prompts/prompt-design/#2-level-0-coordinate-system","title":"2. Level-0 Coordinate System","text":"<p>Evidence (Section 4.1, line 134):</p> <p>\"To orient the model, the thumbnail is overlaid with four evenly spaced axis guides along each dimension, labeled with absolute level-0 pixel coordinates.\"</p> <p>The model must understand: - Coordinates are absolute (Level-0 = full resolution) - Axis guides show these coordinates visually - All bounding boxes use this system</p> <p>Implementation: <pre><code>The image has AXIS GUIDES overlaid - red lines labeled with ABSOLUTE LEVEL-0 PIXEL COORDINATES.\nAll coordinates you output must use this Level-0 system (the slide's native resolution).\n</code></pre></p>"},{"location":"prompts/prompt-design/#3-bounding-box-output-format","title":"3. Bounding Box Output Format","text":"<p>Evidence (Algorithm 1, line 159): <pre><code>(rt, at) \u2190 LMM(C) ; // at = (x, y, w, h)\n</code></pre></p> <p>Each step produces reasoning + action, where action is a 4-tuple.</p> <p>Implementation (this repo):</p> <p>The paper does not specify a serialization format (e.g., JSON) for <code>(x, y, w, h)</code>. In this repo, response structure is enforced externally by provider integrations (OpenAI structured output / Anthropic tool use) using <code>StepResponse</code>:</p> <pre><code>{\n  \"reasoning\": \"I observe ...\",\n  \"action\": {\n    \"action_type\": \"crop\",\n    \"x\": 10000,\n    \"y\": 20000,\n    \"width\": 5000,\n    \"height\": 5000\n  }\n}\n</code></pre> <p>See: <code>src/giant/llm/protocol.py</code> (<code>StepResponse</code>, <code>BoundingBoxAction</code>, <code>FinalAnswerAction</code>).</p>"},{"location":"prompts/prompt-design/#4-final-answer-enforcement","title":"4. Final Answer Enforcement","text":"<p>Evidence (Figure 5 caption, line 200):</p> <p>\"We use a system prompt to enforce that the model provide its final response after a specific number of iterations, marking a trial incorrect if the model exceeds this limit after 3 retries.\"</p> <p>The prompt must explicitly require the model to answer on the final step.</p> <p>Implementation: <pre><code>On step {max_steps}, you MUST provide your final answer using the `answer` action.\n</code></pre></p>"},{"location":"prompts/prompt-design/#5-two-action-types","title":"5. Two Action Types","text":"<p>Evidence (Algorithm 1, lines 151-160; Section 4.1, line 140):</p> <p>Algorithm 1 defines repeated crop selection via bounding boxes, and returning a final answer <code>y\u02c6</code> once navigation ends. Section 4.1 explicitly mentions navigation repeats \"until a step limit T or early stop\".</p> <p>Evidence (Section 4.1, line 140):</p> <p>\"The environment returns the next image It+1 = CropRegion(W, at, S), repeating until a step limit T or early stop.\"</p> <p>This repo represents this contract with two action modes: - <code>crop(x, y, w, h)</code>: Continue navigation by selecting the next region of interest. - <code>answer(text)</code>: Terminate navigation and provide the final answer.</p> <p>Implementation: <pre><code>Your available actions:\n1. crop(x, y, width, height) - Zoom into a region for more detail\n2. answer(text) - Provide your final answer to the question\n</code></pre></p>"},{"location":"prompts/prompt-design/#inferred-components-medium-confidence","title":"Inferred Components (Medium Confidence)","text":"<p>These are not explicitly stated but are strongly implied by the paper's methodology.</p>"},{"location":"prompts/prompt-design/#6-pathology-domain-context","title":"6. Pathology Domain Context","text":"<p>The paper tests on pathology slides. The model should know it's analyzing tissue.</p> <p>Implementation: <pre><code>You are analyzing a Whole Slide Image (WSI) - a gigapixel pathology slide.\n</code></pre></p>"},{"location":"prompts/prompt-design/#7-thumbnail-vs-crop-distinction","title":"7. Thumbnail vs Crop Distinction","text":"<p>Figure 4 shows the agent receives different image types: - Initial: Low-resolution thumbnail (blurry) - After crop: Higher-resolution region</p> <p>Implementation: <pre><code>The initial view is a low-resolution thumbnail. Cellular details require zooming in.\n</code></pre></p>"},{"location":"prompts/prompt-design/#8-iterative-refinement","title":"8. Iterative Refinement","text":"<p>Algorithm 1 shows a loop: observe \u2192 reason \u2192 act \u2192 observe...</p> <p>Implementation: <pre><code>At each step:\n1. Analyze the current image\n2. Explain your reasoning\n3. Choose to crop (for more detail) or answer (if sufficient evidence)\n</code></pre></p>"},{"location":"prompts/prompt-design/#2025-pathology-vlm-best-practices-domain-enhancement","title":"2025 Pathology VLM Best Practices (Domain Enhancement)","text":"<p>Note: This section contains enhancements derived from Dec 2025 literature review. These are NOT from the GIANT paper and can be removed for strict paper reproduction.</p>"},{"location":"prompts/prompt-design/#literature-sources","title":"Literature Sources","text":"<p>Derived from literature search: \"whole slide image LLM prompting 2025\", \"GPT-4V medical image navigation\"</p> <ol> <li>Anatomical Precision &amp; Domain Vocabulary</li> <li>Insight: Generic \"describe the image\" prompts perform poorly.</li> <li> <p>Action: Use specific pathology terms (e.g., \"stroma\", \"nuclei\", \"architecture\", \"mitotic figures\") in the system prompt to prime the model's vocabulary.</p> </li> <li> <p>Hierarchical Observation (Multi-Scale)</p> </li> <li>Insight: Pathologists scan low-power (architecture) before high-power (cellular details).</li> <li> <p>Action: Explicitly instruct the model to follow this \"Architecture -&gt; Cellular\" observation flow.</p> </li> <li> <p>Visual Anchoring</p> </li> <li>Insight: Models hallucinate less when forced to reference specific coordinates or visual markers.</li> <li> <p>Action: Reinforce the \"Axis Guides\" instruction and ask the model to cite coordinates in its reasoning.</p> </li> <li> <p>Role &amp; Goal Specificity</p> </li> <li>Insight: \"You are a pathologist\" is good, but \"You are a pathologist diagnosing cancer grade\" is better.</li> <li>Action: Ensure the prompt adapts to the specific task type if known (e.g., QA vs Diagnosis).</li> </ol>"},{"location":"prompts/prompt-design/#gap-analysis-current-implementation","title":"Gap Analysis (Current Implementation)","text":"Current State Gap Fix Applied Generic \"Analyze image\" Lacks domain specificity Added \"Scan for architectural patterns, then cellular details\" \"Provide reasoning\" Unstructured thought process Enforced \"Observation -&gt; Reasoning -&gt; Action\" structure \"Low-res thumbnail\" Doesn't explain why zoom is needed Explained \"Low-res = Architecture only; High-res = Cellular\" No coordinate citing Hallucination risk Ask model to \"Reference coordinates in reasoning\""},{"location":"prompts/prompt-design/#what-we-cannot-determine","title":"What We Cannot Determine","text":"<p>Without the Supplementary Material, we cannot verify:</p> <ol> <li>Exact wording - The precise phrases used</li> <li>Provider differences - Whether OpenAI/Anthropic prompts differ</li> <li>Additional constraints - Any rules not mentioned in the main text</li> <li>Retry instructions - How the 3-retry policy is communicated</li> <li>Question formatting - How the user's question is presented</li> </ol>"},{"location":"prompts/prompt-design/#current-implementation","title":"Current Implementation","text":"<p>See: <code>src/giant/prompts/templates.py</code></p> <p>Our templates implement all high-confidence requirements: - Crop budget (Step X of Y, N crops remaining) - Level-0 coordinate system (axis guides explanation) - Bounding box format (crop action with x, y, width, height) - Final answer enforcement (MUST use answer on final step) - Two action types (crop, answer)</p> <p>Plus domain enhancements (currently always included; remove from templates for strict reproduction): - Hierarchical analysis workflow - Pathology-specific vocabulary - Coordinate referencing in reasoning</p>"},{"location":"prompts/prompt-design/#verification-plan","title":"Verification Plan","text":"<p>When Supplementary Material becomes available:</p> <ol> <li>Compare official prompts to our templates</li> <li>Document any differences</li> <li>Update templates to match (or document intentional divergence)</li> <li>Add regression tests for paper-required invariants</li> <li>Decide whether to keep or remove domain enhancements</li> </ol>"},{"location":"prompts/prompt-design/#references","title":"References","text":"<ul> <li>GIANT paper: <code>_literature/markdown/giant/giant.md</code></li> <li>Algorithm 1: lines 144-161</li> <li>Section 4.1 (Axis guides): line 134</li> <li>Figure 5 caption (step enforcement): line 200</li> <li>Baseline thumbnail size: line 183</li> <li>Current implementation: <code>src/giant/prompts/templates.py</code></li> <li>Bug tracking: <code>docs/bugs/BUG-020-placeholder-prompts.md</code></li> <li>2025 Literature: Web search \"whole slide image LLM prompting 2025\"</li> </ul>"},{"location":"reference/cli/","title":"CLI Reference","text":"<p>Complete reference for the GIANT command-line interface.</p>"},{"location":"reference/cli/#synopsis","title":"Synopsis","text":"<pre><code>giant &lt;command&gt; [options]\n</code></pre>"},{"location":"reference/cli/#global-commands","title":"Global Commands","text":""},{"location":"reference/cli/#giant-version","title":"<code>giant version</code>","text":"<p>Show version information.</p> <pre><code>giant version\ngiant version --json\n</code></pre> Option Description <code>--json</code> Output as JSON"},{"location":"reference/cli/#giant-run","title":"<code>giant run</code>","text":"<p>Run GIANT on a single whole-slide image.</p> <pre><code>giant run &lt;wsi_path&gt; -q &lt;question&gt; [options]\n</code></pre>"},{"location":"reference/cli/#arguments","title":"Arguments","text":"Argument Required Description <code>WSI_PATH</code> Yes Path to WSI file (<code>.svs</code>, <code>.tiff</code>, <code>.ndpi</code>)"},{"location":"reference/cli/#options","title":"Options","text":"Option Default Description <code>-q, --question</code> Required Question to answer about the slide <code>-m, --mode</code> <code>giant</code> Evaluation mode (<code>giant</code>, <code>thumbnail</code>, <code>patch</code>) <code>-p, --provider</code> <code>openai</code> LLM provider (<code>openai</code>, <code>anthropic</code>) <code>--model</code> <code>gpt-5.2</code> Model ID (see model-registry.md) <code>-T, --max-steps</code> <code>20</code> Maximum navigation steps <code>--strict-font-check/--no-strict-font-check</code> <code>--no-strict-font-check</code> Fail if TrueType fonts unavailable <code>-r, --runs</code> <code>1</code> Number of runs for majority voting <code>--budget-usd</code> <code>0</code> (disabled) Cost limit in USD <code>-o, --output</code> None Save trajectory to JSON file <code>-v, --verbose</code> 0 Verbosity level (<code>-v</code>, <code>-vv</code>, <code>-vvv</code>) <code>--json</code> False Output as JSON"},{"location":"reference/cli/#examples","title":"Examples","text":"<pre><code># Basic usage\ngiant run slide.svs -q \"What tissue is this?\"\n\n# Use Anthropic with cost limit\ngiant run slide.svs -q \"Question?\" --provider anthropic --budget-usd 0.50\n\n# Multiple runs with trajectory output\ngiant run slide.svs -q \"Question?\" --runs 3 -o trajectory.json -v\n</code></pre>"},{"location":"reference/cli/#exit-codes","title":"Exit Codes","text":"Code Meaning <code>0</code> Success <code>1</code> Failure"},{"location":"reference/cli/#giant-benchmark","title":"<code>giant benchmark</code>","text":"<p>Run the full benchmark suite on a dataset.</p> <pre><code>giant benchmark &lt;dataset&gt; [options]\n</code></pre>"},{"location":"reference/cli/#arguments_1","title":"Arguments","text":"Argument Values Description <code>DATASET</code> <code>tcga</code>, <code>panda</code>, <code>gtex</code>, <code>tcga_expert_vqa</code>, <code>tcga_slidebench</code> Dataset name"},{"location":"reference/cli/#options_1","title":"Options","text":"Option Default Description <code>--csv-path</code> <code>data/multipathqa/MultiPathQA.csv</code> Path to benchmark CSV <code>--wsi-root</code> <code>data/wsi</code> Root directory for WSI files <code>-o, --output-dir</code> <code>results</code> Output directory for results <code>-m, --mode</code> <code>giant</code> Evaluation mode <code>-p, --provider</code> <code>openai</code> LLM provider <code>--model</code> <code>gpt-5.2</code> Model ID <code>-T, --max-steps</code> <code>20</code> Max steps per item <code>--strict-font-check/--no-strict-font-check</code> <code>--no-strict-font-check</code> Font check <code>-r, --runs</code> <code>1</code> Runs per item <code>-c, --concurrency</code> <code>4</code> Max concurrent API calls <code>--budget-usd</code> <code>0</code> (disabled) Total cost budget <code>--max-items</code> <code>0</code> (all) Max items to process <code>--skip-missing/--no-skip-missing</code> <code>--skip-missing</code> Skip missing WSIs <code>--resume/--no-resume</code> <code>--resume</code> Resume from checkpoint <code>-v, --verbose</code> 0 Verbosity level <code>--json</code> False Output as JSON"},{"location":"reference/cli/#examples_1","title":"Examples","text":"<pre><code># Full GTEx benchmark\ngiant benchmark gtex --provider openai -v\n\n# Quick test (5 items)\ngiant benchmark tcga --max-items 5 -v\n\n# High concurrency with resume\ngiant benchmark panda --concurrency 8 --resume -v\n\n# Cost-limited run\ngiant benchmark gtex --budget-usd 10.00 -v\n</code></pre>"},{"location":"reference/cli/#giant-download","title":"<code>giant download</code>","text":"<p>Download benchmark datasets from HuggingFace.</p> <pre><code>giant download [dataset] [options]\n</code></pre>"},{"location":"reference/cli/#arguments_2","title":"Arguments","text":"Argument Default Description <code>DATASET</code> <code>multipathqa</code> Dataset to download"},{"location":"reference/cli/#options_2","title":"Options","text":"Option Default Description <code>-o, --output-dir</code> <code>data</code> Output directory <code>--force</code> False Re-download if exists <code>-v, --verbose</code> 0 Verbosity level <code>--json</code> False Output as JSON"},{"location":"reference/cli/#examples_2","title":"Examples","text":"<pre><code># Download MultiPathQA metadata\ngiant download multipathqa\n\n# Force re-download\ngiant download multipathqa --force\n</code></pre>"},{"location":"reference/cli/#giant-check-data","title":"<code>giant check-data</code>","text":"<p>Validate that WSI files for a benchmark exist locally.</p> <pre><code>giant check-data &lt;dataset&gt; [options]\n</code></pre>"},{"location":"reference/cli/#arguments_3","title":"Arguments","text":"Argument Values Description <code>DATASET</code> <code>tcga</code>, <code>panda</code>, <code>gtex</code>, <code>tcga_expert_vqa</code>, <code>tcga_slidebench</code> Dataset name"},{"location":"reference/cli/#options_3","title":"Options","text":"Option Default Description <code>--csv-path</code> <code>data/multipathqa/MultiPathQA.csv</code> Path to benchmark CSV <code>--wsi-root</code> <code>data/wsi</code> Root directory for WSIs <code>-v, --verbose</code> 0 Verbosity (shows missing files) <code>--json</code> False Output as JSON"},{"location":"reference/cli/#examples_3","title":"Examples","text":"<pre><code># Check GTEx data\ngiant check-data gtex\n\n# Verbose output showing missing files\ngiant check-data tcga -v\n\n# JSON output\ngiant check-data panda --json\n</code></pre>"},{"location":"reference/cli/#output","title":"Output","text":"<pre><code>All WSIs present for gtex: 191/191 under data/wsi\n</code></pre>"},{"location":"reference/cli/#giant-visualize","title":"<code>giant visualize</code>","text":"<p>Generate interactive visualization of navigation trajectory.</p> <pre><code>giant visualize &lt;trajectory_path&gt; [options]\n</code></pre>"},{"location":"reference/cli/#arguments_4","title":"Arguments","text":"Argument Required Description <code>TRAJECTORY_PATH</code> Yes Path to trajectory JSON file"},{"location":"reference/cli/#options_4","title":"Options","text":"Option Default Description <code>-o, --output</code> Auto-generated Output HTML file path <code>--open/--no-open</code> <code>--open</code> Open in browser <code>-v, --verbose</code> 0 Verbosity level <code>--json</code> False Output as JSON"},{"location":"reference/cli/#examples_4","title":"Examples","text":"<pre><code># Visualize and open in browser\ngiant visualize trajectory.json\n\n# Save without opening\ngiant visualize trajectory.json --no-open -o output.html\n</code></pre>"},{"location":"reference/cli/#environment-variables","title":"Environment Variables","text":"Variable Required For Description <code>OPENAI_API_KEY</code> <code>--provider openai</code> OpenAI API key <code>ANTHROPIC_API_KEY</code> <code>--provider anthropic</code> Anthropic API key"},{"location":"reference/cli/#verbosity-levels","title":"Verbosity Levels","text":"Level Flag Output 0 (none) Errors and results only 1 <code>-v</code> Info messages 2 <code>-vv</code> Debug messages 3+ <code>-vvv</code> Trace messages"},{"location":"reference/cli/#see-also","title":"See Also","text":"<ul> <li>Running Inference</li> <li>Running Benchmarks</li> <li>Configuration</li> </ul>"},{"location":"reference/configuration/","title":"Configuration Reference","text":"<p>This page documents all configuration options for GIANT.</p>"},{"location":"reference/configuration/#environment-variables","title":"Environment Variables","text":""},{"location":"reference/configuration/#api-keys","title":"API Keys","text":"Variable Required Description <code>OPENAI_API_KEY</code> For OpenAI OpenAI API key (<code>sk-...</code>) <code>ANTHROPIC_API_KEY</code> For Anthropic Anthropic API key (<code>sk-ant-...</code>) <code>GOOGLE_API_KEY</code> Reserved Used only by a future Google/Gemini provider (not wired into the CLI yet) <code>HUGGINGFACE_TOKEN</code> Optional HuggingFace token (only needed for gated downloads)"},{"location":"reference/configuration/#example-env","title":"Example <code>.env</code>","text":"<pre><code># .env - never commit this file!\nOPENAI_API_KEY=sk-proj-abc123...\nANTHROPIC_API_KEY=sk-ant-api03-xyz789...\n</code></pre> <p>GIANT reads <code>.env</code> automatically via pydantic-settings; no need to <code>source</code> it. You can also export variables manually in your shell if preferred.</p>"},{"location":"reference/configuration/#agent-configuration","title":"Agent Configuration","text":""},{"location":"reference/configuration/#agentconfig","title":"<code>AgentConfig</code>","text":"<p>Configuration for <code>GIANTAgent</code> behavior.</p> <pre><code>from giant.agent import AgentConfig\n\nconfig = AgentConfig(\n    max_steps=20,           # Maximum navigation steps (T in paper)\n    max_retries=3,          # Max consecutive errors before termination\n    budget_usd=None,        # Optional cost limit\n    thumbnail_size=1024,    # Initial thumbnail size\n    force_answer_retries=3, # Retries for forcing answer at max steps\n    strict_font_check=False # Fail if axis fonts unavailable\n)\n</code></pre> Parameter Type Default Description <code>max_steps</code> <code>int</code> <code>20</code> Maximum navigation iterations <code>max_retries</code> <code>int</code> <code>3</code> Max consecutive errors <code>budget_usd</code> <code>float | None</code> <code>None</code> Cost limit (None = unlimited) <code>thumbnail_size</code> <code>int</code> <code>1024</code> Thumbnail max dimension <code>force_answer_retries</code> <code>int</code> <code>3</code> Answer enforcement retries <code>strict_font_check</code> <code>bool</code> <code>False</code> Require TrueType fonts"},{"location":"reference/configuration/#cli-mapping","title":"CLI Mapping","text":"CLI Option AgentConfig Parameter <code>--max-steps</code> / <code>-T</code> <code>max_steps</code> <code>--budget-usd</code> <code>budget_usd</code> <code>--strict-font-check</code> <code>strict_font_check</code>"},{"location":"reference/configuration/#evaluation-configuration","title":"Evaluation Configuration","text":""},{"location":"reference/configuration/#evaluationconfig","title":"<code>EvaluationConfig</code>","text":"<p>Configuration for benchmark runs.</p> <pre><code>from giant.eval.runner import EvaluationConfig\n\nconfig = EvaluationConfig(\n    mode=\"giant\",\n    max_steps=20,\n    runs_per_item=1,\n    max_concurrent=4,\n    max_items=None,\n    skip_missing_wsis=False,\n    budget_usd=None,\n)\n</code></pre> Parameter Type Default Description <code>mode</code> <code>str</code> <code>\"giant\"</code> <code>\"giant\"</code>, <code>\"thumbnail\"</code>, or <code>\"patch\"</code> <code>max_steps</code> <code>int</code> <code>20</code> Steps per item <code>runs_per_item</code> <code>int</code> <code>1</code> Majority voting runs per item <code>max_concurrent</code> <code>int</code> <code>4</code> Concurrent API calls <code>max_items</code> <code>int \\| None</code> <code>None</code> Optional cap on evaluated items <code>skip_missing_wsis</code> <code>bool</code> <code>False</code> Skip missing WSI files under <code>--wsi-root</code> <code>budget_usd</code> <code>float \\| None</code> <code>None</code> Optional total budget for a run"},{"location":"reference/configuration/#provider-configuration","title":"Provider Configuration","text":""},{"location":"reference/configuration/#target-sizes","title":"Target Sizes","text":"Provider Target Size Rationale OpenAI 1000px Higher resolution Anthropic 500px Cost-optimized"},{"location":"reference/configuration/#model-defaults","title":"Model Defaults","text":"Provider Default Model OpenAI <code>gpt-5.2</code> Anthropic <code>claude-sonnet-4-5-20250929</code> <p><code>gemini-3-pro-preview</code> is present in the model registry for future work, but the Google/Gemini provider is not implemented in the CLI yet.</p>"},{"location":"reference/configuration/#overlay-configuration","title":"Overlay Configuration","text":""},{"location":"reference/configuration/#overlaystyle","title":"<code>OverlayStyle</code>","text":"<p>Styling for axis guide overlays.</p> <pre><code>from giant.geometry.overlay import OverlayStyle\n\nstyle = OverlayStyle(\n    line_color=(255, 0, 0, 180),  # RGBA (red, semi-transparent)\n    line_width=2,\n    font_size=12,\n    strict_font_check=False,\n)\n</code></pre> Parameter Type Default Description <code>line_color</code> <code>tuple[int, int, int, int]</code> <code>(255, 0, 0, 180)</code> RGBA color for guide lines <code>line_width</code> <code>int</code> <code>2</code> Line thickness <code>label_color</code> <code>tuple[int, int, int, int]</code> <code>(255, 255, 255, 255)</code> RGBA color for labels <code>font_size</code> <code>int</code> <code>12</code> Label font size <code>label_padding</code> <code>int</code> <code>5</code> Padding from edge for labels <code>num_guides</code> <code>int</code> <code>4</code> Guide lines per axis <code>strict_font_check</code> <code>bool</code> <code>False</code> Require TrueType"},{"location":"reference/configuration/#logging-configuration","title":"Logging Configuration","text":""},{"location":"reference/configuration/#verbosity-levels","title":"Verbosity Levels","text":"Level CLI Python Warning (default) <code>logging.WARNING</code> Info <code>-v</code> <code>logging.INFO</code> Debug <code>-vv</code> <code>logging.DEBUG</code>"},{"location":"reference/configuration/#programmatic-setup","title":"Programmatic Setup","text":"<pre><code>from giant.utils.logging import configure_logging\n\nconfigure_logging(level=\"DEBUG\")\n</code></pre>"},{"location":"reference/configuration/#directory-structure","title":"Directory Structure","text":""},{"location":"reference/configuration/#default-paths","title":"Default Paths","text":"Purpose Default Path CLI Override WSI root <code>data/wsi/</code> <code>--wsi-root</code> Benchmark CSV <code>data/multipathqa/MultiPathQA.csv</code> <code>--csv-path</code> Results <code>results/</code> <code>--output-dir</code> Checkpoints <code>results/checkpoints/</code> (auto) Trajectories <code>results/trajectories/</code> (auto)"},{"location":"reference/configuration/#wsi-subdirectories","title":"WSI Subdirectories","text":"<pre><code>data/wsi/\n\u251c\u2500\u2500 tcga/      # TCGA slides (all 3 benchmarks)\n\u251c\u2500\u2500 gtex/      # GTEx slides\n\u2514\u2500\u2500 panda/     # PANDA slides\n</code></pre>"},{"location":"reference/configuration/#model-registry","title":"Model Registry","text":""},{"location":"reference/configuration/#approved-models","title":"Approved Models","text":"<p>Only these models are allowed at runtime:</p> <pre><code>APPROVED_MODELS = frozenset({\n    \"gpt-5.2\",\n    \"claude-sonnet-4-5-20250929\",\n    \"gemini-3-pro-preview\",\n})\n</code></pre>"},{"location":"reference/configuration/#validation","title":"Validation","text":"<pre><code>from giant.llm.model_registry import validate_model_id\n\nvalidate_model_id(\"gpt-5.2\")  # OK\nvalidate_model_id(\"gpt-4o\")   # Raises ValueError\n</code></pre> <p>See Model Registry for details.</p>"},{"location":"reference/configuration/#pricing","title":"Pricing","text":""},{"location":"reference/configuration/#cost-calculation","title":"Cost Calculation","text":"<p>Costs are calculated using pricing tables:</p> <pre><code>from giant.llm.pricing import calculate_cost\n\ncost = calculate_cost(\n    model=\"gpt-5.2\",\n    prompt_tokens=1000,\n    completion_tokens=500,\n)\n</code></pre>"},{"location":"reference/configuration/#current-pricing-per-1m-tokens","title":"Current Pricing (per 1M tokens)","text":"Model Input Output gpt-5.2 $1.75 $14.00 claude-sonnet-4-5-20250929 $3.00 $15.00 gemini-3-pro-preview $2.00 $12.00"},{"location":"reference/configuration/#see-also","title":"See Also","text":"<ul> <li>CLI Reference</li> <li>Model Registry</li> <li>Architecture</li> </ul>"},{"location":"reference/project-structure/","title":"Project Structure","text":"<p>This page documents the organization of the GIANT codebase.</p>"},{"location":"reference/project-structure/#repository-layout","title":"Repository Layout","text":"<pre><code>gigapixel-goblin/\n\u251c\u2500\u2500 src/giant/              # Main source code\n\u251c\u2500\u2500 tests/                  # Test suite\n\u251c\u2500\u2500 docs/                   # Documentation (MkDocs)\n\u251c\u2500\u2500 data/                   # Data files (not in git)\n\u251c\u2500\u2500 results/                # Benchmark results (not in git)\n\u251c\u2500\u2500 scripts/                # Utility scripts\n\u251c\u2500\u2500 pyproject.toml          # Project configuration\n\u251c\u2500\u2500 mkdocs.yml              # Documentation config\n\u251c\u2500\u2500 CLAUDE.md               # AI assistant instructions\n\u2514\u2500\u2500 README.md               # Project readme\n</code></pre>"},{"location":"reference/project-structure/#source-code-srcgiant","title":"Source Code (<code>src/giant/</code>)","text":""},{"location":"reference/project-structure/#module-overview","title":"Module Overview","text":"<pre><code>src/giant/\n\u251c\u2500\u2500 __init__.py             # Package root, version\n\u251c\u2500\u2500 config.py               # Global configuration\n\u2502\n\u251c\u2500\u2500 agent/                  # Agent orchestration\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 runner.py           # GIANTAgent class\n\u2502   \u251c\u2500\u2500 context.py          # ContextManager\n\u2502   \u2514\u2500\u2500 trajectory.py       # Trajectory recording\n\u2502\n\u251c\u2500\u2500 llm/                    # LLM abstraction\n\u2502   \u251c\u2500\u2500 __init__.py         # create_provider factory\n\u2502   \u251c\u2500\u2500 protocol.py         # LLMProvider Protocol\n\u2502   \u251c\u2500\u2500 openai_client.py    # OpenAI implementation\n\u2502   \u251c\u2500\u2500 anthropic_client.py # Anthropic implementation\n\u2502   \u251c\u2500\u2500 converters.py       # Message format conversion\n\u2502   \u251c\u2500\u2500 model_registry.py   # Approved models\n\u2502   \u251c\u2500\u2500 pricing.py          # Cost calculation\n\u2502   \u251c\u2500\u2500 schemas.py          # JSON schemas\n\u2502   \u2514\u2500\u2500 circuit_breaker.py  # Failure protection\n\u2502\n\u251c\u2500\u2500 wsi/                    # WSI I/O\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 reader.py           # WSIReader class\n\u2502   \u251c\u2500\u2500 types.py            # WSI metadata types\n\u2502   \u2514\u2500\u2500 exceptions.py       # WSI errors\n\u2502\n\u251c\u2500\u2500 core/                   # Core processing\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 crop_engine.py      # CropEngine class\n\u2502   \u251c\u2500\u2500 level_selector.py   # Pyramid level selection\n\u2502   \u2514\u2500\u2500 baselines.py        # Thumbnail/patch baselines\n\u2502\n\u251c\u2500\u2500 geometry/               # Coordinates &amp; overlays\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 primitives.py       # Region, Point, Size\n\u2502   \u251c\u2500\u2500 overlay.py          # Axis guide generation\n\u2502   \u251c\u2500\u2500 transforms.py       # Coordinate transforms\n\u2502   \u2514\u2500\u2500 validators.py       # Bounds validation\n\u2502\n\u251c\u2500\u2500 prompts/                # Prompt engineering\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 builder.py          # PromptBuilder class\n\u2502   \u2514\u2500\u2500 templates.py        # Prompt templates\n\u2502\n\u251c\u2500\u2500 eval/                   # Evaluation framework\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 runner.py           # BenchmarkRunner class\n\u2502   \u251c\u2500\u2500 metrics.py          # Accuracy calculations\n\u2502   \u251c\u2500\u2500 answer_extraction.py# Parse model answers\n\u2502   \u251c\u2500\u2500 wsi_resolver.py     # Resolve WSI paths\n\u2502   \u2514\u2500\u2500 resumable.py        # Checkpoint/resume\n\u2502\n\u251c\u2500\u2500 vision/                 # Computer vision\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 segmentation.py     # Tissue segmentation\n\u2502   \u251c\u2500\u2500 sampler.py          # Patch sampling\n\u2502   \u251c\u2500\u2500 aggregation.py      # Feature aggregation\n\u2502   \u2514\u2500\u2500 constants.py        # Vision constants\n\u2502\n\u251c\u2500\u2500 data/                   # Data utilities\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 download.py         # Dataset download\n\u2502   \u251c\u2500\u2500 schemas.py          # Data schemas\n\u2502   \u2514\u2500\u2500 tcga.py             # TCGA-specific helpers\n\u2502\n\u251c\u2500\u2500 cli/                    # Command-line interface\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 main.py             # Typer app\n\u2502   \u251c\u2500\u2500 runners.py          # Command implementations\n\u2502   \u2514\u2500\u2500 visualizer.py       # Trajectory visualization\n\u2502\n\u2514\u2500\u2500 utils/                  # Utilities\n    \u251c\u2500\u2500 __init__.py\n    \u2514\u2500\u2500 logging.py          # Logging configuration\n</code></pre>"},{"location":"reference/project-structure/#key-classes","title":"Key Classes","text":""},{"location":"reference/project-structure/#agent-layer","title":"Agent Layer","text":"Class File Purpose <code>GIANTAgent</code> <code>agent/runner.py</code> Main navigation loop <code>ContextManager</code> <code>agent/context.py</code> Conversation state <code>Trajectory</code> <code>agent/trajectory.py</code> Step recording"},{"location":"reference/project-structure/#llm-layer","title":"LLM Layer","text":"Class File Purpose <code>LLMProvider</code> <code>llm/protocol.py</code> Provider interface <code>OpenAIProvider</code> <code>llm/openai_client.py</code> OpenAI Responses API <code>AnthropicProvider</code> <code>llm/anthropic_client.py</code> Anthropic Messages API <code>Message</code> <code>llm/protocol.py</code> Message format <code>StepResponse</code> <code>llm/protocol.py</code> LLM output format"},{"location":"reference/project-structure/#wsi-layer","title":"WSI Layer","text":"Class File Purpose <code>WSIReader</code> <code>wsi/reader.py</code> OpenSlide wrapper <code>CropEngine</code> <code>core/crop_engine.py</code> Region extraction <code>PyramidLevelSelector</code> <code>core/level_selector.py</code> Pyramid level selection"},{"location":"reference/project-structure/#geometry-layer","title":"Geometry Layer","text":"Class File Purpose <code>Region</code> <code>geometry/primitives.py</code> Bounding box <code>Point</code> <code>geometry/primitives.py</code> Coordinate point <code>Size</code> <code>geometry/primitives.py</code> Dimensions <code>AxisGuideGenerator</code> <code>geometry/overlay.py</code> Axis labels"},{"location":"reference/project-structure/#evaluation-layer","title":"Evaluation Layer","text":"Class File Purpose <code>BenchmarkRunner</code> <code>eval/runner.py</code> Benchmark orchestration <code>EvaluationConfig</code> <code>eval/runner.py</code> Run configuration"},{"location":"reference/project-structure/#tests-tests","title":"Tests (<code>tests/</code>)","text":"<pre><code>tests/\n\u251c\u2500\u2500 conftest.py             # Shared fixtures\n\u251c\u2500\u2500 unit/                   # Unit tests (fast, mocked)\n\u2502   \u251c\u2500\u2500 agent/\n\u2502   \u251c\u2500\u2500 cli/\n\u2502   \u251c\u2500\u2500 core/\n\u2502   \u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 eval/\n\u2502   \u251c\u2500\u2500 geometry/\n\u2502   \u251c\u2500\u2500 llm/\n\u2502   \u251c\u2500\u2500 prompts/\n\u2502   \u251c\u2500\u2500 vision/\n\u2502   \u2514\u2500\u2500 wsi/\n\u2514\u2500\u2500 integration/            # Integration tests (real I/O)\n    \u251c\u2500\u2500 cli/\n    \u251c\u2500\u2500 llm/\n    \u2514\u2500\u2500 wsi/\n</code></pre>"},{"location":"reference/project-structure/#test-markers","title":"Test Markers","text":"Marker Description <code>@pytest.mark.cost</code> Requires live API (costs money) <code>@pytest.mark.integration</code> Requires real WSI files <code>@pytest.mark.live</code> Requires live external service"},{"location":"reference/project-structure/#documentation-docs","title":"Documentation (<code>docs/</code>)","text":"<pre><code>docs/\n\u251c\u2500\u2500 index.md                # Home page\n\u251c\u2500\u2500 getting-started/        # Tutorials\n\u251c\u2500\u2500 concepts/               # Explanations\n\u251c\u2500\u2500 guides/                 # How-to guides\n\u251c\u2500\u2500 reference/              # Reference docs\n\u251c\u2500\u2500 development/            # Contributing\n\u251c\u2500\u2500 specs/                  # Implementation specs\n\u251c\u2500\u2500 models/                 # Model registry\n\u251c\u2500\u2500 prompts/                # Prompt design\n\u251c\u2500\u2500 results/                # Benchmark results\n\u251c\u2500\u2500 validation/             # Validation reports\n\u251c\u2500\u2500 bugs/                   # Bug tracking\n\u251c\u2500\u2500 archive/                # Archived bugs\n\u251c\u2500\u2500 brainstorming/          # Research notes\n\u2514\u2500\u2500 data-acquisition.md     # Data download guide\n</code></pre>"},{"location":"reference/project-structure/#data-data","title":"Data (<code>data/</code>)","text":"<pre><code>data/\n\u251c\u2500\u2500 multipathqa/\n\u2502   \u2514\u2500\u2500 MultiPathQA.csv     # Benchmark metadata\n\u251c\u2500\u2500 wsi/\n\u2502   \u251c\u2500\u2500 tcga/               # TCGA slides\n\u2502   \u251c\u2500\u2500 gtex/               # GTEx slides\n\u2502   \u251c\u2500\u2500 panda/              # PANDA slides\n\u2502   \u251c\u2500\u2500 tcga_files.txt      # TCGA file list\n\u2502   \u251c\u2500\u2500 gtex_files.txt      # GTEx file list\n\u2502   \u2514\u2500\u2500 panda_files.txt     # PANDA file list\n\u2514\u2500\u2500 test/                   # Test data\n</code></pre>"},{"location":"reference/project-structure/#results-results","title":"Results (<code>results/</code>)","text":"<pre><code>results/\n\u251c\u2500\u2500 *_results.json          # Full benchmark results\n\u251c\u2500\u2500 checkpoints/\n\u2502   \u2514\u2500\u2500 *.checkpoint.json   # Resume state\n\u2514\u2500\u2500 trajectories/\n    \u2514\u2500\u2500 *.json              # Per-item trajectories\n</code></pre>"},{"location":"reference/project-structure/#configuration-files","title":"Configuration Files","text":"File Purpose <code>pyproject.toml</code> Project metadata, dependencies <code>mkdocs.yml</code> Documentation site config <code>.env</code> Environment variables (not in git) <code>.gitignore</code> Git ignore patterns <code>CLAUDE.md</code> AI assistant instructions <code>GEMINI.md</code> AI assistant instructions"},{"location":"reference/project-structure/#import-patterns","title":"Import Patterns","text":""},{"location":"reference/project-structure/#public-api","title":"Public API","text":"<pre><code># Agent\nfrom giant.agent import GIANTAgent, AgentConfig\n\n# LLM\nfrom giant.llm import create_provider, LLMProvider\n\n# Geometry\nfrom giant.geometry import Region, Point, Size\n\n# WSI\nfrom giant.wsi import WSIReader\n\n# Core\nfrom giant.core import CropEngine\n\n# Evaluation\nfrom giant.eval import BenchmarkRunner, EvaluationConfig\n</code></pre>"},{"location":"reference/project-structure/#see-also","title":"See Also","text":"<ul> <li>Architecture</li> <li>Contributing</li> <li>Testing</li> </ul>"},{"location":"results/benchmark-results/","title":"Benchmark Results","text":"<p>This document tracks our MultiPathQA benchmark results and compares them to the published GIANT paper.</p>"},{"location":"results/benchmark-results/#gtex-organ-classification-20-way","title":"GTEx Organ Classification (20-way)","text":"<p>Date: 2025-12-27 Run ID: <code>gtex_giant_openai_gpt-5.2</code></p>"},{"location":"results/benchmark-results/#our-results-vs-paper","title":"Our Results vs Paper","text":"Metric Our Result Paper (GPT-5 GIANT) Paper (GPT-5 GIANT x5) Balanced Accuracy 67.6% \u00b1 3.1% 53.7% 60.7% Bootstrap CI (95%) 61.4% - 73.5% - - Items Processed 191/191 191 191 Errors 6 - - Total Cost $7.21 - -"},{"location":"results/benchmark-results/#analysis","title":"Analysis","text":"<p>Our single-run result of 67.6% exceeds the paper's single-run GIANT result (53.7%) and the paper's 5-run majority vote (60.7%). This is a strong validation that our implementation is working correctly.</p> <p>Possible reasons for improvement over paper: - We used <code>gpt-5.2</code> (latest) vs paper's <code>gpt-5</code> baseline - Minor implementation differences in prompting or crop selection</p>"},{"location":"results/benchmark-results/#comparison-to-baselines-from-paper","title":"Comparison to Baselines (from paper)","text":"Method GTEx Balanced Accuracy Our GIANT (gpt-5.2) 67.6% Paper: GIANT x5 (GPT-5) 60.7% Paper: GIANT x1 (GPT-5) 53.7% Paper: Thumbnail (GPT-5) 36.5% Paper: Patch (GPT-5) 43.7% Paper: TITAN 96.3% Paper: SlideChat 5.0% <p>Our implementation significantly outperforms the paper's thumbnail and patch baselines, but remains below specialized models like TITAN.</p>"},{"location":"results/benchmark-results/#artifacts","title":"Artifacts","text":""},{"location":"results/benchmark-results/#gtex-result-files","title":"GTEx Result Files","text":"<ul> <li>Full Results JSON: <code>results/gtex_giant_openai_gpt-5.2_results.json</code></li> <li>Checkpoint: <code>results/checkpoints/gtex_giant_openai_gpt-5.2.checkpoint.json</code></li> <li>Log File: <code>results/gtex-benchmark-20251227-010151.log</code></li> </ul>"},{"location":"results/benchmark-results/#tcga-result-files","title":"TCGA Result Files","text":"<ul> <li>Full Results JSON: <code>results/tcga_giant_openai_gpt-5.2_results.json</code></li> <li>Checkpoint: <code>results/checkpoints/tcga_giant_openai_gpt-5.2.checkpoint.json</code></li> </ul>"},{"location":"results/benchmark-results/#trajectory-files","title":"Trajectory Files","text":"<p>Individual slide trajectories with full LLM reasoning are saved in: <pre><code>results/trajectories/GTEX-*_run0.json\n</code></pre></p> <p>Each trajectory contains: - WSI path - Question asked - Turn-by-turn data:   - Image shown to model (base64)   - Model reasoning   - Action taken (crop coordinates or answer) - Final prediction - Cost and token usage</p>"},{"location":"results/benchmark-results/#summary-statistics","title":"Summary Statistics","text":"<pre><code>{\n  \"metric_type\": \"balanced_accuracy\",\n  \"point_estimate\": 0.676,\n  \"bootstrap_mean\": 0.676,\n  \"bootstrap_std\": 0.031,\n  \"bootstrap_ci_lower\": 0.614,\n  \"bootstrap_ci_upper\": 0.735,\n  \"n_replicates\": 1000,\n  \"n_total\": 191,\n  \"n_errors\": 6,\n  \"n_extraction_failures\": 0\n}\n</code></pre>"},{"location":"results/benchmark-results/#tcga-cancer-diagnosis-30-way","title":"TCGA Cancer Diagnosis (30-way)","text":"<p>Date: 2025-12-27 Run ID: <code>tcga_giant_openai_gpt-5.2</code></p>"},{"location":"results/benchmark-results/#our-results-vs-paper_1","title":"Our Results vs Paper","text":"Metric Our Result Paper (GPT-5 GIANT) Paper (GPT-5 GIANT x5) Balanced Accuracy 25.2% \u00b1 3.2% 32.3% 29.3% Bootstrap CI (95%) 18.7% - 31.2% - - Items Processed 221/221 221 221 Errors 6 - - Total Cost $15.14 - -"},{"location":"results/benchmark-results/#analysis_1","title":"Analysis","text":"<p>Our single-run result of 25.2% is below the paper's single-run GIANT result (32.3%). This 30-way cancer classification task is significantly harder than GTEx's 20-way organ classification.</p> <p>Possible reasons for underperformance:</p> <ol> <li>Task Difficulty: Cancer diagnosis requires fine-grained cellular features that may need more navigation steps or specialized prompts</li> <li>Class Imbalance: TCGA has 30 cancer types with uneven distribution</li> <li>Single-run variance: In the paper, x5 majority voting (29.3%) did not improve over x1 (32.3%) for this task.</li> </ol>"},{"location":"results/benchmark-results/#comparison-to-baselines-from-paper_1","title":"Comparison to Baselines (from paper)","text":"Method TCGA Balanced Accuracy Paper: GIANT x5 (GPT-5) 29.3% Paper: GIANT x1 (GPT-5) 32.3% Our GIANT (gpt-5.2) 25.2% Paper: Thumbnail (GPT-5) 9.2% Paper: Patch (GPT-5) 12.8% Paper: TITAN 88.8% Paper: SlideChat 3.3% <p>Our implementation outperforms the paper's thumbnail and patch baselines (9.2% and 12.8%), indicating the agent navigation is providing value, but there's room for improvement.</p>"},{"location":"results/benchmark-results/#cost-efficiency","title":"Cost Efficiency","text":"<ul> <li>Cost per item: $15.14 / 221 = ~$0.068/item</li> <li>Average tokens per item: 4,315,199 / 221 = ~19,525 tokens</li> <li>Approximately 1.8x more expensive per item than GTEx (likely due to more navigation steps)</li> </ul>"},{"location":"results/benchmark-results/#summary-statistics-tcga","title":"Summary Statistics (TCGA)","text":"<pre><code>{\n  \"metric_type\": \"balanced_accuracy\",\n  \"point_estimate\": 0.260,\n  \"bootstrap_mean\": 0.252,\n  \"bootstrap_std\": 0.032,\n  \"bootstrap_ci_lower\": 0.187,\n  \"bootstrap_ci_upper\": 0.312,\n  \"n_replicates\": 1000,\n  \"n_total\": 221,\n  \"n_errors\": 6,\n  \"n_extraction_failures\": 0\n}\n</code></pre>"},{"location":"results/benchmark-results/#future-benchmarks","title":"Future Benchmarks","text":"Benchmark Status Paper Result (GIANT x1) GTEx (Organ, 20-way) COMPLETE \u2713 53.7% TCGA (Cancer Dx, 30-way) COMPLETE \u2713 32.3% PANDA (Grading, 6-way) Pending 23.2% ExpertVQA Pending 57.0% SlideBenchVQA Pending 58.9%"},{"location":"results/benchmark-results/#reproducibility","title":"Reproducibility","text":"<p>To reproduce these results:</p> <pre><code># Ensure GTEx WSIs are in data/wsi/gtex/\nuv run giant check-data gtex\n\n# Run benchmark\nsource .env  # Load API keys\nuv run giant benchmark gtex --provider openai --model gpt-5.2 -v\n</code></pre>"},{"location":"results/benchmark-results/#notes","title":"Notes","text":"<ol> <li>Cost Tracking: Total API cost for 191 items was $7.21 (~$0.038/item)</li> <li>Error Rate: 6/191 items (3.1%) failed due to JSON parsing errors</li> <li>WSI Format: Used DICOM format from IDC (OpenSlide 4.0.0+ compatible)</li> </ol>"},{"location":"specs/","title":"GIANT Implementation Specifications","text":"<p>This directory contains the detailed technical specifications for implementing the GIANT (Gigapixel Image Agent for Navigating Tissue) framework.</p> <p>\u26a0\ufe0f CRITICAL: Data Acquisition Required</p> <p>Before running benchmarks, you must acquire 862 unique WSI files from TCGA, GTEx, and PANDA. TCGA alone is ~472 GiB for the 474 slides referenced by MultiPathQA, so plan for many hundreds of GiB of storage. The MultiPathQA CSV contains only metadata - the slides themselves are not included.</p> <ul> <li>TCGA: 474 <code>.svs</code> files (for 3 benchmarks: cancer diagnosis, expert VQA, slidebench)</li> <li>GTEx: 191 <code>.tiff</code> files (organ classification)</li> <li>PANDA: 197 <code>.tiff</code> files (prostate grading)</li> </ul> <p>See: data-acquisition.md for download instructions and file lists.</p>"},{"location":"specs/#specification-index","title":"Specification Index","text":"<p>The specifications are designed to be implemented in sequential order, building a vertical slice of the system.</p> Spec ID Title Status Dependencies Spec-01 Project Foundation &amp; Tooling Ready None Spec-02 WSI Data Layer &amp; OpenSlide Integration Ready Spec-01 Spec-03 Coordinate System &amp; Geometry Ready Spec-02 Spec-04 Pyramid Level Selection Algorithm Ready Spec-02, Spec-03 Spec-05 Image Cropping &amp; Resampling Pipeline Ready Spec-04 Spec-05.5 \ud83d\uded1 WSI Integration Checkpoint PAUSE Spec-02 \u2192 Spec-05 Spec-06 LLM Provider Abstraction Ready Spec-01 Spec-07 Navigation Prompt Engineering Ready Spec-06, Spec-03 Spec-08 Conversation Context Manager Ready Spec-03, Spec-06 Spec-08.5 \ud83d\uded1 LLM Integration Checkpoint PAUSE Spec-06 \u2192 Spec-08 Spec-09 GIANT Agent Core Loop Ready Spec-05.5, Spec-08.5 Spec-10 Evaluation &amp; Benchmarking Framework Ready Spec-09 Spec-11 CLAM Integration (Optional) Ready Spec-02 Spec-11.5 \ud83d\uded1 E2E Validation Checkpoint PAUSE Spec-09 \u2192 Spec-11, DATA_ACQUISITION Spec-12 CLI &amp; API Surface Ready Spec-11.5"},{"location":"specs/#tdd-principles-non-negotiable","title":"TDD Principles (Non-Negotiable)","text":"<p>This project follows strict TDD as per Uncle Bob's Clean Code:</p> <ol> <li>Red-Green-Refactor: Write failing test first, make it pass, then refactor</li> <li>SOLID Principles:<ul> <li>Single Responsibility: Each class does one thing</li> <li>Open/Closed: Extend via protocols, not modification</li> <li>Liskov Substitution: Subtypes must be substitutable</li> <li>Interface Segregation: Small, focused protocols</li> <li>Dependency Inversion: Depend on abstractions (Protocols)</li> </ul> </li> <li>Gang of Four Patterns Used:<ul> <li>Strategy: LLMProvider, PyramidLevelSelector</li> <li>Factory: Provider creation from config</li> <li>Template Method: Agent run loop</li> <li>Observer: Trajectory recording</li> </ul> </li> <li>Test Coverage: Minimum 90% enforced in CI</li> <li>Mutation Testing: Run <code>make mutmut</code> before PRs</li> </ol> <p>All new code MUST have tests written FIRST.</p>"},{"location":"specs/#implementation-dependency-graph","title":"Implementation Dependency Graph","text":"<pre><code>graph TD\n    S01[Spec-01: Foundation] --&gt; S02[Spec-02: WSI Data]\n    S01 --&gt; S06[Spec-06: LLM Provider]\n    S02 --&gt; S03[Spec-03: Coordinates]\n    S02 --&gt; S11[Spec-11: CLAM]\n    S03 --&gt; S04[Spec-04: Level Selection]\n    S03 --&gt; S07[Spec-07: Prompting]\n    S03 --&gt; S08[Spec-08: Context]\n    S04 --&gt; S05[Spec-05: Cropping]\n    S06 --&gt; S07\n    S06 --&gt; S08\n    S05 --&gt; S055[\ud83d\uded1 Spec-05.5: WSI Checkpoint]\n    S08 --&gt; S085[\ud83d\uded1 Spec-08.5: LLM Checkpoint]\n    S055 --&gt; S09[Spec-09: GIANT Agent]\n    S085 --&gt; S09\n    S09 --&gt; S10[Spec-10: Eval]\n    S10 --&gt; S115[\ud83d\uded1 Spec-11.5: E2E Validation]\n    S11 --&gt; S115\n    S115 --&gt; S12[Spec-12: CLI]\n\n    style S055 fill:#ff6b6b,stroke:#333,stroke-width:3px\n    style S085 fill:#ff6b6b,stroke:#333,stroke-width:3px\n    style S115 fill:#ff6b6b,stroke:#333,stroke-width:3px\n</code></pre> <p>Critical Path: Spec-01 \u2192 Spec-02 \u2192 ... \u2192 Spec-05 \u2192 \ud83d\uded1 Spec-05.5 \u2192 Spec-06 \u2192 ... \u2192 Spec-08 \u2192 \ud83d\uded1 Spec-08.5 \u2192 Spec-09 \u2192 Spec-10 \u2192 \ud83d\uded1 Spec-11.5 \u2192 Spec-12</p>"},{"location":"specs/#integration-checkpoints","title":"Integration Checkpoints","text":"<p>These are mandatory pause points before proceeding:</p> Checkpoint Purpose Duration Cost Spec-05.5 Verify WSI pipeline works end-to-end with real <code>.svs</code> files 2-4 hours Free Spec-08.5 Verify LLM pipeline works with real API calls 2-4 hours ~$2-5 Spec-11.5 Verify full system works on MultiPathQA benchmark data 4-8 hours ~$5-50 <p>DO NOT skip these checkpoints. Debugging issues in the CLI/API layer (Spec-12) is 10x harder than catching them here.</p> <p>Lesson Learned: We should validate against real benchmark data at each checkpoint, not just after all specs are implemented. Unit tests with mocks are necessary but not sufficient.</p>"},{"location":"specs/spec-01-foundation/","title":"Spec-01: Project Foundation &amp; Tooling","text":""},{"location":"specs/spec-01-foundation/#overview","title":"Overview","text":"<p>This specification establishes the modern Python project structure, dependency management, and quality assurance tooling required for the GIANT framework. It sets the groundwork for a robust, production-grade codebase using 2025 best practices.</p>"},{"location":"specs/spec-01-foundation/#dependencies","title":"Dependencies","text":"<p>None.</p>"},{"location":"specs/spec-01-foundation/#acceptance-criteria","title":"Acceptance Criteria","text":"<ul> <li>[ ] <code>pyproject.toml</code> is configured with <code>uv</code> as the package manager and includes all initial dependencies.</li> <li>[ ] <code>uv.lock</code> is generated and committed for reproducible installs; CI uses <code>uv sync --locked</code>.</li> <li>[ ] Project directory structure follows the <code>src</code> layout standard.</li> <li>[ ] <code>Makefile</code> exists and successfully runs <code>install</code>, <code>test</code>, <code>lint</code>, <code>format</code>, <code>check</code>, <code>download-data</code>, <code>benchmark</code>, <code>clean</code> and <code>mutmut</code> targets.</li> <li>[ ] <code>pytest</code> is configured with <code>pytest-sugar</code>, <code>pytest-cov</code>, <code>pytest-xdist</code>, <code>pytest-asyncio</code>, <code>pytest-randomly</code>, <code>pytest-watch</code>, and a dummy test passes.</li> <li>[ ] <code>mypy</code> is configured in <code>pyproject.toml</code> with <code>strict = true</code> and passes on the skeleton code.</li> <li>[ ] <code>ruff</code> is configured for both linting and formatting, replacing <code>black</code> and <code>isort</code>.</li> <li>[ ] Pre-commit hooks are installed and pass locally.</li> <li>[ ] Structured logging is implemented using <code>structlog</code>.</li> <li>[ ] Settings management is implemented using <code>pydantic-settings</code> with <code>.env</code> support.</li> <li>[ ] <code>.github/workflows/ci.yml</code> is created with matrix testing and cache.</li> <li>[ ] <code>.github/workflows/codeql.yml</code> is created for CodeQL scanning.</li> <li>[ ] <code>.github/workflows/release.yml</code> is created for tag-based releases (PyPI publishing via Trusted Publishing).</li> <li>[ ] <code>.github/dependabot.yml</code> is created for dependency update PRs.</li> <li>[ ] PR + issue templates are added under <code>.github/</code>.</li> </ul>"},{"location":"specs/spec-01-foundation/#technical-design","title":"Technical Design","text":""},{"location":"specs/spec-01-foundation/#directory-structure","title":"Directory Structure","text":"<pre><code>gigapixel-goblin/\n\u251c\u2500\u2500 .github/\n\u2502   \u251c\u2500\u2500 dependabot.yml\n\u2502   \u251c\u2500\u2500 pull_request_template.md\n\u2502   \u251c\u2500\u2500 ISSUE_TEMPLATE/\n\u2502   \u2502   \u251c\u2500\u2500 bug_report.yml\n\u2502   \u2502   \u2514\u2500\u2500 feature_request.yml\n\u2502   \u2514\u2500\u2500 workflows/\n\u2502       \u251c\u2500\u2500 ci.yml\n\u2502       \u251c\u2500\u2500 codeql.yml\n\u2502       \u2514\u2500\u2500 release.yml\n\u251c\u2500\u2500 docs/\n\u2502   \u2514\u2500\u2500 specs/\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 giant/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u251c\u2500\u2500 config.py\n\u2502       \u251c\u2500\u2500 py.typed\n\u2502       \u2514\u2500\u2500 utils/\n\u2502           \u251c\u2500\u2500 __init__.py\n\u2502           \u2514\u2500\u2500 logging.py\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 conftest.py\n\u2502   \u2514\u2500\u2500 unit/\n\u2502       \u2514\u2500\u2500 test_config.py\n\u251c\u2500\u2500 .env.example\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 .pre-commit-config.yaml\n\u251c\u2500\u2500 Makefile\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 uv.lock\n\u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"specs/spec-01-foundation/#dependency-management-uv","title":"Dependency Management (uv)","text":"<p>We use <code>uv</code> for extremely fast dependency resolution and installation. <code>pyproject.toml</code> configuration: <pre><code>[project]\nname = \"giant\"\nversion = \"0.1.0\"\ndescription = \"Gigapixel Image Agent for Navigating Tissue\"\nrequires-python = \"&gt;=3.11\"\ndependencies = [\n    # Core WSI handling (Dec 2025: v1.4.3)\n    \"openslide-python&gt;=1.4.3\",\n    \"openslide-bin&gt;=4.0.0.10\",\n    # Config &amp; validation\n    \"pydantic&gt;=2.12.5\",\n    \"pydantic-settings&gt;=2.12.0\",\n    \"structlog&gt;=25.5.0\",\n    # CLI (Dec 2025: v0.20.0)\n    \"typer&gt;=0.20.0\",\n    # Numerics\n    # NOTE: `opencv-python` 4.12.0.88 pins NumPy &lt;2.3.0\n    \"numpy&gt;=2.2.6,&lt;2.3.0\",\n    \"pillow&gt;=12.0.0\",\n    # LLM APIs\n    \"anthropic&gt;=0.75.0\",\n    \"openai&gt;=2.13.0\",\n    \"tenacity&gt;=9.1.2\",\n    \"httpx&gt;=0.28.1\",  # Async HTTP for API clients\n    # Data &amp; HuggingFace (Dec 2025: datasets v4.4.1)\n    \"datasets&gt;=4.4.1\",\n    \"huggingface_hub&gt;=1.2.3\",\n    # Computer Vision (for CLAM-style segmentation)\n    \"opencv-python&gt;=4.12.0.88\",\n    \"scipy&gt;=1.16.3\",\n    \"scikit-image&gt;=0.25.2\",\n    # Caching (persistent crop/metadata cache)\n    \"diskcache&gt;=5.6.3\",\n    # Rate limiting for concurrent API calls\n    \"aiolimiter&gt;=1.2.1\",\n    # DevEx\n    \"rich&gt;=14.2.0\",\n    \"tqdm&gt;=4.67.1\",\n]\n\n[dependency-groups]\ndev = [\n    \"pytest&gt;=9.0.2\",\n    \"pytest-cov&gt;=7.0.0\",\n    \"pytest-sugar&gt;=1.1.1\",\n    \"pytest-xdist&gt;=3.8.0\",\n    \"pytest-asyncio&gt;=1.3.0\",\n    \"pytest-randomly&gt;=4.0.1\",\n    \"pytest-watch&gt;=4.2.0\",\n    \"pytest-timeout&gt;=2.4.0\",  # Prevent hanging tests (OpenSlide + async)\n    \"pytest-benchmark&gt;=5.2.3\",  # Performance regression tests\n    \"mutmut&gt;=3.4.0\",  # Mutation testing\n    \"hypothesis&gt;=6.148.7\",\n    \"respx&gt;=0.22.0\",  # Mock httpx requests for LLM tests\n    \"tifffile&gt;=2025.12.12\",  # Create synthetic TIFF test assets\n    \"polyfactory&gt;=3.1.0\",\n    \"faker&gt;=38.2.0\",\n    \"mypy&gt;=1.19.1\",\n    \"ruff&gt;=0.14.9\",\n    \"pre-commit&gt;=4.5.1\",\n    \"types-Pillow&gt;=10.2.0.20240822\",\n]\n</code></pre></p> <p>Note: <code>openslide-python</code> does not have official type stubs. Use inline type annotations with <code># type: ignore</code> for OpenSlide calls, or create local stubs in <code>src/giant/stubs/</code>.</p>"},{"location":"specs/spec-01-foundation/#code-quality-tools","title":"Code Quality Tools","text":""},{"location":"specs/spec-01-foundation/#ruff-linting-formatting","title":"Ruff (Linting &amp; Formatting)","text":"<p>Ruff replaces <code>black</code>, <code>isort</code>, and <code>flake8</code>. <code>pyproject.toml</code> configuration: <pre><code>[tool.ruff]\nline-length = 88\ntarget-version = \"py311\"\n\n[tool.ruff.lint]\nselect = [\"E\", \"F\", \"B\", \"I\", \"N\", \"UP\", \"PL\", \"RUF\"]\nignore = []\n\n[tool.ruff.format]\nquote-style = \"double\"\n</code></pre></p>"},{"location":"specs/spec-01-foundation/#mypy-static-type-checking","title":"Mypy (Static Type Checking)","text":"<p>Strict mode is mandatory. <code>pyproject.toml</code> configuration: <pre><code>[tool.mypy]\nstrict = true\npython_version = \"3.11\"\nwarn_return_any = true\nwarn_unused_configs = true\ndisallow_untyped_defs = true\n</code></pre></p>"},{"location":"specs/spec-01-foundation/#pytest-coverage","title":"Pytest + Coverage","text":"<p>Keep the test pyramid healthy (unit-heavy, minimal live API tests). <code>pyproject.toml</code> configuration: <pre><code>[tool.pytest.ini_options]\nminversion = \"9.0\"\ntestpaths = [\"tests\"]\naddopts = \"-ra\"\nmarkers = [\n  \"cost: live API tests (opt-in, may spend money)\",\n]\nasyncio_mode = \"auto\"\n\n[tool.coverage.run]\nbranch = true\nsource = [\"giant\"]\n\n[tool.coverage.report]\nfail_under = 90\nshow_missing = true\nskip_covered = true\n\n[tool.mutmut]\npaths_to_mutate = [\"src/giant\"]\npytest_add_cli_args = [\"-q\"]\npytest_add_cli_args_test_selection = [\"tests/unit\"]\n</code></pre></p>"},{"location":"specs/spec-01-foundation/#build-system-packaging","title":"Build System (Packaging)","text":"<p>Use a modern PEP 517 build backend so <code>uv build</code> works in CI/release workflows. <pre><code>[build-system]\nrequires = [\"hatchling&gt;=1.28.0\"]\nbuild-backend = \"hatchling.build\"\n</code></pre></p>"},{"location":"specs/spec-01-foundation/#logging-structlog","title":"Logging (structlog)","text":"<p>Structured logging is essential for observability of the agent's reasoning traces. <code>src/giant/utils/logging.py</code> will configure <code>structlog</code> to output JSON in production and colored console output in development.</p> <p>Correlation IDs (Required): Use <code>contextvars</code> to attach <code>run_id</code>, <code>item_id</code> (benchmark row), and <code>step</code> to every log line to support tracing/debugging across concurrent runs.</p>"},{"location":"specs/spec-01-foundation/#configuration-pydantic-settings","title":"Configuration (Pydantic Settings)","text":"<p>All configuration must be strongly typed. <code>src/giant/config.py</code>: <pre><code>from pydantic_settings import BaseSettings, SettingsConfigDict\n\nclass Settings(BaseSettings):\n    model_config = SettingsConfigDict(env_file=\".env\", env_file_encoding=\"utf-8\")\n\n    OPENAI_API_KEY: str | None = None\n    ANTHROPIC_API_KEY: str | None = None\n    HUGGINGFACE_TOKEN: str | None = None\n    LOG_LEVEL: str = \"INFO\"\n\n    # Paper Parameters\n    WSI_LONG_SIDE_TARGET: int = 1000  # S parameter\n    MAX_ITERATIONS: int = 20  # T parameter\n    OVERSAMPLING_BIAS: float = 0.85\n    THUMBNAIL_SIZE: int = 1024  # Paper baseline\n\n    # Baselines\n    PATCH_SIZE: int = 224\n    PATCH_COUNT: int = 30\n\n    # Evaluation\n    BOOTSTRAP_REPLICATES: int = 1000\n\n    # Image Generation\n    JPEG_QUALITY: int = 85\n    # Per-provider image sizes (paper uses 500px for Claude due to pricing)\n    IMAGE_SIZE_OPENAI: int = 1000\n    IMAGE_SIZE_ANTHROPIC: int = 500\n\nsettings = Settings()\n</code></pre></p>"},{"location":"specs/spec-01-foundation/#test-plan","title":"Test Plan","text":""},{"location":"specs/spec-01-foundation/#unit-tests","title":"Unit Tests","text":"<ol> <li>Config Test: Verify that settings are loaded correctly from env vars and defaults.</li> <li>Logging Test: Verify that the logger produces valid JSON/structured output.</li> </ol>"},{"location":"specs/spec-01-foundation/#integration-tests","title":"Integration Tests","text":"<p>N/A for this foundation spec.</p>"},{"location":"specs/spec-01-foundation/#file-structure-content","title":"File Structure &amp; Content","text":""},{"location":"specs/spec-01-foundation/#envexample","title":"<code>.env.example</code>","text":"<pre><code>OPENAI_API_KEY=\nANTHROPIC_API_KEY=\nHUGGINGFACE_TOKEN=\nLOG_LEVEL=INFO\n</code></pre>"},{"location":"specs/spec-01-foundation/#makefile","title":"<code>Makefile</code>","text":"<pre><code>.PHONY: install install-system test test-watch test-cov lint format format-check typecheck check all clean download-data benchmark mutmut\n\ninstall:\n    uv sync\n\ninstall-system:\n    @echo \"Installing system dependencies...\"\n    @echo \"Note: openslide-bin pip package now auto-installs OpenSlide binaries\"\n    @echo \"Manual install only needed if openslide-bin fails:\"\n    @if [ \"$(shell uname)\" = \"Darwin\" ]; then \\\n        brew install openslide; \\\n    elif [ -f /etc/debian_version ]; then \\\n        sudo apt-get install -y openslide-tools; \\\n    fi\n\ntest:\n    uv run pytest\n\ntest-watch:\n    uv run ptw -- --maxfail=1\n\ntest-cov:\n    uv run pytest --cov=src/giant --cov-report=html --cov-fail-under=90\n\nlint:\n    uv run ruff check .\n\nformat:\n    uv run ruff format .\n\nformat-check:\n    uv run ruff format --check .\n\ntypecheck:\n    uv run mypy src\n\ncheck: lint typecheck test\n\ndownload-data:\n    uv run python -m giant.data.download\n\nbenchmark:\n    uv run giant benchmark ./data/multipathqa --output-dir ./results\n\nmutmut:\n    uv run mutmut run\n\nclean:\n    rm -rf .pytest_cache .mypy_cache .ruff_cache htmlcov .coverage\n    find . -type d -name __pycache__ -exec rm -rf {} +\n\nall: format check\n</code></pre>"},{"location":"specs/spec-01-foundation/#githubworkflowsciyml","title":"<code>.github/workflows/ci.yml</code>","text":"<pre><code>name: CI\n\non:\n  push:\n    branches: [main]\n  pull_request:\n    branches: [main]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        python-version: [\"3.11\", \"3.12\"]\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Install uv\n        uses: astral-sh/setup-uv@v7\n        with:\n          version: \"0.9.18\"\n          python-version: ${{ matrix.python-version }}\n          enable-cache: true\n          cache-suffix: ${{ matrix.python-version }}\n          cache-dependency-glob: |\n            pyproject.toml\n            uv.lock\n\n      - name: Install system deps\n        run: |\n          sudo apt-get update\n          sudo apt-get install -y openslide-tools\n\n      - name: Install dependencies\n        run: uv sync --locked\n\n      - name: Format check\n        run: uv run ruff format --check .\n\n      - name: Lint\n        run: uv run ruff check .\n\n      - name: Type check\n        run: uv run mypy src\n\n      - name: Test with coverage\n        run: uv run pytest --cov=src/giant --cov-report=xml --cov-fail-under=90\n\n      - name: Upload coverage\n        uses: codecov/codecov-action@v5\n        with:\n          files: coverage.xml\n          fail_ci_if_error: true\n</code></pre>"},{"location":"specs/spec-01-foundation/#githubworkflowscodeqlyml","title":"<code>.github/workflows/codeql.yml</code>","text":"<pre><code>name: CodeQL\n\non:\n  push:\n    branches: [main]\n  pull_request:\n    branches: [main]\n  schedule:\n    - cron: \"0 3 * * 1\"\n\njobs:\n  analyze:\n    runs-on: ubuntu-latest\n    permissions:\n      actions: read\n      contents: read\n      security-events: write\n\n    steps:\n      - uses: actions/checkout@v4\n      - uses: github/codeql-action/init@v3\n        with:\n          languages: python\n      - uses: github/codeql-action/analyze@v3\n</code></pre>"},{"location":"specs/spec-01-foundation/#githubworkflowsreleaseyml","title":"<code>.github/workflows/release.yml</code>","text":"<pre><code>name: Release\n\non:\n  push:\n    tags: [\"v*\"]\n  workflow_dispatch:\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Install uv\n        uses: astral-sh/setup-uv@v7\n        with:\n          version: \"0.9.18\"\n          python-version: \"3.12\"\n          enable-cache: true\n          cache-dependency-glob: |\n            pyproject.toml\n            uv.lock\n\n      - name: Build distributions\n        run: uv build --no-sources\n\n      - uses: actions/upload-artifact@v4\n        with:\n          name: dist\n          path: dist/*\n\n  publish:\n    needs: build\n    runs-on: ubuntu-latest\n    permissions:\n      id-token: write\n      contents: read\n    steps:\n      - uses: actions/download-artifact@v4\n        with:\n          name: dist\n          path: dist\n\n      - name: Publish to PyPI\n        uses: pypa/gh-action-pypi-publish@v1.13.0\n        with:\n          packages-dir: dist\n</code></pre>"},{"location":"specs/spec-01-foundation/#githubdependabotyml","title":"<code>.github/dependabot.yml</code>","text":"<pre><code>version: 2\nupdates:\n  - package-ecosystem: \"github-actions\"\n    directory: \"/\"\n    schedule:\n      interval: \"weekly\"\n    groups:\n      github-actions:\n        patterns: [\"*\"]\n\n  - package-ecosystem: \"pip\"\n    directory: \"/\"\n    schedule:\n      interval: \"weekly\"\n    open-pull-requests-limit: 10\n</code></pre>"},{"location":"specs/spec-01-foundation/#pre-commit-configyaml","title":"<code>.pre-commit-config.yaml</code>","text":"<pre><code>repos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v6.0.0\n    hooks:\n      - id: trailing-whitespace\n      - id: end-of-file-fixer\n      - id: check-yaml\n      - id: check-toml\n      - id: check-merge-conflict\n      - id: detect-private-key\n      - id: check-added-large-files\n        args: [\"--maxkb=2048\"]\n\n  - repo: https://github.com/astral-sh/ruff-pre-commit\n    rev: v0.14.9\n    hooks:\n      - id: ruff\n        args: [--fix, --exit-non-zero-on-fix]\n      - id: ruff-format\n\n  - repo: local\n    hooks:\n      - id: uv-lock-check\n        name: uv lock --check\n        entry: uv lock --check\n        language: system\n        pass_filenames: false\n\n      - id: mypy\n        name: mypy (uv)\n        entry: uv run mypy src\n        language: system\n        pass_filenames: false\n</code></pre>"},{"location":"specs/spec-01-foundation/#githubpull_request_templatemd","title":"<code>.github/pull_request_template.md</code>","text":"<pre><code>## Summary\n-\n\n## Test Plan\n- [ ] `make check`\n- [ ] Unit tests added/updated\n\n## Checklist\n- [ ] No hardcoded secrets\n- [ ] Logging is structured and non-PHI by default\n- [ ] Spec acceptance criteria met\n</code></pre>"},{"location":"specs/spec-01-foundation/#githubissue_templatebug_reportyml","title":"<code>.github/ISSUE_TEMPLATE/bug_report.yml</code>","text":"<pre><code>name: Bug report\ndescription: Report a problem in GIANT\ntitle: \"bug: \"\nlabels: [\"bug\"]\nbody:\n  - type: textarea\n    id: what-happened\n    attributes:\n      label: What happened?\n      description: Include error messages and a short description.\n    validations:\n      required: true\n\n  - type: textarea\n    id: steps\n    attributes:\n      label: Steps to reproduce\n      description: Provide a minimal repro if possible.\n\n  - type: textarea\n    id: logs\n    attributes:\n      label: Logs\n      render: text\n\n  - type: input\n    id: version\n    attributes:\n      label: GIANT version\n\n  - type: input\n    id: python\n    attributes:\n      label: Python version\n</code></pre>"},{"location":"specs/spec-01-foundation/#githubissue_templatefeature_requestyml","title":"<code>.github/ISSUE_TEMPLATE/feature_request.yml</code>","text":"<pre><code>name: Feature request\ndescription: Suggest an enhancement\ntitle: \"feat: \"\nlabels: [\"enhancement\"]\nbody:\n  - type: textarea\n    id: problem\n    attributes:\n      label: Problem statement\n      description: What problem are you trying to solve?\n    validations:\n      required: true\n\n  - type: textarea\n    id: proposal\n    attributes:\n      label: Proposed solution\n\n  - type: textarea\n    id: alternatives\n    attributes:\n      label: Alternatives considered\n</code></pre>"},{"location":"specs/spec-01-foundation/#branch-protection-documented-policy","title":"Branch Protection (Documented Policy)","text":"<p>Configure GitHub branch protection for <code>main</code>: - Require status checks: <code>CI</code>, <code>CodeQL</code> - Require 1+ approvals - Require linear history (optional) - Require conversation resolution - Restrict who can push to <code>main</code></p>"},{"location":"specs/spec-01-foundation/#dockerfile-optional-production-readiness","title":"<code>Dockerfile</code> (Optional, Production-Readiness)","text":"<p>If the project is deployed as a service or run reproducibly in a container, provide a minimal Dockerfile. <pre><code>FROM python:3.12-slim\n\nENV PYTHONDONTWRITEBYTECODE=1\nENV PYTHONUNBUFFERED=1\n\nRUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\\\\n    openslide-tools \\\\\n  &amp;&amp; rm -rf /var/lib/apt/lists/*\n\nWORKDIR /app\n\nRUN pip install --no-cache-dir uv==0.9.18\n\nCOPY pyproject.toml uv.lock /app/\nRUN uv sync --frozen --no-dev --no-install-project\n\nCOPY src /app/src\n\nRUN uv sync --frozen --no-dev\n\nENTRYPOINT [\"giant\"]\n</code></pre></p>"},{"location":"specs/spec-01-foundation/#dockerignore-optional","title":"<code>.dockerignore</code> (Optional)","text":"<pre><code>.git\n__pycache__/\n.pytest_cache/\n.mypy_cache/\n.ruff_cache/\nhtmlcov/\ndist/\n*.egg-info/\n.venv/\ndata/\nresults/\n</code></pre>"},{"location":"specs/spec-01-foundation/#api-reference","title":"API Reference","text":"<p>N/A</p>"},{"location":"specs/spec-02-wsi-data/","title":"Spec-02: WSI Data Layer &amp; OpenSlide Integration","text":""},{"location":"specs/spec-02-wsi-data/#overview","title":"Overview","text":"<p>This specification defines the data access layer for Whole Slide Images (WSIs). It abstracts the <code>openslide</code> library to provide a clean, type-safe interface for querying slide metadata, navigating pyramid levels, and extracting regions (patches) at various resolutions. This layer handles the complexity of coordinate transformations and format compatibility.</p>"},{"location":"specs/spec-02-wsi-data/#dependencies","title":"Dependencies","text":"<ul> <li>Spec-01: Project Foundation &amp; Tooling</li> </ul>"},{"location":"specs/spec-02-wsi-data/#acceptance-criteria","title":"Acceptance Criteria","text":"<ul> <li>[ ] <code>WSIReader</code> class exists and can open standard WSI formats (.svs, .ndpi, .tiff, .mrxs).</li> <li>[ ] <code>WSIReader</code> correctly reports <code>dimensions</code>, <code>level_count</code>, <code>level_dimensions</code>, and <code>level_downsamples</code>.</li> <li>[ ] <code>read_region</code> method accepts Level-0 coordinates and returns a PIL Image at the specified level.</li> <li>[ ] <code>get_thumbnail</code> method returns a PIL Image of the full slide at a specified maximum size.</li> <li>[ ] Coordinate translation utility correctly maps coordinates between levels.</li> <li>[ ] Custom exceptions (<code>WSIOpenError</code>, <code>WSIReadError</code>) are raised for failure modes.</li> <li>[ ] Unit tests use synthetic/mocked OpenSlide objects (avoiding large binary assets in repo).</li> </ul>"},{"location":"specs/spec-02-wsi-data/#technical-design","title":"Technical Design","text":""},{"location":"specs/spec-02-wsi-data/#data-models","title":"Data Models","text":"<pre><code>from dataclasses import dataclass\nfrom typing import Tuple\n\n@dataclass(frozen=True)\nclass WSIMetadata:\n    path: str\n    width: int\n    height: int\n    level_count: int\n    level_dimensions: Tuple[Tuple[int, int], ...]\n    level_downsamples: Tuple[float, ...]\n    vendor: str\n    mpp_x: float | None  # Microns per pixel\n    mpp_y: float | None\n</code></pre>"},{"location":"specs/spec-02-wsi-data/#interfaces","title":"Interfaces","text":"<pre><code>from typing import Protocol, Tuple\nfrom PIL import Image\n\nclass WSIReaderProtocol(Protocol):\n    def get_metadata(self) -&gt; WSIMetadata: ...\n    def read_region(self, location: Tuple[int, int], level: int, size: Tuple[int, int]) -&gt; Image.Image: ...\n    def get_thumbnail(self, max_size: Tuple[int, int]) -&gt; Image.Image: ...\n    def close(self) -&gt; None: ...\n</code></pre>"},{"location":"specs/spec-02-wsi-data/#implementation-details","title":"Implementation Details","text":""},{"location":"specs/spec-02-wsi-data/#wsireader-class","title":"<code>WSIReader</code> Class","text":"<p>Wraps <code>openslide.OpenSlide</code>.</p> <ul> <li>Initialization: Takes a file path. Validates file existence and supported extension.</li> <li>Coordinate System: <code>openslide.read_region</code> always expects (x, y) in Level-0 (highest resolution) pixel coordinates. The <code>WSIReader.read_region</code> wrapper must enforce this and document it clearly.</li> <li>Context Manager: Should implement <code>__enter__</code> and <code>__exit__</code> for proper resource cleanup.</li> </ul>"},{"location":"specs/spec-02-wsi-data/#error-handling","title":"Error Handling","text":"<ul> <li><code>FileNotFoundError</code>: If path is invalid.</li> <li><code>openslide.OpenSlideError</code>: Caught and re-raised as <code>WSIOpenError</code> or <code>WSIReadError</code> with context.</li> </ul>"},{"location":"specs/spec-02-wsi-data/#synthetic-testing","title":"Synthetic Testing","text":"<p>Since WSIs are gigabytes in size, we cannot check them into git. - Strategy: Use <code>unittest.mock</code> to mock <code>openslide.OpenSlide</code>. - Advanced: Create a small Tiled TIFF using <code>tifffile</code> in the test setup to test actual reading logic without 1GB+ files.</p>"},{"location":"specs/spec-02-wsi-data/#test-plan","title":"Test Plan","text":""},{"location":"specs/spec-02-wsi-data/#unit-tests","title":"Unit Tests","text":"<ol> <li>Metadata Extraction: Mock an OpenSlide object with known dimensions/levels. Verify <code>get_metadata</code> returns correct <code>WSIMetadata</code>.</li> <li>Read Region Passthrough: Verify <code>read_region</code> calls the underlying <code>openslide.read_region</code> with correct arguments.</li> <li>Error Propagation: Mock <code>OpenSlideError</code> and assert <code>WSIReadError</code> is raised.</li> <li>Context Manager: Verify <code>close()</code> is called on exit.</li> </ol>"},{"location":"specs/spec-02-wsi-data/#integration-tests","title":"Integration Tests","text":"<ul> <li>Real File Test: (Skipped in CI unless a sample file is present) Test opening a small valid <code>.svs</code> or <code>.tiff</code> if available in a local data directory.</li> </ul>"},{"location":"specs/spec-02-wsi-data/#file-structure","title":"File Structure","text":"<pre><code>src/giant/wsi/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 reader.py       # WSIReader implementation\n\u251c\u2500\u2500 exceptions.py   # Custom errors\n\u2514\u2500\u2500 types.py        # WSIMetadata and Protocols\ntests/unit/wsi/\n\u2514\u2500\u2500 test_reader.py\n</code></pre>"},{"location":"specs/spec-02-wsi-data/#api-reference","title":"API Reference","text":""},{"location":"specs/spec-02-wsi-data/#wsireader","title":"<code>WSIReader</code>","text":"<pre><code>class WSIReader:\n    def __init__(self, path: str | Path):\n        \"\"\"Opens a WSI file.\"\"\"\n\n    def read_region(self, location: Tuple[int, int], level: int, size: Tuple[int, int]) -&gt; Image.Image:\n        \"\"\"\n        Reads a region from the WSI.\n\n        Args:\n            location: (x, y) tuple of top-left corner in LEVEL-0 coordinates.\n            level: The pyramid level to read from.\n            size: (width, height) of the region to read AT THE SPECIFIED LEVEL.\n        \"\"\"\n</code></pre>"},{"location":"specs/spec-03-coordinates/","title":"Spec-03: Coordinate System &amp; Geometry","text":""},{"location":"specs/spec-03-coordinates/#overview","title":"Overview","text":"<p>This specification defines the core geometry primitives and coordinate systems used by GIANT. The paper specifies that all navigation occurs in Level-0 (absolute pixel) coordinates. This module provides robust types for handling these coordinates, verifying they are within image bounds, and generating the visual \"axis guides\" overlaid on thumbnails to help the LLM orient itself.</p>"},{"location":"specs/spec-03-coordinates/#dependencies","title":"Dependencies","text":"<ul> <li>Spec-02: WSI Data Layer &amp; OpenSlide Integration</li> </ul>"},{"location":"specs/spec-03-coordinates/#acceptance-criteria","title":"Acceptance Criteria","text":"<ul> <li>[ ] <code>Point</code>, <code>Size</code>, and <code>Region</code> Pydantic models are implemented with integer fields.</li> <li>[ ] <code>Region</code> supports conversion to/from <code>(x, y, w, h)</code> tuples.</li> <li>[ ] <code>GeometryValidator</code> class exists to check if a <code>Region</code> is valid within a given <code>WSIMetadata</code>.</li> <li>[ ] <code>AxisGuideGenerator</code> creates a transparent overlay with 4 evenly spaced grid lines/labels per dimension.</li> <li>[ ] <code>OverlayService</code> combines the thumbnail and axis guides into a single image for the LLM.</li> <li>[ ] Coordinate transformation utilities (Level-N &lt;-&gt; Level-0) are implemented and tested.</li> </ul>"},{"location":"specs/spec-03-coordinates/#technical-design","title":"Technical Design","text":""},{"location":"specs/spec-03-coordinates/#data-models","title":"Data Models","text":"<pre><code>from pydantic import BaseModel, Field\n\nclass Point(BaseModel):\n    x: int = Field(..., ge=0)\n    y: int = Field(..., ge=0)\n\nclass Size(BaseModel):\n    width: int = Field(..., gt=0)\n    height: int = Field(..., gt=0)\n\nclass Region(BaseModel):\n    \"\"\"Represents a rectangular region (x, y, w, h) in Level-0 coordinates.\"\"\"\n    x: int = Field(..., ge=0)\n    y: int = Field(..., ge=0)\n    width: int = Field(..., gt=0)\n    height: int = Field(..., gt=0)\n\n    @property\n    def area(self) -&gt; int:\n        return self.width * self.height\n\n    @property\n    def right(self) -&gt; int:\n        return self.x + self.width\n\n    @property\n    def bottom(self) -&gt; int:\n        return self.y + self.height\n</code></pre>"},{"location":"specs/spec-03-coordinates/#implementation-details","title":"Implementation Details","text":""},{"location":"specs/spec-03-coordinates/#coordinate-systems","title":"Coordinate Systems","text":"<ul> <li>Level-0: The \"Ground Truth\" absolute pixel coordinates. All <code>Region</code> objects are Level-0 by default.</li> <li>Thumbnail Space: The coordinates on the downsampled thumbnail.</li> <li>Transformation:</li> <li><code>Thumbnail -&gt; Level-0</code>: <code>coord_L0 = coord_thumb * downsample_factor</code></li> <li><code>Level-0 -&gt; Thumbnail</code>: <code>coord_thumb = coord_L0 / downsample_factor</code></li> </ul>"},{"location":"specs/spec-03-coordinates/#axis-guides-implementation","title":"Axis Guides Implementation","text":"<p>The paper states: \"Thumbnail is overlaid with four evenly spaced axis guides along each dimension, labeled with absolute level-0 pixel coordinates.\"</p> <p>Algorithm: 1.  Input: <code>thumbnail_image</code>, <code>wsi_metadata</code>. 2.  Calculate step size: <code>step_x = thumbnail_width / 5</code>, <code>step_y = thumbnail_height / 5</code> (to get 4 internal lines). 3.  Draw lines:     - Vertical lines at <code>x = step_x * i</code> for <code>i in 1..4</code>.     - Horizontal lines at <code>y = step_y * i</code> for <code>i in 1..4</code>. 4.  Draw Labels:     - Calculate the corresponding Level-0 coordinate for each line: <code>L0_x = x * (wsi_width / thumbnail_width)</code>.     - Render text label (e.g., \"15000\") near the edge of the line. 5.  Style: Semi-transparent red or contrasting color to be visible but not obscure tissue.</p>"},{"location":"specs/spec-03-coordinates/#validation","title":"Validation","text":"<p><code>GeometryValidator.validate(region: Region, bounds: Size)</code>: - Checks if <code>region.right &lt;= bounds.width</code> and <code>region.bottom &lt;= bounds.height</code>. - Raises a <code>ValidationError</code> if out of bounds (strict by default).</p>"},{"location":"specs/spec-03-coordinates/#optional-clamping-explicit-recovery-path","title":"Optional Clamping (Explicit Recovery Path)","text":"<p>GIANT paper does not specify how invalid crops are handled. For robustness, implement clamping as a separate method (<code>clamp_region</code>) and only use it when explicitly chosen by the agent error-recovery policy (Spec-09).</p>"},{"location":"specs/spec-03-coordinates/#geometryvalidatorclamp_region","title":"<code>GeometryValidator.clamp_region</code>","text":"<pre><code>def clamp_region(self, region: Region, bounds: Size) -&gt; Region:\n    \"\"\"Clamp region to valid bounds (common LLM error recovery).\n\n    Ensures:\n    - x, y are within [0, bounds-1]\n    - width, height respect the clamped origin\n    - Minimum dimension of 1px is preserved\n    \"\"\"\n    clamped_x = max(0, min(region.x, bounds.width - 1))\n    clamped_y = max(0, min(region.y, bounds.height - 1))\n    clamped_w = max(1, min(region.width, bounds.width - clamped_x))\n    clamped_h = max(1, min(region.height, bounds.height - clamped_y))\n    return Region(x=clamped_x, y=clamped_y, width=clamped_w, height=clamped_h)\n</code></pre>"},{"location":"specs/spec-03-coordinates/#test-plan","title":"Test Plan","text":""},{"location":"specs/spec-03-coordinates/#unit-tests","title":"Unit Tests","text":"<ol> <li>Region Math: Test <code>right</code>, <code>bottom</code>, <code>area</code> properties.</li> <li>Validation: Test <code>validate</code> with inside, outside, and overlapping regions.</li> <li>Clamping: Test <code>clamp_region</code> with edge cases:<ul> <li>Region fully inside bounds \u2192 unchanged</li> <li>Region x/y exceeds bounds \u2192 clamped to edge</li> <li>Region width/height exceeds remaining space \u2192 truncated</li> <li>Region entirely outside bounds \u2192 clamped to corner with 1px</li> </ul> </li> <li>Transformation: Test converting coordinates between Level-0 and arbitrary downsamples.</li> <li>Overlay Generation: Use a blank image, run <code>AxisGuideGenerator</code>, and verify (via pixel inspection or mock calls) that lines are drawn at correct intervals.</li> </ol>"},{"location":"specs/spec-03-coordinates/#property-based-tests","title":"Property-Based Tests","text":"<ul> <li>Generate random valid <code>Region</code>s and <code>WSIMetadata</code>. Verify that transforming L0 -&gt; LN -&gt; L0 returns the original coordinate (within rounding error margin).</li> </ul>"},{"location":"specs/spec-03-coordinates/#file-structure","title":"File Structure","text":"<pre><code>src/giant/geometry/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 primitives.py   # Point, Size, Region\n\u251c\u2500\u2500 validators.py   # Bounds checking\n\u251c\u2500\u2500 transforms.py   # Coordinate transforms\n\u2514\u2500\u2500 overlay.py      # Axis guide rendering\ntests/unit/geometry/\n\u251c\u2500\u2500 test_primitives.py\n\u2514\u2500\u2500 test_overlay.py\n</code></pre>"},{"location":"specs/spec-03-coordinates/#api-reference","title":"API Reference","text":"<p>N/A</p>"},{"location":"specs/spec-04-level-selection/","title":"Spec-04: Pyramid Level Selection Algorithm","text":""},{"location":"specs/spec-04-level-selection/#overview","title":"Overview","text":"<p>This specification implements the core heuristic used by GIANT to select the optimal pyramid level for a given crop. The goal is to retrieve a region that, when scaled to the target size $S$ (default 1000px), retains maximum detail without unnecessary processing overhead. The paper specifies a distinct \"oversampling bias\" logic.</p>"},{"location":"specs/spec-04-level-selection/#dependencies","title":"Dependencies","text":"<ul> <li>Spec-02: WSI Data Layer &amp; OpenSlide Integration</li> <li>Spec-03: Coordinate System &amp; Geometry</li> </ul>"},{"location":"specs/spec-04-level-selection/#acceptance-criteria","title":"Acceptance Criteria","text":"<ul> <li>[ ] <code>PyramidLevelSelector</code> class is implemented.</li> <li>[ ] <code>select_level</code> method accepts a <code>Region</code> (L0), <code>WSIMetadata</code>, and target size <code>S</code>.</li> <li>[ ] Implements the specific logic: <code>bias = 0.85</code>, check if projected size &lt; S, shift level.</li> <li>[ ] Returns the optimal <code>level_index</code> (int) and the <code>downsample_factor</code> (float).</li> <li>[ ] Property-based tests verify the selected level is always valid (0 &lt;= level &lt; count).</li> </ul>"},{"location":"specs/spec-04-level-selection/#technical-design","title":"Technical Design","text":""},{"location":"specs/spec-04-level-selection/#algorithm-paper-faithful","title":"Algorithm (Paper-Faithful)","text":"<p>Paper Reference: \"We choose the pyramid level that will render the crop to a target long side (default S=1000 px) while biasing toward finer levels (oversampling bias 0.85)... If this level undershoots S, we move one level finer.\"</p> <p>Inputs: - <code>region</code>: Requested crop in Level-0 coordinates. - <code>metadata.level_downsamples</code>: Pyramid downsample factors (relative to Level-0). - <code>target_S</code>: Target long-side in pixels (default: 1000). - <code>bias</code>: Oversampling bias (default: 0.85).</p> <p>Definitions: - <code>L0 = max(region.width, region.height)</code> \u2014 Region long-side in Level-0 pixels. - For each level <code>k</code>: <code>Lk = L0 / metadata.level_downsamples[k]</code> \u2014 Projected size at level k. - <code>target_native = target_S / bias</code> \u2014 The \"ideal\" native resolution before downsampling. By dividing by 0.85, we bias toward selecting levels that will oversample (requiring downscale to S) rather than undersample (requiring upscale).</p> <p>Steps: 1. Compute <code>target_native = target_S / bias</code>. 2. Select <code>k*</code> as the level minimizing <code>abs(Lk - target_native)</code>.    - Tie-breaker: Choose the finer level (smaller <code>k</code> / smaller downsample factor) to match \"biasing toward finer levels\". 3. Undershoot correction: If <code>Lk* &lt; target_S</code>, move one level finer: <code>k* = max(k* - 1, 0)</code>.    - Repeat until <code>Lk* &gt;= target_S</code> OR <code>k* == 0</code> (can't go finer). 4. Return <code>k*</code> and <code>metadata.level_downsamples[k*]</code>.</p> <p>Invariant: After this algorithm, we always downsample (or 1:1) to reach <code>target_S</code>, never upsample, unless the region is smaller than <code>target_S</code> at Level-0.</p> <p>Implementation: <pre><code>def select_level(\n    region: Region,\n    metadata: WSIMetadata,\n    target_size: int = 1000,\n    bias: float = 0.85,\n) -&gt; SelectedLevel:\n    L0 = max(region.width, region.height)\n    target_native = target_size / bias\n\n    # Find level closest to target_native, prefer finer on tie\n    best_k = 0\n    best_diff = float(\"inf\")\n    for k, ds in enumerate(metadata.level_downsamples):\n        Lk = L0 / ds\n        diff = abs(Lk - target_native)\n        if diff &lt; best_diff or (diff == best_diff and k &lt; best_k):\n            best_diff = diff\n            best_k = k\n\n    # Undershoot correction: ensure Lk &gt;= target_size\n    while best_k &gt; 0:\n        Lk = L0 / metadata.level_downsamples[best_k]\n        if Lk &gt;= target_size:\n            break\n        best_k -= 1\n\n    return SelectedLevel(level=best_k, downsample=metadata.level_downsamples[best_k])\n</code></pre></p>"},{"location":"specs/spec-04-level-selection/#interface","title":"Interface","text":"<pre><code>from typing import Protocol, NamedTuple\n\nclass SelectedLevel(NamedTuple):\n    \"\"\"Result of level selection.\"\"\"\n    level: int        # Pyramid level index (0 = finest)\n    downsample: float # Downsample factor at this level\n\nclass LevelSelectorProtocol(Protocol):\n    def select_level(\n        self,\n        region: Region,\n        metadata: WSIMetadata,\n        target_size: int = 1000,\n        bias: float = 0.85,\n    ) -&gt; SelectedLevel: ...\n</code></pre>"},{"location":"specs/spec-04-level-selection/#test-plan","title":"Test Plan","text":""},{"location":"specs/spec-04-level-selection/#unit-tests","title":"Unit Tests","text":"<ol> <li>Standard Case: Region 10,000px, S=1000, bias=0.85.<ul> <li><code>target_native = 1000 / 0.85 \u2248 1176</code>.</li> <li>Levels: ds=[1, 4, 16] \u2192 L=[10000, 2500, 625].</li> <li>Closest to 1176: L1=2500 (diff=1324) vs L2=625 (diff=551) \u2192 L2 is closer.</li> <li>Undershoot check: 625 &lt; 1000 \u2192 move to L1.</li> <li>L1=2500 &gt;= 1000 \u2192 Result: Level 1.</li> </ul> </li> <li>Undershoot Correction: Region 2000px, S=1000, bias=0.85.<ul> <li><code>target_native \u2248 1176</code>.</li> <li>Levels: ds=[1, 4] \u2192 L=[2000, 500].</li> <li>Closest to 1176: L0=2000 (diff=824) vs L1=500 (diff=676) \u2192 L1 is closer.</li> <li>Undershoot check: 500 &lt; 1000 \u2192 move to L0.</li> <li>L0=2000 &gt;= 1000 \u2192 Result: Level 0.</li> </ul> </li> <li>Tie-breaker (prefer finer): If two levels equidistant from target_native, pick smaller k.</li> <li>Small region: Region 500px at Level-0 with S=1000.<ul> <li>Even Level-0 undershoots. Return Level 0 (can't go finer).</li> </ul> </li> <li>Exact match: Region where Lk == target_native exactly.</li> </ol>"},{"location":"specs/spec-04-level-selection/#property-based-tests","title":"Property-Based Tests","text":"<ul> <li><code>given(region_size=integers(100, 100000), target_s=integers(500, 2000), bias=floats(0.7, 1.0))</code></li> <li>Verify selected level index is within bounds: <code>0 &lt;= level &lt; metadata.level_count</code>.</li> <li>Verify invariant: <code>Lk &gt;= target_s OR level == 0</code> (never upsample unless forced).</li> <li>Verify determinism: same inputs \u2192 same output.</li> </ul>"},{"location":"specs/spec-04-level-selection/#file-structure","title":"File Structure","text":"<pre><code>src/giant/core/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 level_selector.py\ntests/unit/core/\n\u2514\u2500\u2500 test_level_selector.py\n</code></pre>"},{"location":"specs/spec-05-cropping/","title":"Spec-05: Image Cropping &amp; Resampling Pipeline","text":""},{"location":"specs/spec-05-cropping/#overview","title":"Overview","text":"<p>This specification implements the <code>CropRegion(W, at, S)</code> function defined in Algorithm 1. It orchestrates the <code>WSIReader</code> and <code>PyramidLevelSelector</code> to efficiently retrieve a region of interest, resize it to the exact target dimensions using high-quality resampling, and format it for consumption by the LMM (Base64).</p>"},{"location":"specs/spec-05-cropping/#dependencies","title":"Dependencies","text":"<ul> <li>Spec-02: WSI Data Layer &amp; OpenSlide Integration</li> <li>Spec-04: Pyramid Level Selection Algorithm</li> </ul>"},{"location":"specs/spec-05-cropping/#acceptance-criteria","title":"Acceptance Criteria","text":"<ul> <li>[ ] <code>CropEngine</code> class initialized with <code>WSIReader</code>.</li> <li>[ ] <code>crop</code> method accepts <code>Region</code> (L0) and <code>target_size</code> (S).</li> <li>[ ] Uses <code>PyramidLevelSelector</code> to pick the optimal read level.</li> <li>[ ] Reads the region using <code>WSIReader.read_region</code>.</li> <li>[ ] Downsamples the resulting image so the long side equals <code>S</code> (preserving aspect ratio) using <code>PIL.Image.Resampling.LANCZOS</code> (never upsample; if the read image is already \u2264 <code>S</code>, return it unchanged).</li> <li>[ ] Returns a <code>CroppedImage</code> object containing the PIL image and its Base64 representation.</li> <li>[ ] Performance: Does not load full slide into memory.</li> <li>[ ] (Optional) Disk-backed caching (via <code>diskcache</code>) can memoize crops across runs (GIANT\u00d7N / benchmarks).</li> </ul>"},{"location":"specs/spec-05-cropping/#technical-design","title":"Technical Design","text":""},{"location":"specs/spec-05-cropping/#data-models","title":"Data Models","text":"<pre><code>from dataclasses import dataclass\nfrom PIL import Image\n\n@dataclass\nclass CroppedImage:\n    image: Image.Image\n    base64_content: str\n    original_region: Region  # The requested L0 region\n    read_level: int         # The level it was read from\n    scale_factor: float     # How much it was scaled after reading\n</code></pre>"},{"location":"specs/spec-05-cropping/#implementation-details","title":"Implementation Details","text":""},{"location":"specs/spec-05-cropping/#cropenginecropregion-region-target_size-int-1000","title":"<code>CropEngine.crop(region: Region, target_size: int = 1000)</code>","text":"<ol> <li>Level Selection: Call <code>level_selector.select_level(region, metadata, target_size)</code>. Get <code>level_idx</code>.</li> <li>Read: Call <code>wsi_reader.read_region(location=(region.x, region.y), level=level_idx, size=(width_at_level, height_at_level))</code>.<ul> <li>Note: <code>size</code> passed to <code>read_region</code> must be calculated!</li> <li><code>width_at_level = region.width / downsample[level_idx]</code></li> <li><code>height_at_level = region.height / downsample[level_idx]</code></li> <li>These must be cast to <code>int</code>.</li> </ul> </li> <li>Resize:<ul> <li>If the read crop\u2019s long side is greater than <code>target_size</code>, downsample to new dimensions maintaining aspect ratio such that <code>max(new_w, new_h) == target_size</code>.</li> <li>Never upsample: if the read crop\u2019s long side is \u2264 <code>target_size</code>, return it unchanged.</li> <li><code>image.resize((new_w, new_h), resample=Image.Resampling.LANCZOS)</code>.</li> </ul> </li> <li>Encode: Convert to Base64 (JPEG format, quality=85 default).</li> </ol>"},{"location":"specs/spec-05-cropping/#edge-cases","title":"Edge Cases","text":"<ul> <li>Aspect Ratio: The region might be extremely wide or tall. The resize logic must handle this.</li> <li>Rounding Errors: When converting L0 dimension to Level-k dimension, flooring might lose a pixel. This is generally acceptable for LMM context, but we should be consistent.</li> <li>Small Regions: If the crop is smaller than <code>S</code> even at Level-0, do not upsample; return the native-resolution crop as-is.</li> </ul>"},{"location":"specs/spec-05-cropping/#optional-crop-cache-diskcache","title":"Optional: Crop Cache (diskcache)","text":"<p>For long benchmarks and majority-vote runs, enable an on-disk cache to avoid recomputing identical crops. - Cache key: <code>(wsi_path, region.x, region.y, region.width, region.height, target_size, read_level)</code> - Cache value: JPEG bytes (or base64) + metadata (<code>read_level</code>, <code>scale_factor</code>) - Requirements:   - Configurable cache directory and max size   - Safe to disable (default off)</p>"},{"location":"specs/spec-05-cropping/#test-plan","title":"Test Plan","text":""},{"location":"specs/spec-05-cropping/#unit-tests","title":"Unit Tests","text":"<ol> <li>End-to-End Flow: Mock <code>WSIReaderProtocol</code> and <code>LevelSelectorProtocol</code>. Call <code>crop</code>. Verify:<ul> <li><code>level_selector</code> was called.</li> <li><code>wsi_reader.read_region</code> was called with transformed coordinates/size.</li> <li>Image was resized to <code>target_size</code>.</li> <li>Base64 string is valid.</li> </ul> </li> <li>Resize Math: Test the aspect ratio calculation separately.</li> </ol>"},{"location":"specs/spec-05-cropping/#file-structure","title":"File Structure","text":"<pre><code>src/giant/core/\n\u251c\u2500\u2500 crop_engine.py\ntests/unit/core/\n\u2514\u2500\u2500 test_crop_engine.py\n</code></pre>"},{"location":"specs/spec-05.5-wsi-integration-checkpoint/","title":"Spec-05.5: WSI Pipeline Integration Checkpoint","text":""},{"location":"specs/spec-05.5-wsi-integration-checkpoint/#overview","title":"Overview","text":"<p>This is a PAUSE POINT. Before proceeding to Spec-06, complete all integration tests below to catch P0-P4 issues in the WSI pipeline (Specs 02-05).</p> <p>Why pause here? - Specs 02-05 form a complete subsystem: read \u2192 coordinates \u2192 level select \u2192 crop - Bugs here compound exponentially when the LLM loop is added - Real <code>.svs</code> files expose edge cases mocks cannot catch - This is the last chance to fix WSI issues before they're entangled with LLM logic</p>"},{"location":"specs/spec-05.5-wsi-integration-checkpoint/#prerequisites","title":"Prerequisites","text":"<ul> <li>[ ] Spec-02: WSI Data Layer \u2014 merged to main</li> <li>[ ] Spec-03: Coordinates \u2014 merged to main</li> <li>[ ] Spec-04: Level Selection \u2014 merged to main</li> <li>[ ] Spec-05: Cropping \u2014 merged to main</li> <li>[ ] All unit tests passing with \u226590% coverage</li> </ul>"},{"location":"specs/spec-05.5-wsi-integration-checkpoint/#integration-test-checklist","title":"Integration Test Checklist","text":""},{"location":"specs/spec-05.5-wsi-integration-checkpoint/#p0-critical-path-must-pass","title":"P0: Critical Path (Must Pass)","text":"<p>These tests validate the core navigation loop will function.</p> ID Test Command/Steps Expected Status P0-1 Open real SVS file <code>WSIReader(\"test.svs\")</code> with a real Aperio file No exceptions, metadata populated [ ] P0-2 Thumbnail generation <code>reader.get_thumbnail((1024, 1024))</code> Returns RGB PIL Image, correct aspect ratio [ ] P0-3 Read region at L0 <code>reader.read_region((0, 0), level=0, size=(512, 512))</code> Returns 512x512 RGB image [ ] P0-4 Read region at max level <code>reader.read_region((0, 0), level=max_level, size=(256, 256))</code> Returns valid image (may be smaller if at edge) [ ] P0-5 Coordinate roundtrip <code>level_to_level0(level0_to_level(coord, ds), ds)</code> Within \u00b1downsample of original [ ] P0-6 Level selection for target <code>select_level(metadata, target_size=1000)</code> Returns valid level index [ ] P0-7 Crop pipeline end-to-end <code>crop_region(reader, bbox, target_size=1000)</code> Returns correctly sized RGB image [ ]"},{"location":"specs/spec-05.5-wsi-integration-checkpoint/#p1-high-priority-should-pass","title":"P1: High Priority (Should Pass)","text":"<p>Edge cases that will cause subtle bugs in production.</p> ID Test Command/Steps Expected Status P1-1 Boundary crop (right edge) Crop region extending past slide width Graceful handling (clamp or pad) [ ] P1-2 Boundary crop (bottom edge) Crop region extending past slide height Graceful handling (clamp or pad) [ ] P1-3 Tiny region (&lt; target_size) Request 100x100 L0 region with target_size=1000 Returns native resolution unchanged (never upsample) [ ] P1-4 Huge region (entire slide) Request full slide dimensions Falls back to thumbnail or errors gracefully [ ] P1-5 Non-square aspect ratio Crop 1000x100 region Maintains aspect ratio in output [ ] P1-6 Missing MPP metadata Open slide without mpp-x/mpp-y properties <code>mpp_x=None</code>, no crashes [ ] P1-7 Non-power-of-2 downsample Slide with downsample=3.999 Level selection still works [ ]"},{"location":"specs/spec-05.5-wsi-integration-checkpoint/#p2-medium-priority-edge-cases","title":"P2: Medium Priority (Edge Cases)","text":"<p>Less common scenarios that could cause issues.</p> ID Test Command/Steps Expected Status P2-1 Single-level slide Open slide with <code>level_count=1</code> No index errors, level 0 used [ ] P2-2 Unicode path Open slide at path with Unicode chars Opens successfully [ ] P2-3 Symlinked path Open slide via symlink Resolves to real path [ ] P2-4 Very high resolution Slide with dimensions &gt; 100,000 px Coordinate math doesn't overflow [ ] P2-5 Different vendors Test with Aperio (.svs), Hamamatsu (.ndpi), Leica (.scn) All parse metadata correctly [ ]"},{"location":"specs/spec-05.5-wsi-integration-checkpoint/#p3-low-priority-nice-to-have","title":"P3: Low Priority (Nice to Have)","text":"<p>Performance and robustness.</p> ID Test Command/Steps Expected Status P3-1 Memory under load Open 10 slides, read 100 regions each Memory stable, no leaks [ ] P3-2 Concurrent reads Read regions from same slide in parallel Thread-safe or clear error [ ] P3-3 Rapid open/close Open and close same slide 100 times No resource exhaustion [ ] P3-4 Cache efficiency Call <code>get_metadata()</code> 1000 times Returns cached value, &lt;1ms each [ ]"},{"location":"specs/spec-05.5-wsi-integration-checkpoint/#p4-stretch-future-proofing","title":"P4: Stretch (Future-Proofing)","text":"<p>Won't block progress but good to note.</p> ID Test Notes Status P4-1 DICOM WSI support OpenSlide doesn't support DICOM natively Document limitation P4-2 Cloud storage paths S3/GCS URLs Would need separate implementation P4-3 Streaming large regions Avoid loading full region into memory Future optimization"},{"location":"specs/spec-05.5-wsi-integration-checkpoint/#test-data-requirements","title":"Test Data Requirements","text":""},{"location":"specs/spec-05.5-wsi-integration-checkpoint/#minimum-test-set","title":"Minimum Test Set","text":"<p>You need at least ONE real WSI file. Options:</p> <ol> <li> <p>GTEx sample (smallest, ~50-100MB):    <pre><code># Download from GTEx portal or use HuggingFace cache\n</code></pre></p> </li> <li> <p>TCGA sample (~200-500MB):    <pre><code># Requires GDC authentication\n</code></pre></p> </li> <li> <p>OpenSlide test data (~10MB synthetic):    <pre><code>curl -LO https://openslide.cs.cmu.edu/download/openslide-testdata/Aperio/CMU-1-Small-Region.svs\n</code></pre></p> </li> </ol>"},{"location":"specs/spec-05.5-wsi-integration-checkpoint/#recommended-test-set-for-full-coverage","title":"Recommended Test Set (for full coverage)","text":"File Vendor Size Purpose <code>CMU-1-Small-Region.svs</code> Aperio 10MB Basic smoke test <code>CMU-1.svs</code> Aperio 300MB Full-size Aperio <code>test.ndpi</code> Hamamatsu ~200MB Vendor compatibility <code>single-level.tiff</code> Generic ~5MB Edge case: single level"},{"location":"specs/spec-05.5-wsi-integration-checkpoint/#running-integration-tests","title":"Running Integration Tests","text":"<pre><code># Create integration test directory\nmkdir -p tests/integration/wsi\n\n# Run integration tests (requires test data)\nuv run pytest tests/integration/wsi/ -v --tb=long\n\n# Run with specific test file\nWSI_TEST_FILE=/path/to/slide.svs uv run pytest tests/integration/wsi/ -v\n</code></pre>"},{"location":"specs/spec-05.5-wsi-integration-checkpoint/#sign-off-criteria","title":"Sign-Off Criteria","text":"<p>Proceed to Spec-06 when:</p> <ul> <li>[x] All P0 tests pass (2025-12-20)</li> <li>[x] All P1 tests pass (or have documented workarounds) (2025-12-20)</li> <li>[x] P2 tests reviewed (failures documented as known limitations) (2025-12-20)</li> <li>[x] At least ONE real <code>.svs</code> file tested end-to-end (2025-12-20)</li> <li>[x] No memory leaks observed in P3-1 (2025-12-20)</li> <li>[x] Integration test file committed to <code>tests/integration/wsi/</code> (2025-12-20)</li> </ul>"},{"location":"specs/spec-05.5-wsi-integration-checkpoint/#discovered-issues-log","title":"Discovered Issues Log","text":"<p>Document any issues found during integration testing:</p> Date ID Severity Description Resolution 2025-12-20 - - No issues found All 17 tests passed"},{"location":"specs/spec-05.5-wsi-integration-checkpoint/#notes","title":"Notes","text":"<ul> <li>This checkpoint should take 2-4 hours for thorough testing</li> <li>Do NOT skip this checkpoint to \"save time\" \u2014 debugging WSI issues inside the agent loop is 10x harder</li> <li>If you find P0/P1 issues, fix them before proceeding</li> <li>P2-P4 issues can be logged and addressed later</li> </ul>"},{"location":"specs/spec-06-llm-provider/","title":"Spec-06: LLM Provider Abstraction","text":""},{"location":"specs/spec-06-llm-provider/#overview","title":"Overview","text":"<p>This specification defines the abstraction layer for Large Multimodal Models (LMMs). It decouples the core agent logic from specific API implementations (OpenAI, Anthropic). It handles the complexity of constructing multimodal messages, managing rate limits, tracking costs, and robustly parsing structured outputs (reasoning + bounding box) from the models.</p>"},{"location":"specs/spec-06-llm-provider/#dependencies","title":"Dependencies","text":"<ul> <li>Spec-01: Project Foundation &amp; Tooling</li> <li>Spec-03: Coordinate System &amp; Geometry</li> </ul>"},{"location":"specs/spec-06-llm-provider/#acceptance-criteria","title":"Acceptance Criteria","text":"<ul> <li>[x] <code>LLMProvider</code> Protocol defined.</li> <li>[x] <code>OpenAIProvider</code> implemented (default: <code>gpt-5.2</code>).</li> <li>[x] <code>AnthropicProvider</code> implemented (default: <code>claude-sonnet-4-5-20250929</code>).</li> <li>[ ] <code>GoogleProvider</code> for Gemini (P4 - Future, see Spec-XX).</li> <li>[x] Support for multimodal inputs (text + base64 images).</li> <li>[x] Robust parsing of <code>StepResponse</code> (reasoning text + action).</li> <li>[x] Automatic retry logic for API errors and rate limits (using <code>tenacity</code>).</li> <li>[x] Cost tracking per request.</li> </ul> <p>Note: Gemini/Google provider is scaffolded in <code>model_registry.py</code> and <code>pricing.py</code> but the provider implementation (<code>google_client.py</code>) is deferred to a future spec. Config includes <code>GOOGLE_API_KEY</code> placeholder for consistency.</p> <p>Model Registry: See <code>docs/models/model-registry.md</code> for approved frontier models. This diverges from the original paper to use Dec 2025 frontier models.</p>"},{"location":"specs/spec-06-llm-provider/#technical-design","title":"Technical Design","text":""},{"location":"specs/spec-06-llm-provider/#data-models","title":"Data Models","text":"<pre><code>from pydantic import BaseModel, Field\nfrom typing import Literal, Union, List\n\nclass BoundingBoxAction(BaseModel):\n    action_type: Literal[\"crop\"] = \"crop\"\n    x: int\n    y: int\n    width: int\n    height: int\n\nclass FinalAnswerAction(BaseModel):\n    action_type: Literal[\"answer\"] = \"answer\"\n    answer_text: str\n\nclass StepResponse(BaseModel):\n    reasoning: str\n    action: Union[BoundingBoxAction, FinalAnswerAction]\n\nclass MessageContent(BaseModel):\n    type: Literal[\"text\", \"image\"]\n    text: str | None = None\n    image_base64: str | None = None\n    media_type: str = \"image/jpeg\"\n\nclass Message(BaseModel):\n    role: Literal[\"system\", \"user\", \"assistant\"]\n    content: List[MessageContent]\n\nclass TokenUsage(BaseModel):\n    \"\"\"Track token usage and cost per API call.\"\"\"\n    prompt_tokens: int\n    completion_tokens: int\n    total_tokens: int\n    cost_usd: float\n\nclass LLMResponse(BaseModel):\n    \"\"\"Full response including parsed output and metadata.\"\"\"\n    step_response: StepResponse\n    usage: TokenUsage\n    model: str\n    latency_ms: float\n</code></pre>"},{"location":"specs/spec-06-llm-provider/#interfaces","title":"Interfaces","text":"<pre><code>from typing import Protocol\n\nclass LLMProvider(Protocol):\n    async def generate_response(self, messages: List[Message]) -&gt; LLMResponse: ...\n    def get_model_name(self) -&gt; str: ...\n    def get_target_size(self) -&gt; int: ...  # 1000 for OpenAI, 500 for Anthropic\n</code></pre>"},{"location":"specs/spec-06-llm-provider/#cost-calculation","title":"Cost Calculation","text":"<p>Token costs vary by model and image tokens. See <code>docs/models/model-registry.md</code> for SSOT.</p> <pre><code># src/giant/llm/pricing.py\n# FRONTIER MODELS (Dec 2025)\nPRICING_USD_PER_1K = {\n    # Claude Opus 4.5 - Best for coding &amp; agents\n    \"claude-sonnet-4-5-20250929\": {\"input\": 0.005, \"output\": 0.025, \"image_per_1k_px\": 0.00048},\n    # Gemini 3.0 Pro - 1M context, advanced reasoning\n    \"gemini-3-pro-preview\": {\"input\": 0.002, \"output\": 0.012},\n    # GPT-5.2 - 400K context, cost-effective frontier model\n    \"gpt-5.2\": {\"input\": 0.00175, \"output\": 0.014, \"image_base\": 0.00255},\n}\n\ndef calculate_cost(model: str, prompt_tokens: int, completion_tokens: int) -&gt; float:\n    # Unknown models are rejected (see docs/models/model-registry.md)\n    prices = PRICING_USD_PER_1K[model]\n    return (prompt_tokens * prices[\"input\"] + completion_tokens * prices[\"output\"]) / 1000\n</code></pre> <p>Note: Image token costs are approximations. See model-registry.md for current pricing.</p>"},{"location":"specs/spec-06-llm-provider/#implementation-details","title":"Implementation Details","text":""},{"location":"specs/spec-06-llm-provider/#structured-output-strategy","title":"Structured Output Strategy","text":"<p>To ensure reliability and consistent parsing (reasoning + action), use provider-native structured output where available.</p> <ol> <li>OpenAI (SDK v2.x): Use the Responses API with a JSON Schema <code>response_format</code>, then parse into <code>StepResponse</code>.<ul> <li>Request: <code>client.responses.create(model=..., input=[...], response_format={\"type\": \"json_schema\", \"json_schema\": {\"name\": \"StepResponse\", \"schema\": StepResponse.model_json_schema()}})</code></li> <li>Parse: <code>StepResponse.model_validate_json(response.output_text)</code></li> </ul> </li> <li>Anthropic: Use Tool Use. Define a tool <code>submit_step</code> that takes <code>reasoning</code> and <code>action</code> arguments. Force the model to use this tool (<code>tool_choice={\"type\": \"tool\", \"name\": \"submit_step\"}</code>).</li> </ol>"},{"location":"specs/spec-06-llm-provider/#image-resolution-handling","title":"Image Resolution Handling","text":"<p>Per the paper, different providers may require different image resolutions due to cost/performance trade-offs. - <code>OpenAIProvider._get_target_size()</code> -&gt; <code>settings.IMAGE_SIZE_OPENAI</code> (1000px) - <code>AnthropicProvider._get_target_size()</code> -&gt; <code>settings.IMAGE_SIZE_ANTHROPIC</code> (500px)</p>"},{"location":"specs/spec-06-llm-provider/#rate-limiting-retries","title":"Rate Limiting &amp; Retries","text":"<p>Retries: Use <code>tenacity</code> decorator: <pre><code>@retry(\n    wait=wait_random_exponential(min=1, max=60),\n    stop=stop_after_attempt(6),\n    retry=retry_if_exception_type((RateLimitError, APIConnectionError))\n)\n</code></pre></p> <p>Rate limiting (must-have for benchmarks): Use <code>aiolimiter.AsyncLimiter</code> in each provider to cap request throughput and avoid cascading 429s. <pre><code>from aiolimiter import AsyncLimiter\n\nclass OpenAIProvider:\n    def __init__(..., rpm: int = 60):\n        self._limiter = AsyncLimiter(max_rate=rpm, time_period=60)\n\n    async def generate_response(...):\n        async with self._limiter:\n            return await self._call_openai(...)\n</code></pre></p>"},{"location":"specs/spec-06-llm-provider/#circuit-breaker-failure-containment","title":"Circuit Breaker (Failure Containment)","text":"<p>Add a small circuit breaker around API calls: - Open the circuit after <code>N</code> consecutive failures (e.g., 10). - While open, fail fast for <code>cooldown_seconds</code> (e.g., 60) to protect budgets and provider limits. - Transition to half-open and allow a limited number of trial calls to close the circuit on success.</p>"},{"location":"specs/spec-06-llm-provider/#provider-factory-fallback","title":"Provider Factory + Fallback","text":"<p>Implement a <code>ProviderFactory</code> that creates providers from <code>Settings</code> (Dependency Inversion): - <code>ProviderFactory.create(provider: Literal[\"openai\",\"anthropic\"], model: str) -&gt; LLMProvider</code> - Optional <code>FallbackProvider(primary, fallbacks)</code> that retries on retriable exceptions by switching providers/models.</p>"},{"location":"specs/spec-06-llm-provider/#message-construction","title":"Message Construction","text":"<ul> <li>OpenAI (Responses API): Maps to <code>{\"type\": \"input_image\", \"image_url\": f\"data:image/jpeg;base64,{b64}\"}</code> plus <code>{\"type\": \"input_text\", \"text\": ...}</code>.</li> <li>Anthropic: Maps to <code>{\"type\": \"image\", \"source\": {\"type\": \"base64\", \"media_type\": \"image/jpeg\", \"data\": b64}}</code>.</li> </ul>"},{"location":"specs/spec-06-llm-provider/#test-plan","title":"Test Plan","text":""},{"location":"specs/spec-06-llm-provider/#unit-tests","title":"Unit Tests","text":"<ol> <li>Message Formatting: Verify <code>Message</code> objects are correctly converted to provider-specific JSON payloads.</li> <li>Parsing: Mock raw API responses (JSON strings) and verify they parse into <code>StepResponse</code> objects.</li> <li>Error Handling: Mock API errors and verify <code>tenacity</code> retry logic (using <code>wait=0</code> for tests).</li> </ol>"},{"location":"specs/spec-06-llm-provider/#integration-tests","title":"Integration Tests","text":"<ul> <li>Live API Test: (Marked <code>@pytest.mark.cost</code>) A test that actually calls the API with a small dummy image to verify auth and schema alignment.</li> </ul>"},{"location":"specs/spec-06-llm-provider/#file-structure","title":"File Structure","text":"<pre><code>src/giant/llm/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 protocol.py         # LLMProvider, Message, StepResponse, TokenUsage\n\u251c\u2500\u2500 pricing.py          # Cost calculation per model\n\u251c\u2500\u2500 model_registry.py   # Approved models by provider\n\u251c\u2500\u2500 openai_client.py\n\u251c\u2500\u2500 anthropic_client.py\n\u251c\u2500\u2500 google_client.py    # Future (P4) - Gemini provider\n\u2514\u2500\u2500 converters.py       # Helpers for message format conversion\ntests/unit/llm/\n\u251c\u2500\u2500 test_openai.py\n\u251c\u2500\u2500 test_anthropic.py\n\u2514\u2500\u2500 test_pricing.py\n</code></pre>"},{"location":"specs/spec-07-navigation-prompt/","title":"Spec-07: Navigation Prompt Engineering","text":""},{"location":"specs/spec-07-navigation-prompt/#overview","title":"Overview","text":"<p>This specification defines the prompt templates and construction logic used to guide the LMM through the GIANT navigation task. It translates the abstract goal (\"diagnose this slide\") into specific, enforceable instructions for the model, ensuring it understands the coordinate system, the iteration limit, and the required output format.</p>"},{"location":"specs/spec-07-navigation-prompt/#dependencies","title":"Dependencies","text":"<ul> <li>Spec-06: LLM Provider Abstraction</li> </ul>"},{"location":"specs/spec-07-navigation-prompt/#acceptance-criteria","title":"Acceptance Criteria","text":"<ul> <li>[ ] <code>PromptBuilder</code> class is implemented.</li> <li>[ ] System prompt clearly explains the \"Pathologist\" persona, the level-0 coordinate system, and the available tools.</li> <li>[ ] User prompt dynamically inserts the Question and Iteration Status (\"Step X of T\").</li> <li>[ ] Prompt includes specific \"Visual Cues\" instructions (referencing the axis guides).</li> <li>[ ] Few-shot examples (optional but recommended) are defined for coordinate selection.</li> </ul>"},{"location":"specs/spec-07-navigation-prompt/#technical-design","title":"Technical Design","text":""},{"location":"specs/spec-07-navigation-prompt/#note-on-official-prompts","title":"Note on Official Prompts","text":"<p>The paper states: \"The system prompt used for OpenAI and Anthropic models is included in the Supplementary Material.\"</p> <p>When the official prompts become available, replace the templates below. The current templates are reverse-engineered from Algorithm 1: - Include \"at most T-1 crops\" phrasing per Algorithm 1 - Reference P0 = {q, nav instructions + \"at most T-1 crops\"}</p>"},{"location":"specs/spec-07-navigation-prompt/#system-prompt-template","title":"System Prompt Template","text":"<pre><code>You are GIANT (Gigapixel Image Agent for Navigating Tissue), an expert computational pathologist assistant.\n\nYOUR GOAL:\nAnswer the user's question about a Whole Slide Image (WSI) by iteratively examining regions of interest.\n\nTHE IMAGE:\n- You are viewing a gigapixel pathology slide.\n- You cannot see the whole slide in full detail at once.\n- You are currently provided with a view (either the full slide thumbnail or a zoomed-in crop).\n- The thumbnail view has AXIS GUIDES overlaid. These red lines are labeled with ABSOLUTE LEVEL-0 PIXEL COORDINATES (e.g., 10000, 20000).\n- ALL coordinates you output must be in this LEVEL-0 system.\n\nYOUR TOOLS:\n1. `crop(x, y, width, height)`: Zoom into a specific region.\n   - `x`, `y`: Top-left corner in Level-0 coordinates.\n   - `width`, `height`: Dimensions in Level-0 pixels.\n   - Use the axis guides on the thumbnail to estimate these numbers.\n2. `answer(text)`: Provide the final answer to the user's question.\n\nPROCESS:\n1. Analyze the current image. Look for tissue structures relevant to the question.\n2. Provide `reasoning`: Explain what you see and why you need to zoom in or answer.\n3. Choose an `action`:\n   - If you need more detail, choose `crop`.\n   - If you have sufficient evidence, choose `answer`.\n\nCONSTRAINTS:\n- You have a limited number of steps. Make every crop count.\n- If the current view is blurry, it means you are looking at a thumbnail. You MUST zoom in to see cellular details.\n</code></pre>"},{"location":"specs/spec-07-navigation-prompt/#user-prompt-construction","title":"User Prompt Construction","text":"<p>The user prompt updates at every step to maintain state awareness.</p> <p>Initial Prompt (Paper-Faithful): <pre><code>Question: {question}\nStatus: Step 1 of {max_steps}. You have {max_steps - 1} crops remaining.\nInstruction: For Steps 1..{max_steps - 1} you MUST use `crop`. On Step {max_steps} you MUST use `answer`.\nImage: [Thumbnail with Axis Guides]\n</code></pre></p> <p>Edge Case: <code>max_steps=1</code></p> <p>If <code>max_steps</code> is set to <code>1</code>, there are no crop steps. The prompt builder should skip the \u201cSteps 1..T\u22121 must crop\u201d instruction and use the final-step wording immediately (\u201cYou MUST use <code>answer</code> now\u201d).</p> <p>Subsequent Prompts: <pre><code>Status: Step {current_step} of {max_steps}.\nLast Action: You cropped region {last_region}.\nCurrent View: [High-res Crop]\nImage Context: [Thumbnail with Axis Guides (optional/repeated for reference)]\nQuestion: {question}\n</code></pre></p>"},{"location":"specs/spec-07-navigation-prompt/#promptbuilder-class","title":"PromptBuilder Class","text":"<pre><code>class PromptBuilder:\n    def build_system_message(self) -&gt; Message: ...\n    def build_user_message(self, question: str, step: int, max_steps: int, context_images: List[str]) -&gt; Message: ...\n</code></pre>"},{"location":"specs/spec-07-navigation-prompt/#test-plan","title":"Test Plan","text":""},{"location":"specs/spec-07-navigation-prompt/#unit-tests","title":"Unit Tests","text":"<ol> <li>Template Rendering: Verify variables (<code>question</code>, <code>max_steps</code>) are correctly substituted.</li> <li>Instruction Check: Assert that key phrases (\"Level-0\", \"axis guides\") are present in the output.</li> </ol>"},{"location":"specs/spec-07-navigation-prompt/#file-structure","title":"File Structure","text":"<pre><code>src/giant/prompts/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 templates.py    # Raw string templates\n\u2514\u2500\u2500 builder.py      # Logic to construct messages\ntests/unit/prompts/\n\u2514\u2500\u2500 test_builder.py\n</code></pre>"},{"location":"specs/spec-08-context-manager/","title":"Spec-08: Conversation Context Manager","text":""},{"location":"specs/spec-08-context-manager/#overview","title":"Overview","text":"<p>This specification defines the <code>ContextManager</code> which maintains the state of the agent's navigation session. It tracks the sequence of observations (images), thoughts (reasoning), and actions. It is responsible for formatting this history into the list of messages expected by the <code>LLMProvider</code> and managing context window usage (e.g., by pruning old images if configured).</p>"},{"location":"specs/spec-08-context-manager/#dependencies","title":"Dependencies","text":"<ul> <li>Spec-06: LLM Provider Abstraction</li> </ul>"},{"location":"specs/spec-08-context-manager/#acceptance-criteria","title":"Acceptance Criteria","text":"<ul> <li>[ ] <code>ContextManager</code> class is implemented.</li> <li>[ ] <code>add_turn</code> method accepts <code>(image, reasoning, action)</code>.</li> <li>[ ] <code>get_messages</code> returns the full formatted conversation history for the LLM.</li> <li>[ ] <code>Trajectory</code> model stores the full history in a serializable format (JSON).</li> <li>[ ] Configurable \"Image History Limit\" (default: keep all). If exceeded, older images in the history are replaced with placeholder text to save tokens.</li> </ul>"},{"location":"specs/spec-08-context-manager/#technical-design","title":"Technical Design","text":""},{"location":"specs/spec-08-context-manager/#data-models","title":"Data Models","text":"<pre><code>from pydantic import BaseModel\nfrom typing import List, Optional\nfrom giant.llm.protocol import StepResponse, Message\nfrom giant.geometry.primitives import Region\n\nclass Turn(BaseModel):\n    step_index: int\n    image_base64: str  # The crop seen at this step\n    response: StepResponse\n    region: Optional[Region] = None # The region that was cropped to get here\n\nclass Trajectory(BaseModel):\n    wsi_path: str\n    question: str\n    turns: List[Turn] = []\n    final_answer: Optional[str] = None\n</code></pre>"},{"location":"specs/spec-08-context-manager/#context-management-logic","title":"Context Management Logic","text":"<p>The <code>get_messages</code> method constructs the prompt sent to the LLM. Structure: 1.  System Message (from PromptBuilder). 2.  User Message (Turn 0): Initial Query + Thumbnail. 3.  Assistant Message (Turn 0): Reasoning + Action (Crop 1). 4.  User Message (Turn 1): \"Here is the crop...\" + Crop 1 Image. 5.  ...</p> <p>Token Optimization: If <code>max_history_images</code> is set (e.g., to 5) and we are at step 10: - Keep the Thumbnail (always). - Keep the last 5 crops (Step 5-9). - For Steps 1-4, replace the <code>image_base64</code> in the User Message with text <code>[Image from Step X removed to save context]</code>. The reasoning/action text remains.</p>"},{"location":"specs/spec-08-context-manager/#visualization-support","title":"Visualization Support","text":"<p>The <code>Trajectory</code> object can be dumped to JSON. This JSON is the input for the future <code>giant visualize</code> command.</p>"},{"location":"specs/spec-08-context-manager/#test-plan","title":"Test Plan","text":""},{"location":"specs/spec-08-context-manager/#unit-tests","title":"Unit Tests","text":"<ol> <li>History Tracking: Add 3 turns. Verify <code>get_messages</code> produces alternating User/Assistant messages.</li> <li>Pruning: Set <code>max_history_images=1</code>. Add 3 turns. Verify <code>get_messages</code> contains only the most recent image and the thumbnail (if treated special), and others are placeholders.</li> <li>Serialization: Verify <code>Trajectory</code> dumps to valid JSON and loads back.</li> </ol>"},{"location":"specs/spec-08-context-manager/#file-structure","title":"File Structure","text":"<pre><code>src/giant/agent/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 context.py      # ContextManager implementation\n\u2514\u2500\u2500 trajectory.py   # Data models\ntests/unit/agent/\n\u2514\u2500\u2500 test_context.py\n</code></pre>"},{"location":"specs/spec-08.5-llm-integration-checkpoint/","title":"Spec-08.5: LLM Pipeline Integration Checkpoint","text":""},{"location":"specs/spec-08.5-llm-integration-checkpoint/#overview","title":"Overview","text":"<p>This is a PAUSE POINT. Before proceeding to Spec-09 (GIANT Agent), complete all integration tests below to catch P0-P4 issues in the LLM pipeline (Specs 06-08).</p> <p>Why pause here? - Specs 06-08 form a complete subsystem: provider \u2192 prompts \u2192 context management - Spec-09 merges WSI + LLM pipelines \u2014 debugging becomes exponentially harder - API edge cases (rate limits, token overflows, malformed responses) must be caught now - Cost estimation bugs here lead to real money wasted in production</p>"},{"location":"specs/spec-08.5-llm-integration-checkpoint/#prerequisites","title":"Prerequisites","text":"<ul> <li>[ ] Spec-05.5: WSI Integration Checkpoint \u2014 PASSED</li> <li>[ ] Spec-06: LLM Provider Abstraction \u2014 merged to main</li> <li>[ ] Spec-07: Navigation Prompt Engineering \u2014 merged to main</li> <li>[ ] Spec-08: Conversation Context Manager \u2014 merged to main</li> <li>[ ] All unit tests passing with \u226590% coverage</li> </ul>"},{"location":"specs/spec-08.5-llm-integration-checkpoint/#integration-test-checklist","title":"Integration Test Checklist","text":""},{"location":"specs/spec-08.5-llm-integration-checkpoint/#p0-critical-path-must-pass","title":"P0: Critical Path (Must Pass)","text":"<p>These tests validate the agent loop can function.</p> ID Test Command/Steps Expected Status P0-1 OpenAI provider init <code>OpenAIProvider(api_key=...)</code> Initializes without error [ ] P0-2 Anthropic provider init <code>AnthropicProvider(api_key=...)</code> Initializes without error [ ] P0-3 Send text message <code>provider.generate(messages=[text_only])</code> Returns valid response [ ] P0-4 Send image message <code>provider.generate(messages=[with_image])</code> Returns valid response with image analysis [ ] P0-5 System prompt applied Build system message, verify in API call System message in first position [ ] P0-6 User prompt construction <code>build_user_message(question, step, max_steps)</code> Contains question, step count, instructions [ ] P0-7 Context accumulation Add 3 turns to ContextManager <code>get_messages()</code> returns alternating user/assistant [ ] P0-8 Parse crop action Model returns <code>crop(x, y, w, h)</code> Parses to Region object [ ] P0-9 Parse answer action Model returns <code>answer(text)</code> Extracts final answer [ ]"},{"location":"specs/spec-08.5-llm-integration-checkpoint/#p1-high-priority-should-pass","title":"P1: High Priority (Should Pass)","text":"<p>Edge cases that will cause production failures.</p> ID Test Command/Steps Expected Status P1-1 Rate limit handling Trigger 429 response Retry with backoff, eventually succeeds [ ] P1-2 Token limit approach Build context near max tokens Warns or truncates gracefully [ ] P1-3 Image pruning Set <code>max_history_images=3</code>, add 5 turns Older images replaced with placeholder [ ] P1-4 Malformed model response Mock response without valid action Returns error, doesn't crash [ ] P1-5 Empty model response Mock empty string response Returns error, doesn't crash [ ] P1-6 Invalid coordinates in crop Model returns <code>crop(-100, -100, 500, 500)</code> Validation catches, returns error [ ] P1-7 Cost tracking Run 5 API calls Accumulated cost matches expected [ ] P1-8 Provider switching Same context, different provider Both produce valid responses [ ]"},{"location":"specs/spec-08.5-llm-integration-checkpoint/#p2-medium-priority-edge-cases","title":"P2: Medium Priority (Edge Cases)","text":"<p>Less common scenarios that could cause issues.</p> ID Test Command/Steps Expected Status P2-1 Very long question Question &gt; 1000 tokens Truncated or error, doesn't crash [ ] P2-2 Unicode in question Question with emoji, CJK chars Handles correctly [ ] P2-3 20 iteration context Full 20-step conversation Context builds correctly, token count reasonable [ ] P2-4 Trajectory serialization Dump trajectory to JSON, reload Roundtrips perfectly [ ] P2-5 Concurrent API calls 3 simultaneous generate() calls All complete (may be rate limited) [ ] P2-6 API timeout Mock 30s+ response time Times out gracefully [ ] P2-7 Network error Mock connection failure Retries, then clean error [ ]"},{"location":"specs/spec-08.5-llm-integration-checkpoint/#p3-low-priority-nice-to-have","title":"P3: Low Priority (Nice to Have)","text":"<p>Performance and robustness.</p> ID Test Command/Steps Expected Status P3-1 Token estimation accuracy Compare estimated vs actual usage Within 10% [ ] P3-2 Cost estimation accuracy Compare estimated vs actual cost Within 5% [ ] P3-3 Prompt caching (Anthropic) Same system prompt, multiple calls Cache hits logged [ ] P3-4 Response streaming If implemented, verify stream works Chunks arrive incrementally [ ]"},{"location":"specs/spec-08.5-llm-integration-checkpoint/#p4-stretch-future-proofing","title":"P4: Stretch (Future-Proofing)","text":"<p>Won't block progress but good to note.</p> ID Test Notes Status P4-1 Gemini provider Not in current spec Document as future work P4-2 Local LLM (Ollama) Would need separate provider Future enhancement P4-3 Function calling Structured output vs regex parsing Consider for v2"},{"location":"specs/spec-08.5-llm-integration-checkpoint/#api-key-requirements","title":"API Key Requirements","text":"<p>You need valid API keys for live integration testing.</p>"},{"location":"specs/spec-08.5-llm-integration-checkpoint/#setup","title":"Setup","text":"<pre><code># Copy the template and add your keys\ncp .env.example .env\n\n# Edit .env with your actual keys:\n# OPENAI_API_KEY=sk-...\n# ANTHROPIC_API_KEY=sk-ant-...\n</code></pre> <p>WARNING: Shell Environment Behavior</p> <p>Tests use <code>os.getenv()</code> which reads from BOTH your shell environment AND <code>.env</code> file. If you have API keys exported in <code>~/.zshrc</code> or <code>~/.bashrc</code>, live tests will run automatically even without a <code>.env</code> file. Use <code>-m mock</code> to skip live tests.</p> <p>Cost estimate for full live test run: ~$0.50-2.00 (mostly from image tokens)</p>"},{"location":"specs/spec-08.5-llm-integration-checkpoint/#test-modes","title":"Test Modes","text":"Mode Command API Calls Cost Mock only <code>pytest -m mock</code> None Free All tests <code>pytest</code> If keys set ~$0.50-2.00 Live only <code>pytest -m live</code> Yes ~$0.50-2.00"},{"location":"specs/spec-08.5-llm-integration-checkpoint/#running-integration-tests","title":"Running Integration Tests","text":"<pre><code># RECOMMENDED: Run mock tests (safe, no API calls, no cost)\nuv run pytest tests/integration/llm/ -v -m \"mock\"\n\n# Run ALL tests (live tests run if keys are set)\nuv run pytest tests/integration/llm/ -v --tb=long\n\n# Run ONLY live API tests (opt-in, costs money)\nuv run pytest tests/integration/llm/test_p0_critical.py -v -m \"live\"\n</code></pre>"},{"location":"specs/spec-08.5-llm-integration-checkpoint/#mock-mode-for-cicd","title":"Mock Mode for CI/CD","text":"<p>For CI/CD or cost-sensitive testing, use <code>respx</code> to mock HTTP calls:</p> <pre><code>import respx\n\n@respx.mock\ndef test_openai_generate():\n    respx.post(\"https://api.openai.com/v1/responses\").respond(\n        json={\"output_text\": '{\"reasoning\": \"...\", \"action\": {...}}'}\n    )\n    # ... test code\n</code></pre>"},{"location":"specs/spec-08.5-llm-integration-checkpoint/#sign-off-criteria","title":"Sign-Off Criteria","text":"<p>Proceed to Spec-09 when:</p> <ul> <li>[x] All P0 tests pass with BOTH OpenAI and Anthropic (2025-12-20)</li> <li>[x] All P1 tests pass (or have documented workarounds) (2025-12-20)</li> <li>[x] P2 tests reviewed (failures documented as known limitations) (2025-12-20)</li> <li>[x] At least ONE real API call made to each provider (2025-12-20)</li> <li>[x] Cost tracking verified accurate (2025-12-20)</li> <li>[x] Rate limit handling verified (2025-12-20)</li> <li>[x] Trajectory JSON serialization works end-to-end (2025-12-20)</li> <li>[x] Integration test file committed to <code>tests/integration/llm/</code> (2025-12-20)</li> </ul>"},{"location":"specs/spec-08.5-llm-integration-checkpoint/#discovered-issues-log","title":"Discovered Issues Log","text":"<p>Document any issues found during integration testing:</p> Date ID Severity Description Resolution 2025-12-20 - - No issues found 61 mock + 2 live tests passed"},{"location":"specs/spec-08.5-llm-integration-checkpoint/#pre-flight-check-for-spec-09","title":"Pre-Flight Check for Spec-09","text":"<p>Before starting Spec-09, verify:</p> <ol> <li>WSI Pipeline (Spec-05.5): Can open slide, get thumbnail, crop regions</li> <li>LLM Pipeline (this checkpoint): Can send messages, parse responses, track context</li> <li>Both pipelines tested independently with real data/APIs</li> <li>Cost estimation working \u2014 you'll burn money fast in the agent loop</li> </ol>"},{"location":"specs/spec-08.5-llm-integration-checkpoint/#notes","title":"Notes","text":"<ul> <li>This checkpoint should take 2-4 hours for thorough testing</li> <li>Budget ~$2-5 for API calls during integration testing</li> <li>Do NOT skip rate limit testing \u2014 it WILL happen in production</li> <li>If you find P0/P1 issues, fix them before proceeding to Spec-09</li> <li>The agent loop (Spec-09) amplifies every bug \u2014 find them now</li> </ul>"},{"location":"specs/spec-09-giant-agent/","title":"Spec-09: GIANT Agent Core Loop","text":""},{"location":"specs/spec-09-giant-agent/#overview","title":"Overview","text":"<p>This specification implements the main <code>GIANTAgent</code> class. This is the high-level controller that orchestrates the entire navigation process defined in Algorithm 1. It integrates the WSI data layer, the LLM provider, and the context manager to autonomously explore a slide and answer a question.</p>"},{"location":"specs/spec-09-giant-agent/#dependencies","title":"Dependencies","text":"<ul> <li>Spec-05: Image Cropping &amp; Resampling Pipeline</li> <li>Spec-06: LLM Provider Abstraction</li> <li>Spec-07: Navigation Prompt Engineering</li> <li>Spec-08: Conversation Context Manager</li> </ul>"},{"location":"specs/spec-09-giant-agent/#acceptance-criteria","title":"Acceptance Criteria","text":"<ul> <li>[x] <code>GIANTAgent</code> class is implemented.</li> <li>[x] <code>run()</code> method executes the navigation loop up to <code>max_steps</code>.</li> <li>[x] Correctly handles the initial \"Thumbnail\" step.</li> <li>[x] Correctly executes \"Crop\" actions using the <code>CropEngine</code>.</li> <li>[x] Correctly handles \"Answer\" actions (early stopping).</li> <li>[x] Returns a <code>RunResult</code> object containing the final answer and the full <code>Trajectory</code>.</li> <li>[x] Error handling: If LLM produces invalid coordinates or fails repeatedly, the agent should degrade gracefully (e.g., stop and return partial info).</li> </ul>"},{"location":"specs/spec-09-giant-agent/#technical-design","title":"Technical Design","text":""},{"location":"specs/spec-09-giant-agent/#data-models","title":"Data Models","text":"<pre><code>from pydantic import BaseModel\nfrom giant.agent.trajectory import Trajectory\n\nclass RunResult(BaseModel):\n    answer: str\n    trajectory: Trajectory\n    total_tokens: int\n    total_cost: float\n    success: bool\n    error_message: str | None = None\n</code></pre>"},{"location":"specs/spec-09-giant-agent/#core-logic-giantagentrun","title":"Core Logic (<code>GIANTAgent.run</code>)","text":"<ol> <li> <p>Initialize:</p> <ul> <li>Open WSI with <code>WSIReader</code>.</li> <li>Create <code>ContextManager</code>.</li> <li>Generate a 1024px Thumbnail + Axis Guides (<code>spec-03</code>, paper baseline uses 1024\u00d71024 thumbnails).</li> <li>Add Thumbnail to Context (Turn 0).</li> </ul> </li> <li> <p>Navigation Loop (t = 1 to T\u22121):</p> <ul> <li>Prompt: <code>messages = context_manager.get_messages()</code>.</li> <li>LLM Call: <code>response = await llm_provider.generate_response(messages)</code>.</li> <li>Enforce crop: At steps 1..T\u22121, the model MUST return <code>BoundingBoxAction</code>.<ul> <li>If model returns <code>FinalAnswer</code> early, log warning but accept (early termination).</li> </ul> </li> <li>Validate: Validate bbox using <code>GeometryValidator</code> (strict by default; clamp only as an explicit, test-covered recovery path).</li> <li>Act: <code>cropped_img = crop_engine.crop(region, target_size=llm_provider.get_target_size())</code>.<ul> <li>Paper fidelity: Use 1000px crops for OpenAI models and 500px crops for Anthropic models (paper note on Claude image pricing).</li> </ul> </li> <li>Record: Append <code>(reasoning, action, cropped_img)</code> to Context/Trajectory.</li> </ul> </li> <li> <p>Final Answer (t = T):</p> <ul> <li>Make one final LLM call with full context, requiring <code>FinalAnswerAction</code>.</li> <li>Enforcement: If model attempts another crop at step T:<ul> <li>Send corrective message: \"You have reached the maximum navigation steps. You MUST provide your final answer now using the <code>answer</code> action.\"</li> <li>Retry up to 3 times.</li> <li>After 3 failures: terminate with <code>success=False, error_message=\"Exceeded step limit after 3 retries\"</code>.</li> <li>For benchmarking: This counts as incorrect (paper evaluation policy).</li> </ul> </li> <li>Return <code>RunResult</code>.</li> </ul> </li> </ol>"},{"location":"specs/spec-09-giant-agent/#budget-cost-guardrails-production-readiness","title":"Budget / Cost Guardrails (Production-Readiness)","text":"<p>To prevent runaway costs during long runs (benchmarks or retries), <code>GIANTAgent.run()</code> should accept an optional <code>budget_usd: float | None</code>. - If <code>total_cost &gt;= budget_usd</code> at any point, force finalization:   - Add a User message using <code>FORCE_ANSWER_TEMPLATE</code> noting the budget was reached.   - Make a final LLM call requiring <code>FinalAnswerAction</code>.   - Return <code>success=False</code> with <code>error_message=\"Budget exceeded\"</code> (benchmark counts as incorrect).</p>"},{"location":"specs/spec-09-giant-agent/#error-handling","title":"Error Handling","text":"<ul> <li>Invalid Coordinate Loop: If the LLM tries to crop outside the image, catch the <code>ValidationError</code>, feed it back to the LLM as a User message using the template below, and retry.</li> <li>Max Retries: If it fails 3 times in a row on the same step, terminate with partial result.</li> <li>Retry Counter: Track <code>consecutive_errors: int</code> and reset to 0 on successful action.</li> </ul>"},{"location":"specs/spec-09-giant-agent/#error-feedback-template","title":"Error Feedback Template","text":"<pre><code>ERROR_FEEDBACK_TEMPLATE = \"\"\"\nError: Your crop coordinates were invalid.\nRequested region: x={x}, y={y}, width={width}, height={height}\nImage bounds: width={max_width}, height={max_height}\n\nIssues:\n{issues}\n\nPlease re-examine the axis guides on the thumbnail and provide valid Level-0 coordinates.\nYour coordinates must satisfy:\n- 0 &lt;= x &lt; {max_width}\n- 0 &lt;= y &lt; {max_height}\n- x + width &lt;= {max_width}\n- y + height &lt;= {max_height}\n\"\"\"\n</code></pre>"},{"location":"specs/spec-09-giant-agent/#force-answer-template-max-steps-reached","title":"Force-Answer Template (Max Steps Reached)","text":"<pre><code>FORCE_ANSWER_TEMPLATE = \"\"\"\nYou have reached the maximum number of navigation steps ({max_steps}).\nBased on all the regions you have examined, you MUST now provide your final answer.\n\nReview your observations:\n{observation_summary}\n\nQuestion: {question}\n\nProvide your best answer using the `answer` action.\n\"\"\"\n</code></pre>"},{"location":"specs/spec-09-giant-agent/#test-plan","title":"Test Plan","text":""},{"location":"specs/spec-09-giant-agent/#unit-tests-mocked-llm","title":"Unit Tests (Mocked LLM)","text":"<ol> <li>Happy Path: Mock LLM to return <code>Crop</code> -&gt; <code>Crop</code> -&gt; <code>Answer</code>. Verify 3 steps in trajectory.</li> <li>Early Stop: Mock LLM to return <code>Answer</code> immediately.</li> <li>Loop Limit: Mock LLM to always <code>Crop</code>. Verify it stops at <code>max_steps</code> and force-answer is invoked.</li> <li>Error Recovery: Mock LLM to return invalid coords, then valid coords. Verify retry works.</li> <li>Max Retries: Mock LLM to return invalid coords 3 times. Verify graceful termination.</li> <li>Force Answer Content: Verify force-answer prompt includes observation summary.</li> </ol>"},{"location":"specs/spec-09-giant-agent/#integration-tests","title":"Integration Tests","text":"<ul> <li>\"Dry Run\": Run with a real <code>OpenAIProvider</code> (if configured) but a dummy image/mocked WSI to test the full message passing pipeline (without spending heavy credits on vision).</li> </ul>"},{"location":"specs/spec-09-giant-agent/#file-structure","title":"File Structure","text":"<pre><code>src/giant/agent/\n\u251c\u2500\u2500 runner.py       # GIANTAgent implementation\ntests/unit/agent/\n\u2514\u2500\u2500 test_runner.py\n</code></pre>"},{"location":"specs/spec-10-evaluation/","title":"Spec-10: Evaluation &amp; Benchmarking Framework","text":""},{"location":"specs/spec-10-evaluation/#overview","title":"Overview","text":"<p>This specification defines the tooling to evaluate GIANT against the MultiPathQA benchmark used in the paper. It includes the logic to load benchmark items, run the agent in batch mode, calculate metrics (Accuracy, Balanced Accuracy), and compute bootstrap uncertainty estimates. Paper reporting format: mean \u00b1 standard deviation from 1000 bootstrap replicates (see Table 1).</p> <p>First-principles note (HuggingFace release): As of 2025-11-26, MultiPathQA is available as a single CSV at <code>tbuckley/MultiPathQA</code> (<code>MultiPathQA.csv</code>). The CSV contains question/prompt metadata and slide IDs/filenames; WSIs themselves may need to be acquired separately and placed under a user-provided <code>--wsi-root</code>.</p>"},{"location":"specs/spec-10-evaluation/#dependencies","title":"Dependencies","text":"<ul> <li>Spec-09: GIANT Agent Core Loop</li> </ul>"},{"location":"specs/spec-10-evaluation/#acceptance-criteria","title":"Acceptance Criteria","text":"<ul> <li>[x] <code>BenchmarkRunner</code> class is implemented.</li> <li>[x] Supports loading MultiPathQA metadata from <code>MultiPathQA.csv</code> (HuggingFace) and/or a local CSV/JSON export.</li> <li>[x] Requires a <code>wsi_root</code> input and resolves each <code>image_path</code> to a local WSI file (clear error if missing).</li> <li>[x] Implements dataset-aware answer extraction (1-based option indices, PANDA JSON <code>isup_grade</code>, GTEx label\u2192index mapping).</li> <li>[x] Supports <code>runs_per_item &gt;= 1</code> with majority voting (paper's GIANT\u00d75 is <code>runs_per_item=5</code>).</li> <li>[x] <code>MetricsCalculator</code> computes Accuracy and Balanced Accuracy.</li> <li>[x] <code>BootstrapEvaluator</code> computes mean \u00b1 std dev using 1000 bootstrap replicates (paper reporting format).</li> <li>[x] (Optional) <code>BootstrapEvaluator</code> can also compute 95% percentile intervals for internal analysis.</li> <li>[x] Results are saved to a structured JSON file with full provenance.</li> <li>[x] Supports resuming interrupted runs.</li> </ul>"},{"location":"specs/spec-10-evaluation/#technical-design","title":"Technical Design","text":""},{"location":"specs/spec-10-evaluation/#data-models","title":"Data Models","text":"<pre><code>from pydantic import BaseModel\nfrom typing import List, Dict, Any\n\nclass BenchmarkItem(BaseModel):\n    benchmark_name: str   # e.g., tcga, gtex, panda, tcga_expert_vqa, tcga_slidebench\n    benchmark_id: str     # row identifier / slide identifier\n    image_path: str       # filename from MultiPathQA.csv (resolved under --wsi-root)\n    prompt: str           # question prompt template from CSV (may include {options})\n    options: List[str] | None = None\n    metric_type: str      # \"accuracy\" or \"balanced_accuracy\" (from CSV)\n    truth_label: int      # canonicalized label for evaluation (see Answer Extraction)\n    wsi_path: str         # resolved local path to WSI\n\nclass BenchmarkResult(BaseModel):\n    item_id: str\n    prediction: str\n    predicted_label: int | None = None\n    truth_label: int\n    correct: bool\n    cost_usd: float = 0.0\n    total_tokens: int = 0\n    trajectory_file: str  # Path to saved trajectory JSON\n    error: str | None = None\n</code></pre>"},{"location":"specs/spec-10-evaluation/#implementation-details","title":"Implementation Details","text":""},{"location":"specs/spec-10-evaluation/#data-acquisition-module","title":"Data Acquisition Module","text":"<p>We use <code>datasets</code> and <code>huggingface_hub</code> to manage benchmark data.</p> <pre><code># src/giant/data/download.py\nfrom huggingface_hub import hf_hub_download\nfrom pathlib import Path\nfrom rich.progress import Progress, SpinnerColumn, TextColumn\nfrom giant.config import settings\nimport os\n\n# Paper states: \"We release the dataset publicly on HuggingFace\"\nMULTIPATHQA_REPO = \"tbuckley/MultiPathQA\"  # Verified via HuggingFace API (2025-11-26)\nMULTIPATHQA_FILENAME = \"MultiPathQA.csv\"\n\n# Dataset structure from paper (Figure 3 table) and confirmed counts in the CSV:\nBENCHMARK_TASKS = {\n    \"tcga\": {\"questions\": 221, \"classes\": 30, \"metric\": \"balanced_accuracy\"},\n    \"panda\": {\"questions\": 197, \"classes\": 6, \"metric\": \"balanced_accuracy\"},\n    \"gtex\": {\"questions\": 191, \"classes\": 20, \"metric\": \"balanced_accuracy\"},\n    \"tcga_expert_vqa\": {\"questions\": 128, \"metric\": \"accuracy\"},\n    \"tcga_slidebench\": {\"questions\": 197, \"metric\": \"accuracy\"},\n}\n\ndef download_multipathqa(output_dir: Path) -&gt; Path:\n    \"\"\"Download MultiPathQA CSV metadata from HuggingFace.\n\n    NOTE: This downloads the question metadata CSV, not the WSIs themselves.\n    \"\"\"\n    os.environ[\"HF_TOKEN\"] = settings.HUGGINGFACE_TOKEN or \"\"\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    with Progress(SpinnerColumn(), TextColumn(\"[progress.description]{task.description}\")) as progress:\n        task = progress.add_task(\"Downloading MultiPathQA.csv...\", total=None)\n        csv_path = hf_hub_download(\n            repo_id=MULTIPATHQA_REPO,\n            repo_type=\"dataset\",\n            filename=MULTIPATHQA_FILENAME,\n            local_dir=output_dir,\n            local_dir_use_symlinks=False,\n        )\n        progress.update(task, completed=True)\n\n    return Path(csv_path)\n\ndef download_sample_wsi(output_dir: Path) -&gt; Path:\n    \"\"\"Download sample WSI for testing (small file).\n\n    Uses OpenSlide test data (CMU-1.svs, ~30MB).\n    \"\"\"\n    OPENSLIDE_TESTDATA_URL = \"https://openslide.cs.cmu.edu/download/openslide-testdata/Aperio/CMU-1.svs\"\n    import httpx\n\n    output_path = output_dir / \"CMU-1.svs\"\n    if not output_path.exists():\n        # Production-readiness: check available disk space before downloading large assets.\n        with Progress(SpinnerColumn(), TextColumn(\"[progress.description]{task.description}\")) as progress:\n            task = progress.add_task(\"Downloading sample WSI...\", total=None)\n            response = httpx.get(OPENSLIDE_TESTDATA_URL, follow_redirects=True)\n            output_path.write_bytes(response.content)\n            progress.update(task, completed=True)\n\n    return output_path\n</code></pre>"},{"location":"specs/spec-10-evaluation/#dataset-loader","title":"Dataset Loader","text":"<ul> <li>Loads from a standard directory structure or a JSON manifest.</li> <li>Loads <code>MultiPathQA.csv</code> and filters <code>is_valid == True</code>.</li> <li>Resolves <code>image_path</code> to a local WSI path under a required <code>wsi_root</code> directory.</li> <li>Resolution strategy (robust):<ul> <li>Try <code>wsi_root / image_path</code> (flat layout)</li> <li>Then try <code>wsi_root / wsi_subdir / image_path</code> (recommended layout), where:</li> <li><code>tcga</code>, <code>tcga_expert_vqa</code>, <code>tcga_slidebench</code> \u2192 <code>tcga/</code></li> <li>All others \u2192 <code>&lt;benchmark_name&gt;/</code></li> <li>For TCGA, also support the default <code>gdc-client</code> layout: <code>wsi_root / tcga / &lt;file_id&gt; / &lt;downloaded file_name&gt;</code>.</li> <li>Otherwise fail with a clear error.</li> </ul> </li> <li>Parses <code>options</code> from the CSV (when present) and substitutes <code>{options}</code> into <code>prompt</code> before sending to the agent.</li> <li>The public MultiPathQA CSV encodes options as Python list literals (single quotes), not strict JSON.</li> <li>Loader must support: JSON lists (<code>[\"A\", \"B\"]</code>), Python literal lists (<code>['A', 'B']</code>), and pipe-delimited fixtures (<code>A|B</code>).</li> <li>If <code>options</code> exist and <code>{options}</code> is present, substitute it with a formatted 1-based list.</li> <li>If <code>options</code> exist and <code>{options}</code> is missing, append a standardized options block and instruct the model to respond with the 1-based option index.</li> <li>Canonicalizes <code>truth_label</code>:</li> <li>If CSV <code>answer</code> is an integer string: <code>truth_label = int(answer)</code> (MultiPathQA uses 1-based indices for options tasks).</li> <li>If CSV <code>answer</code> is a string label (GTEx): <code>truth_label = options.index(answer) + 1</code>.</li> </ul>"},{"location":"specs/spec-10-evaluation/#wsi-acquisition-local-layout-operational-requirement","title":"WSI Acquisition &amp; Local Layout (Operational Requirement)","text":"<p>MultiPathQA CSV rows reference slide filenames in <code>image_path</code>: - TCGA-derived benchmarks use <code>.svs</code> - GTEx and PANDA use <code>.tiff</code></p> <p>Because WSIs may not be redistributed via HuggingFace, evaluation requires the operator to populate <code>--wsi-root</code> with the referenced files.</p> <p>Recommended layout: <pre><code>wsi_root/\n\u251c\u2500\u2500 gtex/*.tiff\n\u251c\u2500\u2500 panda/*.tiff\n\u2514\u2500\u2500 tcga/*.svs\n</code></pre></p> <p>The loader should also work if all files are placed directly under <code>wsi_root/</code>.</p>"},{"location":"specs/spec-10-evaluation/#batch-execution","title":"Batch Execution","text":"<ul> <li>Use <code>asyncio</code> for concurrent LLM calls with a bounded semaphore.</li> <li>Use provider-level rate limiting (<code>aiolimiter</code>, Spec-06) to stay within RPM limits.</li> <li>Keep OpenSlide reads inside the agent and avoid reading full slides; use thread offload only if profiling shows blocking impacts throughput.</li> </ul>"},{"location":"specs/spec-10-evaluation/#answer-extraction-matching-multipathqa-csv-faithful","title":"Answer Extraction &amp; Matching (MultiPathQA CSV-Faithful)","text":"<p>MultiPathQA truth labels are heterogeneous: - <code>tcga</code>, <code>tcga_expert_vqa</code>, <code>tcga_slidebench</code>: <code>answer</code> is a 1-based option index in the CSV. - <code>panda</code>: <code>answer</code> is the ISUP grade <code>0..5</code>; prompt expects a JSON object containing <code>\"isup_grade\"</code>. - <code>gtex</code>: <code>answer</code> is a string label; options list contains the 20 organ names.</p> <p>Implement dataset-aware extraction to canonicalize predictions into an <code>int</code> label for metrics.</p> <pre><code># src/giant/eval/answer_extraction.py\nimport json\nimport re\nfrom dataclasses import dataclass\n\n_INT_RE = re.compile(r\"\\b(\\d+)\\b\")\n_LETTER_RE = re.compile(r\"\\b([A-D])\\b\", re.IGNORECASE)\n\n@dataclass(frozen=True)\nclass ExtractedAnswer:\n    label: int | None\n    raw: str\n\ndef extract_label(prediction: str, *, benchmark_name: str, options: list[str] | None) -&gt; ExtractedAnswer:\n    \\\"\\\"\\\"Return a canonical integer label for scoring.\n\n    Conventions:\n    - Multiple-choice tasks use 1-based labels (matches MultiPathQA CSV).\n    - PANDA uses isup_grade 0..5.\n    \\\"\\\"\\\"\n    text = prediction.strip()\n\n    if benchmark_name == \"panda\":\n        # Extract JSON object and read isup_grade\n        try:\n            obj = json.loads(_extract_json_object(text))\n            grade = int(obj[\"isup_grade\"])\n            return ExtractedAnswer(label=grade, raw=text)\n        except Exception:\n            # fall through to integer extraction\n            pass\n\n    # If options exist, accept letter (A-D), 1..N integer, or option text match.\n    if options:\n        m = _LETTER_RE.search(text)\n        if m and len(options) == 4:\n            return ExtractedAnswer(label=(ord(m.group(1).upper()) - ord(\"A\") + 1), raw=text)\n\n        m = _INT_RE.search(text)\n        if m:\n            k = int(m.group(1))\n            if 1 &lt;= k &lt;= len(options):\n                return ExtractedAnswer(label=k, raw=text)\n\n        lowered = text.lower()\n        for i, opt in enumerate(options, start=1):\n            if opt.lower() in lowered:\n                return ExtractedAnswer(label=i, raw=text)\n\n    # No options: try integer extraction (e.g., PANDA grade)\n    m = _INT_RE.search(text)\n    return ExtractedAnswer(label=(int(m.group(1)) if m else None), raw=text)\n\n\ndef _extract_json_object(text: str) -&gt; str:\n    # Naive but practical: find outermost {...}\n    start = text.find(\"{\")\n    end = text.rfind(\"}\")\n    if start == -1 or end == -1 or end &lt;= start:\n        raise ValueError(\"No JSON object found\")\n    return text[start : end + 1]\n</code></pre>"},{"location":"specs/spec-10-evaluation/#bootstrapping-paper-faithful","title":"Bootstrapping (Paper-Faithful)","text":"<p>Paper Reference: Table 1 reports metrics as \"value \u00b1 std\" from 1000 bootstrap replicates.</p> <p>Algorithm: 1.  Input: List of <code>(prediction, truth)</code> pairs of length N. 2.  Repeat <code>B=1000</code> times:     - Sample N pairs with replacement.     - Calculate the metric (Accuracy or Balanced Accuracy). 3.  Primary output (paper format): Report bootstrap mean and standard deviation. 4.  Optional output: Also compute 2.5th / 97.5th percentiles for 95% bootstrap interval.</p> <pre><code>import numpy as np\nfrom dataclasses import dataclass\n\n@dataclass\nclass BootstrapResult:\n    mean: float\n    std: float\n    ci_lower: float  # 2.5th percentile\n    ci_upper: float  # 97.5th percentile\n    n_replicates: int = 1000\n\ndef bootstrap_metric(\n    predictions: list[str],\n    truths: list[str],\n    metric_fn: Callable[[list[str], list[str]], float],\n    n_replicates: int = 1000,\n    seed: int = 42,\n) -&gt; BootstrapResult:\n    rng = np.random.default_rng(seed)\n    n = len(predictions)\n    scores = []\n    for _ in range(n_replicates):\n        idx = rng.choice(n, size=n, replace=True)\n        sample_pred = [predictions[i] for i in idx]\n        sample_truth = [truths[i] for i in idx]\n        scores.append(metric_fn(sample_pred, sample_truth))\n    return BootstrapResult(\n        mean=np.mean(scores),\n        std=np.std(scores, ddof=1),\n        ci_lower=np.percentile(scores, 2.5),\n        ci_upper=np.percentile(scores, 97.5),\n        n_replicates=n_replicates,\n    )\n</code></pre>"},{"location":"specs/spec-10-evaluation/#test-plan","title":"Test Plan","text":""},{"location":"specs/spec-10-evaluation/#unit-tests","title":"Unit Tests","text":"<ol> <li>Metrics: Test Balanced Accuracy with imbalanced inputs.</li> <li>Bootstrap: Test logic on a small known set of numbers.</li> </ol>"},{"location":"specs/spec-10-evaluation/#file-structure","title":"File Structure","text":"<pre><code>src/giant/data/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 download.py       # HuggingFace download utilities\n\u2514\u2500\u2500 schemas.py        # BenchmarkItem, dataset task definitions\n\nsrc/giant/eval/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 runner.py         # Batch execution (BenchmarkRunner)\n\u251c\u2500\u2500 metrics.py        # Accuracy, Balanced Accuracy, BootstrapEvaluator\n\u251c\u2500\u2500 answer_extraction.py # Dataset-aware answer extraction/canonicalization\n\u2514\u2500\u2500 resumable.py      # Checkpoint/resume logic for interrupted runs\n\ntests/unit/eval/\n\u251c\u2500\u2500 test_metrics.py\n\u251c\u2500\u2500 test_bootstrap.py\n\u2514\u2500\u2500 test_answer_extraction.py\n\ntests/unit/data/\n\u2514\u2500\u2500 test_download.py\n</code></pre>"},{"location":"specs/spec-11-clam-integration/","title":"Spec-11: CLAM Integration (Patch Baseline)","text":""},{"location":"specs/spec-11-clam-integration/#overview","title":"Overview","text":"<p>This specification covers the tissue segmentation + random patch sampling needed to reproduce the paper's \"Random Patch\" baseline: segment tissue (CLAM-style), sample 30 random 224\u00d7224 patches from tissue, run the LMM independently per patch, and aggregate by majority vote.</p> <p>Paper Reference: \"Following prior work, we use the CLAM Python package to segment the tissue on the slide before patching. [...] The model independently answers each patch, and predictions are combined by majority vote.\"</p> <p>Implementation Choice (Paper Fidelity vs. Portability): - Primary (paper-faithful): Use the CLAM pipeline for tissue segmentation when available. - Fallback (portable): Use a documented \"CLAM-parity\" reimplementation with fixed parameters for environments where CLAM is not installable.</p>"},{"location":"specs/spec-11-clam-integration/#dependencies","title":"Dependencies","text":"<ul> <li>Spec-02: WSI Data Layer &amp; OpenSlide Integration</li> </ul>"},{"location":"specs/spec-11-clam-integration/#acceptance-criteria","title":"Acceptance Criteria","text":"<ul> <li>[x] <code>TissueSegmentor</code> supports <code>backend={\"clam\",\"parity\"}</code> (both run the CLAM-parity implementation; the <code>clam</code> name is reserved for future optional external CLAM integration).</li> <li>[x] Produces a binary mask (Tissue/Background) from the WSI thumbnail.</li> <li>[x] Implements Otsu's thresholding + morphological closing (CLAM default behavior).</li> <li>[x] <code>RandomPatchSampler</code> implemented: Returns <code>N=30</code> random <code>Region</code>s (Level-0 coords) of size <code>PATCH_SIZE=224\u00d7224</code> whose center overlaps tissue mask.</li> <li>[x] Determinism: RNG seeding via <code>seed</code> parameter for reproducible sampling.</li> <li>[x] Majority vote aggregation: <code>aggregate_predictions(predictions: list[str]) -&gt; str</code> with deterministic tie-breaking (alphabetical).</li> <li>[x] Dependencies <code>opencv-python</code>, <code>scipy</code>, and <code>scikit-image</code> verified in <code>pyproject.toml</code>.</li> </ul>"},{"location":"specs/spec-11-clam-integration/#technical-design","title":"Technical Design","text":""},{"location":"specs/spec-11-clam-integration/#implementation-details","title":"Implementation Details","text":""},{"location":"specs/spec-11-clam-integration/#tissue-segmentation","title":"Tissue Segmentation","text":"<ol> <li>Get Thumbnail: Load WSI thumbnail (Level roughly corresponding to 32x downsample or fixed size).</li> <li>Color Space: Convert to HSV.</li> <li>Threshold: Apply Otsu thresholding on the Saturation channel (common for H&amp;E) or simply luminance filtering to remove white background.</li> <li>Refine: Use <code>cv2.morphologyEx</code> (Close) to fill small holes.</li> <li>Contours: Find contours to get bounding boxes of tissue chunks.</li> </ol>"},{"location":"specs/spec-11-clam-integration/#random-sampler-paper-faithful","title":"Random Sampler (Paper-Faithful)","text":"<p>Paper Parameters: - <code>N = 30</code> patches per slide - <code>PATCH_SIZE = 224\u00d7224</code> pixels (Level-0)</p> <p>Patch Image Construction (Baseline): - Each sampled <code>Region</code> is read directly via <code>WSIReader.read_region(..., level=0, size=(224,224))</code>. - No additional resizing is applied (the model receives exactly 224\u00d7224 px images, per paper).</p> <p>Algorithm: <pre><code>def sample_patches(\n    mask: np.ndarray,  # Binary tissue mask at thumbnail scale\n    wsi_metadata: WSIMetadata,\n    n_patches: int = 30,\n    patch_size: int = 224,\n    seed: int = 42,\n) -&gt; list[Region]:\n    \"\"\"Sample N random patches from tissue regions.\"\"\"\n    rng = np.random.default_rng(seed)\n\n    # Scale factor: thumbnail \u2192 Level-0\n    scale_x = wsi_metadata.width / mask.shape[1]\n    scale_y = wsi_metadata.height / mask.shape[0]\n\n    # Find all tissue pixel coordinates\n    tissue_coords = np.argwhere(mask &gt; 0)  # (y, x) format\n    if len(tissue_coords) == 0:\n        raise ValueError(\"No tissue detected in slide\")\n\n    patches = []\n    attempts = 0\n    max_attempts = n_patches * 100\n\n    while len(patches) &lt; n_patches and attempts &lt; max_attempts:\n        # Random tissue pixel\n        idx = rng.integers(len(tissue_coords))\n        ty, tx = tissue_coords[idx]\n\n        # Convert to Level-0 coordinates (center of patch)\n        cx = int(tx * scale_x)\n        cy = int(ty * scale_y)\n\n        # Compute patch bounds\n        x = cx - patch_size // 2\n        y = cy - patch_size // 2\n\n        # Ensure within bounds\n        x = max(0, min(x, wsi_metadata.width - patch_size))\n        y = max(0, min(y, wsi_metadata.height - patch_size))\n\n        patches.append(Region(x=x, y=y, width=patch_size, height=patch_size))\n        attempts += 1\n\n    return patches\n</code></pre></p>"},{"location":"specs/spec-11-clam-integration/#majority-vote-aggregation","title":"Majority Vote Aggregation","text":"<pre><code>from collections import Counter\n\ndef aggregate_predictions(predictions: list[str]) -&gt; str:\n    \"\"\"Majority vote with deterministic tie-breaking (alphabetical).\"\"\"\n    if not predictions:\n        raise ValueError(\"No predictions to aggregate\")\n\n    counts = Counter(predictions)\n    max_count = max(counts.values())\n\n    # All predictions with max count (handle ties)\n    winners = [pred for pred, count in counts.items() if count == max_count]\n\n    # Deterministic tie-break: alphabetical order\n    return sorted(winners)[0]\n</code></pre>"},{"location":"specs/spec-11-clam-integration/#test-plan","title":"Test Plan","text":""},{"location":"specs/spec-11-clam-integration/#unit-tests","title":"Unit Tests","text":"<ol> <li>Segmentation Mask: Test that tissue regions produce True, background False.</li> <li>Morphological Closing: Verify small holes are filled.</li> <li>Random Sampler: Test that N patches are returned and all fall within tissue mask.</li> <li>Edge Cases: Empty slide (all background), fully filled slide.</li> </ol>"},{"location":"specs/spec-11-clam-integration/#file-structure","title":"File Structure","text":"<pre><code>src/giant/vision/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 segmentation.py   # TissueSegmentor, tissue_mask_from_thumbnail\n\u251c\u2500\u2500 sampler.py        # RandomPatchSampler, sample_patches()\n\u251c\u2500\u2500 aggregation.py    # aggregate_predictions() for majority vote\n\u2514\u2500\u2500 constants.py      # N_PATCHES=30, PATCH_SIZE=224, HSV thresholds\n\ntests/unit/vision/\n\u251c\u2500\u2500 test_segmentation.py\n\u251c\u2500\u2500 test_sampler.py\n\u2514\u2500\u2500 test_aggregation.py\n</code></pre>"},{"location":"specs/spec-11.5-e2e-validation-checkpoint/","title":"Spec-11.5: End-to-End Validation Checkpoint","text":""},{"location":"specs/spec-11.5-e2e-validation-checkpoint/#overview","title":"Overview","text":"<p>This is a mandatory checkpoint before proceeding to Spec-12 (CLI &amp; API Surface). The purpose is to validate that the entire GIANT system works end-to-end with real WSI files from the MultiPathQA benchmark.</p> <p>Why This Matters: We have built all the components (WSI reader, LLM providers, agent loop, evaluation framework, CLAM integration) but have only tested them in isolation with mocks and a single test WSI. This checkpoint ensures the system actually works on the benchmark data before we build the final CLI/API layer.</p> <p>Critical Lesson: This checkpoint should have been executed earlier, ideally after each major integration (Spec-05.5, Spec-08.5). We proceeded too far with unit tests and mocks without validating against real benchmark data.</p>"},{"location":"specs/spec-11.5-e2e-validation-checkpoint/#dependencies","title":"Dependencies","text":"<ul> <li>Spec-09: GIANT Agent Core Loop - Agent must be functional</li> <li>Spec-10: Evaluation &amp; Benchmarking - Metrics and runner implemented</li> <li>Spec-11: CLAM Integration - Tissue segmentation for baseline</li> <li>data-acquisition.md - WSIs must be downloaded</li> </ul>"},{"location":"specs/spec-11.5-e2e-validation-checkpoint/#multipathqa-benchmark-summary","title":"MultiPathQA Benchmark Summary","text":"<p>The benchmark contains 5 distinct tasks spanning 3 data sources:</p> Benchmark Questions Unique WSIs Task Source <code>tcga</code> 221 221 Cancer Diagnosis (30-way) TCGA <code>tcga_expert_vqa</code> 128 76 Pathologist-Authored VQA TCGA <code>tcga_slidebench</code> 197 183 SlideBench VQA TCGA <code>gtex</code> 191 191 Organ Classification (20-way) GTEx <code>panda</code> 197 197 Prostate Grading (6-way) PANDA Total 934 862 - - <p>WSI Files Required: - TCGA: 474 <code>.svs</code> files (all 3 TCGA benchmarks combined) - GTEx: 191 <code>.tiff</code> files - PANDA: 197 <code>.tiff</code> files - Total: 862 unique WSI files (TCGA alone is ~472 GiB; total is many hundreds of GiB)</p> <p>File lists are provided in <code>data/wsi/{tcga,gtex,panda}_files.txt</code>.</p>"},{"location":"specs/spec-11.5-e2e-validation-checkpoint/#pre-requisites","title":"Pre-Requisites","text":"<p>Before running this checkpoint, you MUST have:</p> <ol> <li>Downloaded at least a subset of WSIs from each benchmark:</li> <li>[ ] At least 5 TCGA slides in <code>data/wsi/tcga/</code> (see <code>data/wsi/tcga_files.txt</code>)</li> <li>[ ] At least 5 GTEx slides in <code>data/wsi/gtex/</code> (see <code>data/wsi/gtex_files.txt</code>)</li> <li> <p>[ ] At least 5 PANDA slides in <code>data/wsi/panda/</code> (see <code>data/wsi/panda_files.txt</code>)</p> </li> <li> <p>Configured API keys for at least one LLM provider:</p> </li> <li> <p>[ ] <code>ANTHROPIC_API_KEY</code> or <code>OPENAI_API_KEY</code> in <code>.env</code></p> </li> <li> <p>Verified OpenSlide installation: <pre><code>python -c \"import openslide; print(openslide.__version__)\"\n</code></pre></p> </li> </ol>"},{"location":"specs/spec-11.5-e2e-validation-checkpoint/#acceptance-criteria","title":"Acceptance Criteria","text":""},{"location":"specs/spec-11.5-e2e-validation-checkpoint/#phase-1-wsi-pipeline-validation","title":"Phase 1: WSI Pipeline Validation","text":"<ul> <li> <p>[ ] WSI Reader works on real files: <pre><code># For each downloaded WSI, verify it can be opened and read\npython -c \"\nfrom giant.wsi import WSIReader\nfrom pathlib import Path\n\nwsi_root = Path('data/wsi')\nfor subdir in ['tcga', 'gtex', 'panda']:\n    dir_path = wsi_root / subdir\n    if not dir_path.exists():\n        print(f'{subdir}: directory not found')\n        continue\n\n    wsi_files = list(dir_path.rglob('*.svs')) + list(dir_path.rglob('*.tiff'))\n    print(f'{subdir}: {len(wsi_files)} files found')\n\n    for wsi in wsi_files[:3]:  # Test first 3\n        try:\n            with WSIReader(wsi) as r:\n                meta = r.get_metadata()\n                print(f'  {wsi.name}: {meta.width}x{meta.height}, {meta.level_count} levels')\n        except Exception as e:\n            print(f'  {wsi.name}: ERROR - {e}')\n\"\n</code></pre></p> </li> <li> <p>[ ] Cropping pipeline works at all levels: <pre><code>python -c \"\nfrom giant.core import CropEngine\nfrom giant.wsi import WSIReader\nfrom giant.geometry import Region\nfrom pathlib import Path\n\n# Find a TCGA slide\ntcga_dir = Path('data/wsi/tcga')\nwsi_file = next(tcga_dir.rglob('*.svs'), None)\nif not wsi_file:\n    print('No TCGA slides found')\n    exit(1)\n\nwith WSIReader(wsi_file) as reader:\n    meta = reader.get_metadata()\n    # Crop from center\n    region = Region(\n        x=meta.width // 4,\n        y=meta.height // 4,\n        width=meta.width // 2,\n        height=meta.height // 2\n    )\n    engine = CropEngine(reader)\n    cropped = engine.crop(region, target_size=1000)\n    print(f'Source: {wsi_file.name}')\n    print(f'Region: {region}')\n    print(f'Crop shape: {cropped.image.size}')\n    cropped.image.save('/tmp/test_crop.png')\n    print('Saved to /tmp/test_crop.png')\n\"\n</code></pre></p> </li> </ul>"},{"location":"specs/spec-11.5-e2e-validation-checkpoint/#phase-2-tissue-segmentation-validation","title":"Phase 2: Tissue Segmentation Validation","text":"<ul> <li> <p>[ ] CLAM parity segmentation works: <pre><code>python -c \"\nfrom giant.wsi import WSIReader\nfrom giant.vision import TissueSegmentor\nimport numpy as np\nfrom pathlib import Path\n\ntcga_dir = Path('data/wsi/tcga')\nwsi_file = next(tcga_dir.rglob('*.svs'), None)\nif not wsi_file:\n    print('No TCGA slides found')\n    exit(1)\n\nwith WSIReader(wsi_file) as reader:\n    thumb = reader.get_thumbnail((2048, 2048))\n    segmentor = TissueSegmentor()\n    mask = segmentor.segment(thumb)\n    tissue_pct = np.mean(mask) * 100\n    print(f'Source: {wsi_file.name}')\n    print(f'Thumbnail size: {thumb.size}')\n    print(f'Tissue coverage: {tissue_pct:.1f}%')\n\"\n</code></pre></p> </li> <li> <p>[ ] Random patch sampling works: <pre><code>python -c \"\nfrom giant.wsi import WSIReader\nfrom giant.vision import TissueSegmentor, sample_patches\nfrom pathlib import Path\n\ntcga_dir = Path('data/wsi/tcga')\nwsi_file = next(tcga_dir.rglob('*.svs'), None)\nif not wsi_file:\n    print('No TCGA slides found')\n    exit(1)\n\nwith WSIReader(wsi_file) as reader:\n    meta = reader.get_metadata()\n    thumb = reader.get_thumbnail((2048, 2048))\n    segmentor = TissueSegmentor()\n    mask = segmentor.segment(thumb)\n    patches = sample_patches(mask, meta, n_patches=10)\n    print(f'Source: {wsi_file.name}')\n    print(f'Sampled {len(patches)} patches')\n    for p in patches[:3]:\n        print(f'  Region: ({p.x}, {p.y}) - {p.width}x{p.height}')\n\"\n</code></pre></p> </li> </ul>"},{"location":"specs/spec-11.5-e2e-validation-checkpoint/#phase-3-agent-loop-validation","title":"Phase 3: Agent Loop Validation","text":"<ul> <li>[ ] Agent can navigate a real WSI: <pre><code># Run agent on a single slide (costs vary by model/steps)\npython -c \"\nimport asyncio\nfrom giant.llm import create_provider\nfrom giant.agent import AgentConfig, GIANTAgent\nfrom pathlib import Path\n\nprovider = create_provider('anthropic')  # or 'openai'\n\ntcga_dir = Path('data/wsi/tcga')\nwsi_file = next(tcga_dir.rglob('*.svs'), None)\nif not wsi_file:\n    print('No TCGA slides found')\n    raise SystemExit(1)\n\nasync def main() -&gt; None:\n    agent = GIANTAgent(\n        wsi_path=wsi_file,\n        question='What type of cancer is shown in this slide?',\n        llm_provider=provider,\n        config=AgentConfig(max_steps=5),\n    )\n    result = await agent.run()\n    print(f'Source: {wsi_file.name}')\n    print(f'Success: {result.success}')\n    print(f'Answer: {result.answer}')\n    print(f'Turns: {len(result.trajectory.turns)}')\n    print(f'Cost: \\${result.total_cost:.4f}')\n    if result.error_message:\n        print(f'Error: {result.error_message}')\n\nasyncio.run(main())\n\"\n</code></pre></li> </ul>"},{"location":"specs/spec-11.5-e2e-validation-checkpoint/#phase-4-evaluation-pipeline-validation","title":"Phase 4: Evaluation Pipeline Validation","text":"<ul> <li> <p>[ ] Benchmark runner can process items from all 5 benchmarks: <pre><code># Run on up to 2 items from each benchmark (costs vary).\n# If you only have a subset of WSIs locally, this will skip missing slides.\npython -c \"\nimport asyncio\nfrom pathlib import Path\n\nfrom giant.eval.runner import BenchmarkRunner, EvaluationConfig\nfrom giant.llm import create_provider\n\nasync def main() -&gt; None:\n    provider = create_provider('anthropic')  # or 'openai'\n    runner = BenchmarkRunner(\n        llm_provider=provider,\n        wsi_root=Path('data/wsi'),\n        output_dir=Path('results/e2e_validation'),\n        config=EvaluationConfig(\n            max_steps=5,\n            max_concurrent=1,\n            max_items=2,\n            skip_missing_wsis=True,\n        ),\n    )\n\n    csv_path = Path('data/multipathqa/MultiPathQA.csv')\n    benchmarks = ['tcga', 'tcga_expert_vqa', 'tcga_slidebench', 'gtex', 'panda']\n\n    for bm in benchmarks:\n        results = await runner.run_benchmark(\n            benchmark_name=bm,\n            csv_path=csv_path,\n            run_id=f'e2e_{bm}',\n        )\n        print(f\\\"{bm}: {results.metrics}\\\")\n\nasyncio.run(main())\n\"\n</code></pre></p> </li> <li> <p>[ ] Metrics calculation is correct: <pre><code># Verify accuracy and balanced accuracy compute correctly\npython -c \"\nfrom giant.eval.metrics import accuracy, balanced_accuracy\n\n# Perfect case\npreds = [1, 2, 3, 4, 5]\ntruth = [1, 2, 3, 4, 5]\nassert accuracy(preds, truth) == 1.0\n\n# Imbalanced case (should penalize biased classifier)\npreds = [1, 1, 1, 1, 1]  # Always predicts 1\ntruth = [1, 1, 1, 1, 2]  # But class 2 exists\nassert accuracy(preds, truth) == 0.8\nassert balanced_accuracy(preds, truth) == 0.5\nprint('Metrics validation passed')\n\"\n</code></pre></p> </li> </ul>"},{"location":"specs/spec-11.5-e2e-validation-checkpoint/#phase-5-full-benchmark-run-optional-but-recommended","title":"Phase 5: Full Benchmark Run (Optional but Recommended)","text":"<ul> <li> <p>[ ] Run full benchmark on one task: <pre><code># Full TCGA cancer diagnosis benchmark (~$50-100, ~2-4 hours)\npython -c \"\nimport asyncio\nfrom pathlib import Path\n\nfrom giant.eval.runner import BenchmarkRunner, EvaluationConfig\nfrom giant.llm import create_provider\n\nasync def main() -&gt; None:\n    provider = create_provider('anthropic')  # or 'openai'\n    runner = BenchmarkRunner(\n        llm_provider=provider,\n        wsi_root=Path('data/wsi'),\n        output_dir=Path('results/tcga_full'),\n        config=EvaluationConfig(max_steps=20),\n    )\n    await runner.run_benchmark(\n        benchmark_name='tcga',\n        csv_path=Path('data/multipathqa/MultiPathQA.csv'),\n        run_id='tcga_full',\n    )\n\nasyncio.run(main())\n\"\n</code></pre></p> </li> <li> <p>[ ] Compare results to paper baseline:</p> </li> </ul> <p>Paper reports for GPT-5 + GIANT on main benchmarks:   | Benchmark | Paper Result | Expected Range |   |-----------|--------------|----------------|   | TCGA (cancer diagnosis) | 32.3% | 25-40% |   | GTEx (organ classification) | 60.7% | 50-70% |   | PANDA (prostate grading) | 25.4% | 18-35% |   | Expert VQA | 62.5% | 50-75% |   | SlideBench VQA | 58.9% | 45-70% |</p>"},{"location":"specs/spec-11.5-e2e-validation-checkpoint/#validation-report-template","title":"Validation Report Template","text":"<p>After completing the checkpoint, create a validation report:</p> <pre><code># E2E Validation Report\n\n**Date:** YYYY-MM-DD\n**Tester:** [Name]\n\n## Environment\n- Python: X.Y.Z\n- OpenSlide: X.Y.Z\n- LLM Provider: [anthropic/openai]\n- Model: [claude-sonnet-4-20250514/gpt-5]\n\n## WSI Data\n- TCGA slides: N / 474\n- GTEx slides: N / 191\n- PANDA slides: N / 197\n\n## Results\n\n### Phase 1: WSI Pipeline\n- [ ] Pass / [ ] Fail\n- Notes:\n\n### Phase 2: Segmentation\n- [ ] Pass / [ ] Fail\n- Notes:\n\n### Phase 3: Agent Loop\n- [ ] Pass / [ ] Fail\n- Cost: $X.XX\n- Notes:\n\n### Phase 4: Evaluation\n- [ ] Pass / [ ] Fail\n- Results:\n  - tcga: X%\n  - tcga_expert_vqa: X%\n  - tcga_slidebench: X%\n  - gtex: X%\n  - panda: X%\n- Notes:\n\n### Phase 5: Full Benchmark (if run)\n- Benchmark: [name]\n- Result: X% +/- Y%\n- Paper baseline: Z% +/- W%\n- Within expected range: [Yes/No]\n\n## Issues Found\n1. [Issue description]\n2. [Issue description]\n\n## Sign-off\n- [ ] All critical phases passed\n- [ ] Ready to proceed to Spec-12\n</code></pre>"},{"location":"specs/spec-11.5-e2e-validation-checkpoint/#estimated-costs","title":"Estimated Costs","text":"Validation Level WSIs Needed API Cost Time Minimal (Phase 1-2) 5 per source $0 10 min Basic (Phase 1-4) 5 per source $2-5 30 min Full (Phase 1-5) All 862 $50-150 4-8 hours"},{"location":"specs/spec-11.5-e2e-validation-checkpoint/#what-to-do-if-validation-fails","title":"What to Do If Validation Fails","text":"<ol> <li>WSI read errors: Check OpenSlide installation, file permissions, file corruption</li> <li>Segmentation errors: Check image mode (must be RGB), check mask dimensions</li> <li>Agent errors: Check API keys, rate limits, prompt formatting</li> <li>Metric mismatches: Check answer extraction logic, verify truth labels from CSV</li> </ol>"},{"location":"specs/spec-11.5-e2e-validation-checkpoint/#post-checkpoint-actions","title":"Post-Checkpoint Actions","text":"<p>Once all phases pass:</p> <ol> <li>[ ] Create validation report and save to <code>docs/validation/</code></li> <li>[ ] Commit report to repository</li> <li>[ ] Proceed to Spec-12: CLI &amp; API Surface</li> </ol> <p>If phases fail:</p> <ol> <li>[ ] Document the failure mode</li> <li>[ ] Create a bug report in <code>docs/bugs/</code></li> <li>[ ] Fix the issue before proceeding</li> <li>[ ] Re-run validation</li> </ol>"},{"location":"specs/spec-12-cli-api/","title":"Spec-12: CLI &amp; API Surface","text":""},{"location":"specs/spec-12-cli-api/#overview","title":"Overview","text":"<p>This specification defines the user-facing command-line interface (CLI) for GIANT. It uses <code>Typer</code> to provide a clean, documented CLI for running single-slide inference, executing benchmarks, downloading data, and visualizing results. The CLI supports all three evaluation modes from the paper: GIANT (agentic), Thumbnail baseline, and Patch baseline.</p>"},{"location":"specs/spec-12-cli-api/#dependencies","title":"Dependencies","text":"<ul> <li>Spec-09: GIANT Agent Core Loop</li> <li>Spec-10: Evaluation &amp; Benchmarking Framework</li> <li>Spec-11: CLAM Integration (for patch baseline)</li> </ul>"},{"location":"specs/spec-12-cli-api/#acceptance-criteria","title":"Acceptance Criteria","text":"<ul> <li>[x] <code>giant</code> command exists and is registered as console script.</li> <li>[x] <code>giant run &lt;path&gt; --question &lt;q&gt;</code> executes the agent and prints the answer + cost.</li> <li>[x] <code>giant run</code> supports <code>--mode {giant,thumbnail,patch}</code> for baseline comparison.</li> <li>[x] <code>giant run</code> supports <code>--runs N</code> for majority voting (GIANT x5).</li> <li>[x] <code>giant run</code> supports <code>--provider {openai,anthropic}</code> and <code>--model</code> selection.</li> <li>[x] <code>giant run</code> supports <code>--budget-usd</code> to cap total spend and fail fast.</li> <li>[x] <code>giant run</code> supports <code>--strict-font-check</code> to fail if axis label fonts are missing.</li> <li>[x] <code>giant benchmark &lt;dataset&gt;</code> runs the evaluation pipeline with resume support.</li> <li>[x] <code>giant benchmark</code> supports <code>--wsi-root &lt;dir&gt;</code> to resolve <code>image_path</code> entries from MultiPathQA metadata to local slide files.</li> <li>[x] <code>giant benchmark</code> supports <code>--budget-usd</code> to cap total spend and fail fast.</li> <li>[x] <code>giant benchmark</code> supports <code>--strict-font-check</code> to fail if axis label fonts are missing.</li> <li>[x] <code>giant benchmark</code> handles SIGINT/SIGTERM gracefully by writing a final checkpoint before exiting.</li> <li>[x] <code>giant download</code> downloads MultiPathQA metadata (<code>MultiPathQA.csv</code>) from HuggingFace.</li> <li>[x] <code>giant check-data &lt;dataset&gt;</code> validates that required WSIs exist under <code>--wsi-root</code>.</li> <li>[x] <code>giant visualize &lt;result.json&gt;</code> outputs trajectory visualization.</li> <li>[x] Logging verbosity controllable via <code>-v</code> / <code>-vv</code> / <code>-vvv</code> flags.</li> <li>[x] <code>--json</code> flag outputs machine-readable JSON for all commands.</li> <li>[x] Exit codes: 0=success, 1=error, 2=invalid args.</li> </ul>"},{"location":"specs/spec-12-cli-api/#technical-design","title":"Technical Design","text":""},{"location":"specs/spec-12-cli-api/#cli-structure-srcgiantclimainpy","title":"CLI Structure (<code>src/giant/cli/main.py</code>)","text":"<pre><code>import typer\nfrom typing import Optional\nfrom enum import Enum\nfrom pathlib import Path\n\napp = typer.Typer(\n    name=\"giant\",\n    help=\"GIANT: Gigapixel Image Agent for Navigating Tissue\",\n    add_completion=False,\n)\n\nclass Mode(str, Enum):\n    giant = \"giant\"       # Full agentic navigation\n    thumbnail = \"thumbnail\"  # Single thumbnail baseline\n    patch = \"patch\"       # Random patch sampling baseline\n\nclass Provider(str, Enum):\n    openai = \"openai\"\n    anthropic = \"anthropic\"\n\n@app.command()\ndef run(\n    wsi_path: Path = typer.Argument(..., help=\"Path to WSI file (.svs, .ndpi, .tiff)\"),\n    question: str = typer.Option(..., \"--question\", \"-q\", help=\"Question to answer\"),\n    mode: Mode = typer.Option(Mode.giant, \"--mode\", \"-m\", help=\"Evaluation mode\"),\n    provider: Provider = typer.Option(Provider.openai, \"--provider\", \"-p\"),\n    model: str = typer.Option(\"gpt-5.2\", \"--model\", help=\"Model name (see docs/models/model-registry.md)\"),\n    max_steps: int = typer.Option(20, \"--max-steps\", \"-T\", help=\"Max navigation steps\"),\n    strict_font_check: bool = typer.Option(False, \"--strict-font-check/--no-strict-font-check\", help=\"Fail if TrueType fonts are unavailable for axis labels\"),\n    runs: int = typer.Option(1, \"--runs\", \"-r\", help=\"Number of runs for majority voting\"),\n    budget_usd: float = typer.Option(0.0, \"--budget-usd\", help=\"Stop early if total cost exceeds this USD budget (0 disables)\"),\n    output: Optional[Path] = typer.Option(None, \"--output\", \"-o\", help=\"Save trajectory to JSON\"),\n    verbose: int = typer.Option(0, \"--verbose\", \"-v\", count=True, help=\"Increase verbosity\"),\n    json_output: bool = typer.Option(False, \"--json\", help=\"Output as JSON\"),\n):\n    \"\"\"Run GIANT on a single WSI.\"\"\"\n    ...\n\n@app.command()\ndef benchmark(\n    dataset: str = typer.Argument(..., help=\"Dataset name or path (tcga, panda, gtex, tcga_expert_vqa|expertvqa, tcga_slidebench|slidebenchvqa)\"),\n    output_dir: Path = typer.Option(Path(\"./results\"), \"--output-dir\", \"-o\"),\n    mode: Mode = typer.Option(Mode.giant, \"--mode\", \"-m\"),\n    provider: Provider = typer.Option(Provider.openai, \"--provider\", \"-p\"),\n    model: str = typer.Option(\"gpt-5.2\", \"--model\"),\n    max_steps: int = typer.Option(20, \"--max-steps\", \"-T\"),\n    strict_font_check: bool = typer.Option(False, \"--strict-font-check/--no-strict-font-check\", help=\"Fail if TrueType fonts are unavailable for axis labels\"),\n    runs: int = typer.Option(1, \"--runs\", \"-r\", help=\"Runs per item for majority voting\"),\n    concurrency: int = typer.Option(4, \"--concurrency\", \"-c\", help=\"Max concurrent API calls\"),\n    wsi_root: Path = typer.Option(Path(\"./wsi\"), \"--wsi-root\", help=\"Root directory containing WSIs referenced by MultiPathQA.csv\"),\n    budget_usd: float = typer.Option(0.0, \"--budget-usd\", help=\"Stop early if total cost exceeds this USD budget (0 disables)\"),\n    resume: bool = typer.Option(True, \"--resume/--no-resume\", help=\"Resume from checkpoint\"),\n    verbose: int = typer.Option(0, \"--verbose\", \"-v\", count=True),\n):\n    \"\"\"Run the full benchmark suite.\"\"\"\n    ...\n\n@app.command()\ndef download(\n    dataset: str = typer.Argument(\"multipathqa\", help=\"Dataset to download\"),\n    output_dir: Path = typer.Option(Path(\"./data\"), \"--output-dir\", \"-o\"),\n    verbose: int = typer.Option(0, \"--verbose\", \"-v\", count=True),\n):\n    \"\"\"Download benchmark datasets from HuggingFace.\"\"\"\n    ...\n\n@app.command()\ndef check_data(\n    dataset: str = typer.Argument(..., help=\"Dataset name (tcga, panda, gtex, tcga_expert_vqa, tcga_slidebench)\"),\n    csv_path: Path = typer.Option(Path(\"data/multipathqa/MultiPathQA.csv\"), \"--csv-path\", help=\"Path to MultiPathQA.csv\"),\n    wsi_root: Path = typer.Option(Path(\"data/wsi\"), \"--wsi-root\", help=\"Root directory containing WSIs\"),\n    verbose: int = typer.Option(0, \"--verbose\", \"-v\", count=True),\n    json_output: bool = typer.Option(False, \"--json\", help=\"Output as JSON\"),\n):\n    \"\"\"Validate that WSI files for a benchmark exist locally.\"\"\"\n    ...\n\n@app.command()\ndef visualize(\n    trajectory_path: Path = typer.Argument(..., help=\"Path to trajectory JSON\"),\n    output: Optional[Path] = typer.Option(None, \"--output\", \"-o\", help=\"Output HTML file\"),\n    open_browser: bool = typer.Option(True, \"--open/--no-open\", help=\"Open in browser\"),\n):\n    \"\"\"Generate interactive visualization of navigation trajectory.\"\"\"\n    ...\n\n@app.command()\ndef version():\n    \"\"\"Show version information.\"\"\"\n    ...\n\nif __name__ == \"__main__\":\n    app()\n</code></pre>"},{"location":"specs/spec-12-cli-api/#pyprojecttoml-script-entry","title":"pyproject.toml Script Entry","text":"<pre><code>[project.scripts]\ngiant = \"giant.cli.main:app\"\n</code></pre>"},{"location":"specs/spec-12-cli-api/#verbosity-levels","title":"Verbosity Levels","text":"<ul> <li><code>-v</code> (1): INFO logs, show step progress</li> <li><code>-vv</code> (2): DEBUG logs, show LLM prompts/responses</li> <li><code>-vvv</code> (3): TRACE logs, show all API calls and timing</li> </ul>"},{"location":"specs/spec-12-cli-api/#majority-voting-runs","title":"Majority Voting (--runs)","text":"<p>When <code>--runs N</code> is specified: 1. Run the agent N times independently on the same question 2. Collect all final answers 3. Return the most common answer (majority vote) 4. Report agreement percentage</p> <p>Paper uses N=5 for \"GIANT x5\" results.</p>"},{"location":"specs/spec-12-cli-api/#mode-implementation","title":"Mode Implementation","text":"<ul> <li>giant: Full agentic loop (Spec-09)</li> <li>thumbnail: Single LLM call with 1024x1024 thumbnail</li> <li>patch: CLAM segmentation + 30 random patches + majority vote (Spec-11)</li> </ul>"},{"location":"specs/spec-12-cli-api/#visualization","title":"Visualization","text":"<p>For <code>visualize</code>, generate a static HTML file that: 1. Loads the <code>trajectory.json</code> 2. Displays the WSI thumbnail with axis guides 3. Shows each step in a carousel with:    - The cropped region highlighted on thumbnail    - The actual crop image    - The model's reasoning    - The action taken 4. Final answer summary</p>"},{"location":"specs/spec-12-cli-api/#optional-http-service-mode-production-deployment","title":"Optional: HTTP Service Mode (Production Deployment)","text":"<p>If GIANT is deployed behind an API (e.g., for a lab internal service), add a separate entrypoint (e.g., <code>giant serve</code>) using FastAPI (optional dependency group): - <code>GET /healthz</code> returns 200 if process is alive - <code>GET /readyz</code> validates required config (API keys, model selection) and can do a lightweight provider auth check (no image) - Graceful shutdown on SIGTERM/SIGINT: stop accepting new requests, persist in-flight trajectory/checkpoint, and close OpenSlide handles</p>"},{"location":"specs/spec-12-cli-api/#test-plan","title":"Test Plan","text":""},{"location":"specs/spec-12-cli-api/#unit-tests","title":"Unit Tests","text":"<ol> <li>CLI Parsing: Test all argument combinations parse correctly.</li> <li>Mode Selection: Verify correct runner is instantiated for each mode.</li> <li>Verbosity: Test log level is set correctly for -v, -vv, -vvv.</li> <li>JSON Output: Verify --json produces valid JSON.</li> <li>Exit Codes: Test exit code 0 on success, 1 on error.</li> </ol>"},{"location":"specs/spec-12-cli-api/#integration-tests","title":"Integration Tests","text":"<ol> <li>Run Command: Mock LLM, verify end-to-end flow.</li> <li>Benchmark Resume: Start benchmark, interrupt, resume, verify no duplicates.</li> <li>Download Command: Mock HuggingFace, verify files created.</li> </ol>"},{"location":"specs/spec-12-cli-api/#file-structure","title":"File Structure","text":"<pre><code>src/giant/cli/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 main.py         # Typer app with all commands\n\u251c\u2500\u2500 runners.py      # Mode-specific runner wrappers\n\u251c\u2500\u2500 visualizer.py   # HTML generation for trajectories\n\ntests/unit/cli/\n\u251c\u2500\u2500 test_main.py\n\u251c\u2500\u2500 test_runners.py\n\u2514\u2500\u2500 test_visualizer.py\n\ntests/integration/cli/\n\u2514\u2500\u2500 test_cli_e2e.py\n</code></pre>"},{"location":"validation/e2e-validation-2025-12-20/","title":"E2E Validation Report","text":"<p>Date: 2025-12-20 Branch: <code>test/e2e-integration-checkpoint</code> Tester: Claude Code (automated)</p>"},{"location":"validation/e2e-validation-2025-12-20/#environment","title":"Environment","text":"<ul> <li>Python: 3.13.5</li> <li>OpenSlide: Installed via system (macOS)</li> <li>Platform: darwin (macOS)</li> </ul>"},{"location":"validation/e2e-validation-2025-12-20/#wsi-data-available","title":"WSI Data Available","text":"Source Available Required Coverage TCGA 50 files 474 files 10.5% GTEx 0 files 191 files 0% PANDA 0 files 197 files 0% <p>MultiPathQA Questions Matched: 53 questions from 50 TCGA WSIs</p> <p>Breakdown: - <code>tcga</code>: 25 questions - <code>tcga_slidebench</code>: 21 questions - <code>tcga_expert_vqa</code>: 7 questions</p>"},{"location":"validation/e2e-validation-2025-12-20/#results-summary","title":"Results Summary","text":""},{"location":"validation/e2e-validation-2025-12-20/#spec-055-wsi-integration-checkpoint","title":"Spec-05.5: WSI Integration Checkpoint","text":"Test Status Details P0-1: Open real SVS file PASS TCGA-06-0875-01Z-00-DX1.svs P0-2: Thumbnail generation PASS 1024x1024 max P0-3: Read region at L0 PASS 256x256 P0-4: Read region at max level PASS Level 2 P0-5: Coordinate roundtrip PASS P0-6: Level selection PASS P0-7: Crop pipeline E2E PASS Boundary tests (P1) PASS 4/4 Total 17/17 PASSED"},{"location":"validation/e2e-validation-2025-12-20/#spec-085-llm-integration-checkpoint","title":"Spec-08.5: LLM Integration Checkpoint","text":"Test Category Status Details Mock tests (P0-P2) 61/61 PASS All providers mocked OpenAI Live API PASS gpt-5.2 Anthropic Live API PASS claude-sonnet-4-5-20250929"},{"location":"validation/e2e-validation-2025-12-20/#full-e2e-agent-loop","title":"Full E2E Agent Loop","text":"Provider WSI Question Ground Truth Answer Match Tokens Cost Anthropic (claude-sonnet-4-5-20250929) TCGA-06-0875-01Z-00-DX1 Cancer Diagnosis 1 1 CORRECT 10,761 $1.83 OpenAI (gpt-5.2) TCGA-06-0875-01Z-00-DX2 Cancer Diagnosis 1 1 CORRECT 9,208 $0.04 <p>Key Finding: OpenAI is ~50x cheaper per run than Anthropic.</p>"},{"location":"validation/e2e-validation-2025-12-20/#agent-trajectory-anthropic-run","title":"Agent Trajectory (Anthropic Run)","text":"<pre><code>Turn 0: crop(1000, 1000, 4000, 4000)\n  Reasoning: Viewing thumbnail with multiple tissue fragments...\n\nTurn 1: crop(2500, 3000, 2000, 2000)\n  Reasoning: Epithelial tissue visible, zooming further...\n\nTurn 2: answer\n  Reasoning: Dense clusters with high nuclear-to-cytoplasmic ratio...\n  Answer: {\"answer\": 1}\n</code></pre>"},{"location":"validation/e2e-validation-2025-12-20/#cost-projections","title":"Cost Projections","text":"<p>For 53 available questions (max_steps=3):</p> Provider Est. Cost Est. Time OpenAI (gpt-5.2) ~$2.12 ~30 min Anthropic (claude-sonnet-4-5-20250929) ~$97 ~30 min <p>For full MultiPathQA (934 questions, max_steps=20):</p> Provider Est. Cost Est. Time OpenAI (gpt-5.2) ~$700 ~8 hours Anthropic (claude-sonnet-4-5-20250929) ~$34,000 ~8 hours <p>Recommendation: Use OpenAI for bulk benchmark runs.</p>"},{"location":"validation/e2e-validation-2025-12-20/#issues-found","title":"Issues Found","text":"<p>None. All tests passed without errors.</p>"},{"location":"validation/e2e-validation-2025-12-20/#phases-completed","title":"Phases Completed","text":"<ul> <li>[x] Phase 1: WSI Pipeline Validation</li> <li>[x] Phase 2: Tissue Segmentation (skipped - not required for agent loop)</li> <li>[x] Phase 3: Agent Loop Validation</li> <li>[ ] Phase 4: Evaluation Pipeline (not run - requires more data)</li> <li>[ ] Phase 5: Full Benchmark Run (not run - cost/time)</li> </ul>"},{"location":"validation/e2e-validation-2025-12-20/#sign-off","title":"Sign-off","text":"<ul> <li>[x] WSI pipeline works with real TCGA files</li> <li>[x] LLM providers work with live API calls</li> <li>[x] Full agent loop completes successfully</li> <li>[x] Agent produces correct answers on test cases</li> <li>[x] Both OpenAI and Anthropic validated</li> </ul> <p>Status: READY TO PROCEED</p> <p>The core GIANT implementation is functional end-to-end. Remaining work: 1. Acquire more WSI data (GTEx, PANDA) for full benchmark 2. Run evaluation pipeline on larger subset 3. Compare results to paper baselines</p>"}]}